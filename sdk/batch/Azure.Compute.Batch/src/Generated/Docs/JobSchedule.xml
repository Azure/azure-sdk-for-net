<?xml version="1.0" encoding="utf-8"?>
<doc>
  <members>
    <member name="ExistsAsync(String,Int32,String,Boolean,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call ExistsAsync with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = await client.ExistsAsync("<jobScheduleId>");
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call ExistsAsync with all parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = await client.ExistsAsync("<jobScheduleId>", 1234, "<clientRequestId>", true, "<ocpDate>", null);
Console.WriteLine(response.Status);
]]></code>
</example>
    </member>
    <member name="Exists(String,Int32,String,Boolean,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call Exists with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = client.Exists("<jobScheduleId>");
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call Exists with all parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = client.Exists("<jobScheduleId>", 1234, "<clientRequestId>", true, "<ocpDate>", null);
Console.WriteLine(response.Status);
]]></code>
</example>
    </member>
    <member name="DeleteAsync(String,Int32,String,Boolean,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call DeleteAsync with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = await client.DeleteAsync("<jobScheduleId>");
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call DeleteAsync with all parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = await client.DeleteAsync("<jobScheduleId>", 1234, "<clientRequestId>", true, "<ocpDate>", null);
Console.WriteLine(response.Status);
]]></code>
</example>
<remarks>
When you delete a Job Schedule, this also deletes all Jobs and Tasks under that
schedule. When Tasks are deleted, all the files in their working directories on
the Compute Nodes are also deleted (the retention period is ignored). The Job
Schedule statistics are no longer accessible once the Job Schedule is deleted,
though they are still counted towards Account lifetime statistics.
</remarks>
    </member>
    <member name="Delete(String,Int32,String,Boolean,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call Delete with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = client.Delete("<jobScheduleId>");
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call Delete with all parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = client.Delete("<jobScheduleId>", 1234, "<clientRequestId>", true, "<ocpDate>", null);
Console.WriteLine(response.Status);
]]></code>
</example>
<remarks>
When you delete a Job Schedule, this also deletes all Jobs and Tasks under that
schedule. When Tasks are deleted, all the files in their working directories on
the Compute Nodes are also deleted (the retention period is ignored). The Job
Schedule statistics are no longer accessible once the Job Schedule is deleted,
though they are still counted towards Account lifetime statistics.
</remarks>
    </member>
    <member name="GetJobScheduleAsync(String,Int32,String,Boolean,String,String,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call GetJobScheduleAsync with required parameters and parse the result.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = await client.GetJobScheduleAsync("<jobScheduleId>");

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.ToString());
]]></code>
This sample shows how to call GetJobScheduleAsync with all parameters, and how to parse the result.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = await client.GetJobScheduleAsync("<jobScheduleId>", 1234, "<clientRequestId>", true, "<ocpDate>", "<select>", "<expand>", null);

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("url").ToString());
Console.WriteLine(result.GetProperty("eTag").ToString());
Console.WriteLine(result.GetProperty("lastModified").ToString());
Console.WriteLine(result.GetProperty("creationTime").ToString());
Console.WriteLine(result.GetProperty("state").ToString());
Console.WriteLine(result.GetProperty("stateTransitionTime").ToString());
Console.WriteLine(result.GetProperty("previousState").ToString());
Console.WriteLine(result.GetProperty("previousStateTransitionTime").ToString());
Console.WriteLine(result.GetProperty("schedule").GetProperty("doNotRunUntil").ToString());
Console.WriteLine(result.GetProperty("schedule").GetProperty("doNotRunAfter").ToString());
Console.WriteLine(result.GetProperty("schedule").GetProperty("startWindow").ToString());
Console.WriteLine(result.GetProperty("schedule").GetProperty("recurrenceInterval").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("priority").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("allowTaskPreemption").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("maxParallelTasks").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("usesTaskDependencies").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("onAllTasksComplete").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("onTaskFailure").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("filePattern").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("path").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("containerUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("uploadOptions").GetProperty("uploadCondition").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("requiredSlots").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("killJobOnCompletion").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("runExclusive").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("authenticationTokenSettings").GetProperty("access")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("allowLowPriorityNode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("waitForSuccess").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("rerunOnNodeRebootAfterSuccess").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("retentionTime").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("commonEnvironmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("commonEnvironmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("poolId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("autoPoolIdPrefix").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("poolLifetimeOption").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("keepAlive").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("vmSize").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osFamily").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osVersion").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("publisher").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("offer").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("sku").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("version").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("virtualMachineImageId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("exactVersion").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodeAgentSKUId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("windowsConfiguration").GetProperty("enableAutomaticUpdates").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("lun").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("caching").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("diskSizeGB").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("storageAccountType").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("licenseType").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("type").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerImageNames")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("diskEncryptionConfiguration").GetProperty("targets")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodePlacementConfiguration").GetProperty("policy").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("publisher").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("type").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("typeHandlerVersion").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("autoUpgradeMinorVersion").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("provisionAfterExtensions")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("osDisk").GetProperty("ephemeralOSDiskSettings").GetProperty("placement").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSlotsPerNode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSchedulingPolicy").GetProperty("nodeFillType").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("resizeTimeout").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetDedicatedNodes").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetLowPriorityNodes").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableAutoScale").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleFormula").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleEvaluationInterval").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableInterNodeCommunication").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("dynamicVNetAssignmentScope").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("protocol").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("backendPort").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeStart").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeEnd").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("priority").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("access").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourceAddressPrefix").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourcePortRanges")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("provision").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("ipAddressIds")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("waitForSuccess").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprint").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprintAlgorithm").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeLocation").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("visibility")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationLicenses")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("uid").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("gid").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("sshPrivateKey").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("windowsUserConfiguration").GetProperty("loginMode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("containerName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountKey").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("sasKey").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("blobfuseOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("source").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("mountOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("source").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("mountOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("azureFileUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountKey").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("mountOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetNodeCommunicationMode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("metadata")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("metadata")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("executionInfo").GetProperty("nextRunTime").ToString());
Console.WriteLine(result.GetProperty("executionInfo").GetProperty("recentJob").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("executionInfo").GetProperty("recentJob").GetProperty("url").ToString());
Console.WriteLine(result.GetProperty("executionInfo").GetProperty("endTime").ToString());
Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("url").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("startTime").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("lastUpdateTime").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("userCPUTime").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("kernelCPUTime").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("wallClockTime").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("readIOps").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOps").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("readIOGiB").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOGiB").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("numSucceededTasks").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("numFailedTasks").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("numTaskRetries").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("waitTime").ToString());
]]></code>
</example>
<remarks>
Below is the JSON schema for the response payload.

Response Body:

Schema for <c>BatchJobSchedule</c>:
<code>{
  id: string, # Optional. A string that uniquely identifies the schedule within the Account.
  displayName: string, # Optional. The display name for the schedule.
  url: string, # Optional. The URL of the Job Schedule.
  eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job Schedule has
changed between requests. In particular, you can be pass the ETag with an
Update Job Schedule request to specify that your changes should take effect
only if nobody else has modified the schedule in the meantime.
  lastModified: string (date &amp; time), # Optional. This is the last time at which the schedule level data, such as the Job
specification or recurrence information, changed. It does not factor in
job-level changes such as new Jobs being created or Jobs changing state.
  creationTime: string (date &amp; time), # Optional. The creation time of the Job Schedule.
  state: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. The state of the Job Schedule.
  stateTransitionTime: string (date &amp; time), # Optional. The time at which the Job Schedule entered the current state.
  previousState: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. This property is not present if the Job Schedule is in its initial active state.
  previousStateTransitionTime: string (date &amp; time), # Optional. This property is not present if the Job Schedule is in its initial active state.
  schedule: {
    doNotRunUntil: string (date &amp; time), # Optional. If you do not specify a doNotRunUntil time, the schedule becomes ready to
create Jobs immediately.
    doNotRunAfter: string (date &amp; time), # Optional. If you do not specify a doNotRunAfter time, and you are creating a recurring
Job Schedule, the Job Schedule will remain active until you explicitly
terminate it.
    startWindow: string (duration ISO 8601 Format), # Optional. If a Job is not created within the startWindow interval, then the &apos;opportunity&apos;
is lost; no Job will be created until the next recurrence of the schedule. If
the schedule is recurring, and the startWindow is longer than the recurrence
interval, then this is equivalent to an infinite startWindow, because the Job
that is &apos;due&apos; in one recurrenceInterval is not carried forward into the next
recurrence interval. The default is infinite. The minimum value is 1 minute. If
you specify a lower value, the Batch service rejects the schedule with an
error; if you are calling the REST API directly, the HTTP status code is 400
(Bad Request).
    recurrenceInterval: string (duration ISO 8601 Format), # Optional. Because a Job Schedule can have at most one active Job under it at any given
time, if it is time to create a new Job under a Job Schedule, but the previous
Job is still running, the Batch service will not create the new Job until the
previous Job finishes. If the previous Job does not finish within the
startWindow period of the new recurrenceInterval, then no new Job will be
scheduled for that interval. For recurring Jobs, you should normally specify a
jobManagerTask in the jobSpecification. If you do not use jobManagerTask, you
will need an external process to monitor when Jobs are created, add Tasks to
the Jobs and terminate the Jobs ready for the next recurrence. The default is
that the schedule does not recur: one Job is created, within the startWindow
after the doNotRunUntil time, and the schedule is complete as soon as that Job
finishes. The minimum value is 1 minute. If you specify a lower value, the
Batch service rejects the schedule with an error; if you are calling the REST
API directly, the HTTP status code is 400 (Bad Request).
  }, # Optional. All times are fixed respective to UTC and are not impacted by daylight saving
time.
  jobSpecification: {
    priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest
priority and 1000 being the highest priority. The default value is 0. This
priority is used as the default for all Jobs under the Job Schedule. You can
update a Job&apos;s priority after it has been created using by using the update Job
API.
    allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system
will take precedence and will be able requeue tasks from this job. You can
update a job&apos;s allowTaskPreemption after it has been created using the update
job API.
    maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not
specified, the default value is -1, which means there&apos;s no limit to the number
of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after
it has been created using the update job API.
    displayName: string, # Optional. The name need not be unique and can contain any Unicode characters up to a
maximum length of 1024.
    usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is
false.
    onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. Note that if a Job contains no Tasks, then all Tasks are considered complete.
This option is therefore most commonly used with a Job Manager task; if you
want to use automatic Job termination without a Job Manager, you should
initially set onAllTasksComplete to noaction and update the Job properties to
set onAllTasksComplete to terminatejob once you have finished adding Tasks. The
default is noaction.
    onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. The default is noaction.
    networkConfiguration: {
      subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes which will run Tasks from the Job. This
can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos;
service principal must have the &apos;Classic Virtual Machine Contributor&apos;
Role-Based Access Control (RBAC) role for the specified VNet so that Azure
Batch service can schedule Tasks on the Nodes. This can be verified by checking
if the specified VNet has any associated Network Security Groups (NSG). If
communication to the Nodes in the specified subnet is denied by an NSG, then
the Batch service will set the state of the Compute Nodes to unusable. This is
of the form
/subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}.
If the specified VNet has any associated Network Security Groups (NSG), then a
few reserved system ports must be enabled for inbound communication from the
Azure Batch service. For Pools created with a Virtual Machine configuration,
enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for
Windows. Port 443 is also required to be open for outbound connections for
communications to Azure Storage. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
    }, # Optional. The network configuration for the Job.
    constraints: {
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service
terminates it and any Tasks that are still running. In this case, the
termination reason will be MaxWallClockTimeExpiry. If this property is not
specified, there is no time limit on how long the Job may run.
      maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch
service will try each Task once, and may then retry up to this limit. For
example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one
initial try and 3 retries). If the maximum retry count is 0, the Batch service
does not retry Tasks. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
    }, # Optional. The execution constraints for a Job.
    jobManagerTask: {
      id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters.
      displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum
length of 1024.
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: {
        containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot;
command, in addition to those controlled by the Batch Service.
        imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If
no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a
default.
        registry: {
          username: string, # Optional. The user name to log into the registry server.
          password: string, # Optional. The password to log into the registry server.
          registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
          identityReference: {
            resourceId: string, # Optional. The ARM resource id of the user assigned identity.
          }, # Optional. The reference to a user assigned identity associated with the Batch pool which
a compute node will use.
        }, # Optional. This setting can be omitted if was already provided at Pool creation.
        workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
      }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must
be set as well. If the Pool that will run this Task doesn&apos;t have
containerConfiguration set, this must not be set. When this is specified, all
directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure
Batch directories on the node) are mapped into the container, all Task
environment variables are mapped into the container, and the Task command line
is executed in the container. Files produced in the container outside of
AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that
Batch file APIs will not be able to access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      outputFiles: [OutputFile], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node
on which the primary Task is executed.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Manager Task.
      constraints: {
        maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
        maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task
executable due to a nonzero exit code. The Batch service will try the Task
once, and may then retry up to this limit. For example, if the maximum retry
count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries).
If the maximum retry count is 0, the Batch service does not retry the Task
after the first attempt. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
      }, # Optional. Execution constraints to apply to a Task.
      requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the
node has enough free scheduling slots available. For multi-instance Tasks, this
property is not supported and must not be specified.
      killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job
as complete. If any Tasks are still running at this time (other than Job
Release), those Tasks are terminated. If false, the completion of the Job
Manager Task does not affect the Job status. In this case, you should either
use the onAllTasksComplete attribute to terminate the Job, or have a client or
user terminate the Job explicitly. An example of this is if the Job Manager
creates a set of Tasks but then takes no further role in their execution. The
default value is true. If you are using the onAllTasksComplete and
onTaskFailure attributes to control Job lifetime, and using the Job Manager
Task only to create the Tasks for the Job (not to monitor progress), then it is
important to set killJobOnCompletion to false.
      userIdentity: {
        username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
        autoUser: {
          scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task
should be specified if stricter isolation between tasks is required. For
example, if the task mutates the registry in a way which could impact other
tasks, or if certificates have been specified on the pool which should not be
accessible by normal tasks but should be accessible by StartTasks.
          elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
      }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
      runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job
Manager is running. If false, other Tasks can run simultaneously with the Job
Manager on a Compute Node. The Job Manager Task counts normally against the
Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute
Node allows multiple concurrent Tasks. The default value is true.
      applicationPackageReferences: [ApplicationPackageReference], # Optional. Application Packages are downloaded and deployed to a shared directory, not the
Task working directory. Therefore, if a referenced Application Package is
already on the Compute Node, and is up to date, then it is not re-downloaded;
the existing copy on the Compute Node is used. If a referenced Application
Package cannot be installed, for example because the package has been deleted
or because download failed, the Task fails.
      authenticationTokenSettings: {
        access: [&quot;job&quot;], # Optional. The authentication token grants access to a limited set of Batch service
operations. Currently the only supported value for the access property is
&apos;job&apos;, which grants access to all operations related to the Job which contains
the Task.
      }, # Optional. If this property is set, the Batch service provides the Task with an
authentication token which can be used to authenticate Batch service operations
without requiring an Account access key. The token is provided via the
AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the
Task can carry out using the token depend on the settings. For example, a Task
can request Job permissions in order to add other Tasks to the Job, or check
the status of the Job or of other Tasks under the Job.
      allowLowPriorityNode: boolean, # Optional. The default value is true.
    }, # Optional. If the Job does not specify a Job Manager Task, the user must explicitly add
Tasks to the Job using the Task API. If the Job does specify a Job Manager
Task, the Batch service creates the Job Manager Task when the Job is created,
and will try to schedule the Job Manager Task before scheduling other Tasks in
the Job.
    jobPreparationTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job
Preparation Task. If you try to submit a Task with the same id, the Batch
service rejects the request with error code TaskIdSameAsJobPreparationTask; if
you are calling the REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory. 
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
      constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
      waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries
the Job Preparation Task up to its maximum retry count (as specified in the
constraints element). If the Task has still not completed successfully after
all retries, then the Batch service will not schedule Tasks of the Job to the
Node. The Node remains active and eligible to run Tasks of other Jobs. If
false, the Batch service will not wait for the Job Preparation Task to
complete. In this case, other Tasks of the Job can start executing on the
Compute Node while the Job Preparation Task is still running; and even if the
Job Preparation Task fails, new Tasks will continue to be scheduled on the
Compute Node. The default value is true.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on
Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux
Compute Nodes.
      rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if
the Job Preparation Task did not complete (e.g. because the reboot occurred
while the Task was running). Therefore, you should always write a Job
Preparation Task to be idempotent and to behave correctly if run multiple
times. The default value is true.
    }, # Optional. If a Job has a Job Preparation Task, the Batch service will run the Job
Preparation Task on a Node before starting any Tasks of that Job on that
Compute Node.
    jobReleaseTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release
Task. If you try to submit a Task with the same id, the Batch service rejects
the request with error code TaskIdSameAsJobReleaseTask; if you are calling the
REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute
Node, measured from the time the Task starts. If the Task does not complete
within the time limit, the Batch service terminates it. The default value is 15
minutes. You may not specify a timeout longer than 15 minutes. If you do, the
Batch service rejects it with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
      retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
    }, # Optional. The primary purpose of the Job Release Task is to undo changes to Nodes made by
the Job Preparation Task. Example activities include deleting local files, or
shutting down services that were started as part of Job preparation. A Job
Release Task cannot be specified without also specifying a Job Preparation Task
for the Job. The Batch service runs the Job Release Task on the Compute Nodes
that have run the Job Preparation Task.
    commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by
specifying the same setting name with a different value.
    poolInfo: {
      poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool
does not exist at the time the Batch service tries to schedule a Job, no Tasks
for the Job will run until you create a Pool with that id. Note that the Batch
service will not reject the Job request; it will simply not run Tasks until the
Pool exists. You must specify either the Pool ID or the auto Pool
specification, but not both.
      autoPoolSpecification: {
        autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To
distinguish between Pools created for different purposes, you can specify this
element to add a prefix to the ID that is assigned. The prefix can be up to 20
characters long.
        poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule
are assigned to Pools.
        keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined
by the poolLifetimeOption setting) expires; that is, when the Job or Job
Schedule completes. If true, the Batch service does not delete the Pool
automatically. It is up to the user to delete auto Pools created with this
option.
        pool: {
          displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up
to a maximum length of 1024.
          vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose
a VM size for Compute Nodes in an Azure Batch Pool
(https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
          cloudServiceConfiguration: {
            osFamily: string, # Required. Possible values are:
2 - OS Family 2, equivalent to Windows Server 2008 R2
SP1.
3 - OS Family 3, equivalent to Windows Server 2012.
4 - OS Family 4,
equivalent to Windows Server 2012 R2.
5 - OS Family 5, equivalent to Windows
Server 2016.
6 - OS Family 6, equivalent to Windows Server 2019. For more
information, see Azure Guest OS Releases
(https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
            osVersion: string, # Optional. The default value is * which specifies the latest operating system version for
the specified OS family.
          }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS
VMs. This property and virtualMachineConfiguration are mutually exclusive and
one of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request). This property cannot be specified if the
Batch Account was created with its poolAllocationMode property set to
&apos;UserSubscription&apos;.
          virtualMachineConfiguration: {
            imageReference: {
              publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
              offer: string, # Optional. For example, UbuntuServer or WindowsServer.
              sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
              version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image.
If omitted, the default is &apos;latest&apos;.
              virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The
Shared Image Gallery Image must have replicas in the same region and must be in
the same subscription as the Azure Batch account. If the image version is not
specified in the imageId, the latest version will be used. For information
about the firewall settings for the Batch Compute Node agent to communicate
with the Batch service see
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
              exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create
the node. This read-only field differs from &apos;version&apos; only if the value
specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
            }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image
Gallery Image. To get the list of all Azure Marketplace Image references
verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
            nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the
Pool, and provides the command-and-control interface between the Compute Node
and the Batch service. There are different implementations of the Compute Node
agent, known as SKUs, for different operating systems. You must specify a
Compute Node agent SKU which matches the selected Image reference. To get the
list of supported Compute Node agent SKUs along with their list of verified
Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
            windowsConfiguration: {
              enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
            }, # Optional. This property must not be specified if the imageReference property specifies a
Linux OS Image.
            dataDisks: [DataDisk], # Optional. This property must be specified if the Compute Nodes in the Pool need to have
empty data disks attached to them. This cannot be updated. Each Compute Node
gets its own disk (the disk is not a file share). Existing disks cannot be
attached, each attached disk is empty. When the Compute Node is removed from
the Pool, the disk and all data associated with it is also deleted. The disk is
not formatted after being attached, it must be formatted before use - for more
information see
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux
and
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
            licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and
should only be used when you hold valid on-premises licenses for the Compute
Nodes which will be deployed. If omitted, no on-premises licensing discount is
applied. Values are:

 Windows_Server - The on-premises license is for Windows
Server.
 Windows_Client - The on-premises license is for Windows Client.

            containerConfiguration: {
              type: &quot;dockerCompatible&quot;, # Required. The container technology to be used.
              containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An
Image will be sourced from the default Docker registry unless the Image is
fully qualified with an alternative registry.
              containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires
credentials, then those credentials must be provided here.
            }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow
Tasks to run in containers. All regular Tasks and Job manager Tasks run on this
Pool must specify the containerSettings property, and all other Tasks may
specify it.
            diskEncryptionConfiguration: {
              targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On
Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot;
and &quot;TemporaryDisk&quot; must be specified.
            }, # Optional. If specified, encryption is performed on each node in the pool during node
provisioning.
            nodePlacementConfiguration: {
              policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not
specified, Batch will use the regional policy.
            }, # Optional. This configuration will specify rules on how nodes in the pool will be
physically allocated.
            extensions: [VMExtension], # Optional. If specified, the extensions mentioned in this configuration will be installed
on each node.
            osDisk: {
              ephemeralOSDiskSettings: {
                placement: &quot;cachedisk&quot;, # Optional. This property can be used by user in the request to choose the location e.g.,
cache disk space for Ephemeral OS disk provisioning. For more information on
Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size
requirements for Windows VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements
and Linux VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
              }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the
compute node (VM).
            }, # Optional. Settings for the operating system disk of the compute node (VM).
          }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS
VMs. This property and cloudServiceConfiguration are mutually exclusive and one
of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request).
          taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number
of cores of the vmSize of the pool or 256.
          taskSchedulingPolicy: {
            nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
          }, # Optional. If not specified, the default is spread.
          resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when
enableAutoScale is set to true. The default value is 15 minutes. The minimum
value is 5 minutes. If you specify a value less than 5 minutes, the Batch
service rejects the request with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
          targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must
be specified. If true, the autoScaleFormula element is required. The Pool
automatically resizes according to the formula. The default value is false.
          autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is
required if enableAutoScale is set to true. The formula is checked for validity
before the Pool is created. If the formula is not valid, the Batch service
rejects the request with detailed error information.
          autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes
and 168 hours respectively. If you specify a value less than 5 minutes or
greater than 168 hours, the Batch service rejects the request with an invalid
property value error; if you are calling the REST API directly, the HTTP status
code is 400 (Bad Request).
          enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to
deployment restrictions on the Compute Nodes of the Pool. This may result in
the Pool not reaching its desired size. The default value is false.
          networkConfiguration: {
            subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have
enough free IP addresses, the Pool will partially allocate Nodes and a resize
error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the
&apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for
the specified VNet. The specified subnet must allow communication from the
Azure Batch service to be able to schedule Tasks on the Nodes. This can be
verified by checking if the specified VNet has any associated Network Security
Groups (NSG). If communication to the Nodes in the specified subnet is denied
by an NSG, then the Batch service will set the state of the Compute Nodes to
unusable. For Pools created with virtualMachineConfiguration only ARM virtual
networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools
created with cloudServiceConfiguration both ARM and classic virtual networks
are supported. If the specified VNet has any associated Network Security Groups
(NSG), then a few reserved system ports must be enabled for inbound
communication. For Pools created with a virtual machine configuration, enable
ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows.
For Pools created with a cloud service configuration, enable ports 10100,
20100, and 30100. Also enable outbound connections to Azure Storage on port
443. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
            dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
            endpointConfiguration: {
              inboundNATPools: [InboundNATPool], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum
number of inbound NAT Pools is exceeded the request fails with HTTP status code
400. This cannot be specified if the IPAddressProvisioningType is
NoPublicIPAddresses.
            }, # Optional. Pool endpoint configuration is only supported on Pools with the
virtualMachineConfiguration property.
            publicIPAddressConfiguration: {
              provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
              ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100
dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public
IP. For example, a pool needing 250 dedicated VMs would need at least 3 public
IPs specified. Each element of this collection is of the form:
/subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
            }, # Optional. Public IP configuration property is only supported on Pools with the
virtualMachineConfiguration property.
          }, # Optional. The network configuration for a Pool.
          startTask: {
            commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
            containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
            resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
            environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
            userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
            maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this
value specifically controls the number of retries. The Batch service will try
the Task once, and may then retry up to this limit. For example, if the maximum
retry count is 3, Batch tries the Task up to 4 times (one initial try and 3
retries). If the maximum retry count is 0, the Batch service does not retry the
Task. If the maximum retry count is -1, the Batch service retries the Task
without limit, however this is not recommended for a start task or any task.
The default value is 0 (no retries)
            waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the
StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has
still not completed successfully after all retries, then the Batch service
marks the Node unusable, and will not schedule Tasks to it. This condition can
be detected via the Compute Node state and failure info details. If false, the
Batch service will not wait for the StartTask to complete. In this case, other
Tasks can start executing on the Compute Node while the StartTask is still
running; and even if the StartTask fails, new Tasks will continue to be
scheduled on the Compute Node. The default is true.
          }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node.
Examples of recovery operations include (but are not limited to) when an
unhealthy Node is rebooted or a Compute Node disappeared due to host failure.
Retries due to recovery operations are independent of and are not counted
against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal
retry due to a recovery operation may occur. Because of this, all Tasks should
be idempotent. This means Tasks need to tolerate being interrupted and
restarted without causing any corruption or duplicate data. The best practice
for long running Tasks is to use some form of checkpointing. In some cases the
StartTask may be re-run even though the Compute Node was not rebooted. Special
care should be taken to avoid StartTasks which create breakaway process or
install/launch services from the StartTask working directory, as this will
block Batch from being able to re-run the StartTask.
          certificateReferences: [CertificateReference], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified
Certificate store and location. For Linux Compute Nodes, the Certificates are
stored in a directory inside the Task working directory and an environment
variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this
location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory
is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and
Certificates are placed in that directory.
          applicationPackageReferences: [ApplicationPackageReference], # Optional. When creating a pool, the package&apos;s application ID must be fully qualified
(/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationName}).
Changes to Package references affect all new Nodes joining the Pool, but do not
affect Compute Nodes that are already in the Pool until they are rebooted or
reimaged. There is a maximum of 10 Package references on any given Pool.
          applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service
application licenses. If a license is requested which is not supported, Pool
creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;,
&apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application
license added to the Pool.
          userAccounts: [UserAccount], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
          metadata: [
            {
              name: string, # Required. The name of the metadata item.
              value: string, # Required. The value of the metadata item.
            }
          ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
          mountConfiguration: [MountConfiguration], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
          targetNodeCommunicationMode: &quot;default&quot; | &quot;classic&quot; | &quot;simplified&quot;, # Optional. If omitted, the default value is Default.
        }, # Optional. Specification for creating a new Pool.
      }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed
state, and the Pool creation error is set in the Job&apos;s scheduling error
property. The Batch service manages the lifetime (both creation and, unless
keepAlive is specified, deletion) of the auto Pool. Any user actions that
affect the lifetime of the auto Pool while the Job is active will result in
unexpected behavior. You must specify either the Pool ID or the auto Pool
specification, but not both.
    }, # Required. Specifies how a Job should be assigned to a Pool.
    metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  }, # Optional. Specifies details of the Jobs to be created on a schedule.
  executionInfo: {
    nextRunTime: string (date &amp; time), # Optional. This property is meaningful only if the schedule is in the active state when
the time comes around. For example, if the schedule is disabled, no Job will be
created at nextRunTime unless the Job is enabled before then.
    recentJob: {
      id: string, # Optional. The ID of the Job.
      url: string, # Optional. The URL of the Job.
    }, # Optional. This property is present only if the at least one Job has run under the
schedule.
    endTime: string (date &amp; time), # Optional. This property is set only if the Job Schedule is in the completed state.
  }, # Optional. Contains information about Jobs that have been and will be run under a Job
Schedule.
  metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  stats: {
    url: string, # Required. The URL of the statistics.
    startTime: string (date &amp; time), # Required. The start time of the time range covered by the statistics.
    lastUpdateTime: string (date &amp; time), # Required. The time at which the statistics were last updated. All statistics are limited
to the range between startTime and lastUpdateTime.
    userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    wallClockTime: string (duration ISO 8601 Format), # Required. The wall clock time is the elapsed time from when the Task started running on a
Compute Node to when it finished (or to the last time the statistics were
updated, if the Task had not finished by then). If a Task was retried, this
includes the wall clock time of all the Task retries.
    readIOps: number, # Required. The total number of disk read operations made by all Tasks in all Jobs created
under the schedule.
    writeIOps: number, # Required. The total number of disk write operations made by all Tasks in all Jobs created
under the schedule.
    readIOGiB: number, # Required. The total gibibytes read from disk by all Tasks in all Jobs created under the
schedule.
    writeIOGiB: number, # Required. The total gibibytes written to disk by all Tasks in all Jobs created under the
schedule.
    numSucceededTasks: number, # Required. The total number of Tasks successfully completed during the given time range in
Jobs created under the schedule. A Task completes successfully if it returns
exit code 0.
    numFailedTasks: number, # Required. The total number of Tasks that failed during the given time range in Jobs
created under the schedule. A Task fails if it exhausts its maximum retry count
without returning exit code 0.
    numTaskRetries: number, # Required. The total number of retries during the given time range on all Tasks in all
Jobs created under the schedule.
    waitTime: string (duration ISO 8601 Format), # Required. This value is only reported in the Account lifetime statistics; it is not
included in the Job statistics.
  }, # Optional. Resource usage statistics for a Job Schedule.
}
</code>

</remarks>
    </member>
    <member name="GetJobSchedule(String,Int32,String,Boolean,String,String,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call GetJobSchedule with required parameters and parse the result.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = client.GetJobSchedule("<jobScheduleId>");

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.ToString());
]]></code>
This sample shows how to call GetJobSchedule with all parameters, and how to parse the result.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = client.GetJobSchedule("<jobScheduleId>", 1234, "<clientRequestId>", true, "<ocpDate>", "<select>", "<expand>", null);

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("url").ToString());
Console.WriteLine(result.GetProperty("eTag").ToString());
Console.WriteLine(result.GetProperty("lastModified").ToString());
Console.WriteLine(result.GetProperty("creationTime").ToString());
Console.WriteLine(result.GetProperty("state").ToString());
Console.WriteLine(result.GetProperty("stateTransitionTime").ToString());
Console.WriteLine(result.GetProperty("previousState").ToString());
Console.WriteLine(result.GetProperty("previousStateTransitionTime").ToString());
Console.WriteLine(result.GetProperty("schedule").GetProperty("doNotRunUntil").ToString());
Console.WriteLine(result.GetProperty("schedule").GetProperty("doNotRunAfter").ToString());
Console.WriteLine(result.GetProperty("schedule").GetProperty("startWindow").ToString());
Console.WriteLine(result.GetProperty("schedule").GetProperty("recurrenceInterval").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("priority").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("allowTaskPreemption").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("maxParallelTasks").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("usesTaskDependencies").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("onAllTasksComplete").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("onTaskFailure").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("filePattern").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("path").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("containerUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("uploadOptions").GetProperty("uploadCondition").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("requiredSlots").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("killJobOnCompletion").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("runExclusive").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("authenticationTokenSettings").GetProperty("access")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("allowLowPriorityNode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("waitForSuccess").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("rerunOnNodeRebootAfterSuccess").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("retentionTime").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("commonEnvironmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("commonEnvironmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("poolId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("autoPoolIdPrefix").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("poolLifetimeOption").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("keepAlive").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("vmSize").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osFamily").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osVersion").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("publisher").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("offer").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("sku").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("version").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("virtualMachineImageId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("exactVersion").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodeAgentSKUId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("windowsConfiguration").GetProperty("enableAutomaticUpdates").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("lun").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("caching").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("diskSizeGB").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("storageAccountType").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("licenseType").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("type").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerImageNames")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("diskEncryptionConfiguration").GetProperty("targets")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodePlacementConfiguration").GetProperty("policy").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("publisher").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("type").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("typeHandlerVersion").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("autoUpgradeMinorVersion").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("provisionAfterExtensions")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("osDisk").GetProperty("ephemeralOSDiskSettings").GetProperty("placement").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSlotsPerNode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSchedulingPolicy").GetProperty("nodeFillType").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("resizeTimeout").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetDedicatedNodes").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetLowPriorityNodes").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableAutoScale").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleFormula").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleEvaluationInterval").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableInterNodeCommunication").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("dynamicVNetAssignmentScope").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("protocol").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("backendPort").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeStart").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeEnd").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("priority").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("access").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourceAddressPrefix").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourcePortRanges")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("provision").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("ipAddressIds")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("waitForSuccess").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprint").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprintAlgorithm").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeLocation").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("visibility")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationLicenses")[0].ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("uid").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("gid").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("sshPrivateKey").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("windowsUserConfiguration").GetProperty("loginMode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("containerName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountKey").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("sasKey").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("blobfuseOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("source").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("mountOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("source").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("mountOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountName").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("azureFileUrl").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountKey").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("mountOptions").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetNodeCommunicationMode").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("metadata")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("jobSpecification").GetProperty("metadata")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("executionInfo").GetProperty("nextRunTime").ToString());
Console.WriteLine(result.GetProperty("executionInfo").GetProperty("recentJob").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("executionInfo").GetProperty("recentJob").GetProperty("url").ToString());
Console.WriteLine(result.GetProperty("executionInfo").GetProperty("endTime").ToString());
Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("metadata")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("url").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("startTime").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("lastUpdateTime").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("userCPUTime").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("kernelCPUTime").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("wallClockTime").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("readIOps").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOps").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("readIOGiB").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("writeIOGiB").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("numSucceededTasks").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("numFailedTasks").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("numTaskRetries").ToString());
Console.WriteLine(result.GetProperty("stats").GetProperty("waitTime").ToString());
]]></code>
</example>
<remarks>
Below is the JSON schema for the response payload.

Response Body:

Schema for <c>BatchJobSchedule</c>:
<code>{
  id: string, # Optional. A string that uniquely identifies the schedule within the Account.
  displayName: string, # Optional. The display name for the schedule.
  url: string, # Optional. The URL of the Job Schedule.
  eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job Schedule has
changed between requests. In particular, you can be pass the ETag with an
Update Job Schedule request to specify that your changes should take effect
only if nobody else has modified the schedule in the meantime.
  lastModified: string (date &amp; time), # Optional. This is the last time at which the schedule level data, such as the Job
specification or recurrence information, changed. It does not factor in
job-level changes such as new Jobs being created or Jobs changing state.
  creationTime: string (date &amp; time), # Optional. The creation time of the Job Schedule.
  state: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. The state of the Job Schedule.
  stateTransitionTime: string (date &amp; time), # Optional. The time at which the Job Schedule entered the current state.
  previousState: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. This property is not present if the Job Schedule is in its initial active state.
  previousStateTransitionTime: string (date &amp; time), # Optional. This property is not present if the Job Schedule is in its initial active state.
  schedule: {
    doNotRunUntil: string (date &amp; time), # Optional. If you do not specify a doNotRunUntil time, the schedule becomes ready to
create Jobs immediately.
    doNotRunAfter: string (date &amp; time), # Optional. If you do not specify a doNotRunAfter time, and you are creating a recurring
Job Schedule, the Job Schedule will remain active until you explicitly
terminate it.
    startWindow: string (duration ISO 8601 Format), # Optional. If a Job is not created within the startWindow interval, then the &apos;opportunity&apos;
is lost; no Job will be created until the next recurrence of the schedule. If
the schedule is recurring, and the startWindow is longer than the recurrence
interval, then this is equivalent to an infinite startWindow, because the Job
that is &apos;due&apos; in one recurrenceInterval is not carried forward into the next
recurrence interval. The default is infinite. The minimum value is 1 minute. If
you specify a lower value, the Batch service rejects the schedule with an
error; if you are calling the REST API directly, the HTTP status code is 400
(Bad Request).
    recurrenceInterval: string (duration ISO 8601 Format), # Optional. Because a Job Schedule can have at most one active Job under it at any given
time, if it is time to create a new Job under a Job Schedule, but the previous
Job is still running, the Batch service will not create the new Job until the
previous Job finishes. If the previous Job does not finish within the
startWindow period of the new recurrenceInterval, then no new Job will be
scheduled for that interval. For recurring Jobs, you should normally specify a
jobManagerTask in the jobSpecification. If you do not use jobManagerTask, you
will need an external process to monitor when Jobs are created, add Tasks to
the Jobs and terminate the Jobs ready for the next recurrence. The default is
that the schedule does not recur: one Job is created, within the startWindow
after the doNotRunUntil time, and the schedule is complete as soon as that Job
finishes. The minimum value is 1 minute. If you specify a lower value, the
Batch service rejects the schedule with an error; if you are calling the REST
API directly, the HTTP status code is 400 (Bad Request).
  }, # Optional. All times are fixed respective to UTC and are not impacted by daylight saving
time.
  jobSpecification: {
    priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest
priority and 1000 being the highest priority. The default value is 0. This
priority is used as the default for all Jobs under the Job Schedule. You can
update a Job&apos;s priority after it has been created using by using the update Job
API.
    allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system
will take precedence and will be able requeue tasks from this job. You can
update a job&apos;s allowTaskPreemption after it has been created using the update
job API.
    maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not
specified, the default value is -1, which means there&apos;s no limit to the number
of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after
it has been created using the update job API.
    displayName: string, # Optional. The name need not be unique and can contain any Unicode characters up to a
maximum length of 1024.
    usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is
false.
    onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. Note that if a Job contains no Tasks, then all Tasks are considered complete.
This option is therefore most commonly used with a Job Manager task; if you
want to use automatic Job termination without a Job Manager, you should
initially set onAllTasksComplete to noaction and update the Job properties to
set onAllTasksComplete to terminatejob once you have finished adding Tasks. The
default is noaction.
    onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. The default is noaction.
    networkConfiguration: {
      subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes which will run Tasks from the Job. This
can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos;
service principal must have the &apos;Classic Virtual Machine Contributor&apos;
Role-Based Access Control (RBAC) role for the specified VNet so that Azure
Batch service can schedule Tasks on the Nodes. This can be verified by checking
if the specified VNet has any associated Network Security Groups (NSG). If
communication to the Nodes in the specified subnet is denied by an NSG, then
the Batch service will set the state of the Compute Nodes to unusable. This is
of the form
/subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}.
If the specified VNet has any associated Network Security Groups (NSG), then a
few reserved system ports must be enabled for inbound communication from the
Azure Batch service. For Pools created with a Virtual Machine configuration,
enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for
Windows. Port 443 is also required to be open for outbound connections for
communications to Azure Storage. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
    }, # Optional. The network configuration for the Job.
    constraints: {
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service
terminates it and any Tasks that are still running. In this case, the
termination reason will be MaxWallClockTimeExpiry. If this property is not
specified, there is no time limit on how long the Job may run.
      maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch
service will try each Task once, and may then retry up to this limit. For
example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one
initial try and 3 retries). If the maximum retry count is 0, the Batch service
does not retry Tasks. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
    }, # Optional. The execution constraints for a Job.
    jobManagerTask: {
      id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters.
      displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum
length of 1024.
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: {
        containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot;
command, in addition to those controlled by the Batch Service.
        imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If
no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a
default.
        registry: {
          username: string, # Optional. The user name to log into the registry server.
          password: string, # Optional. The password to log into the registry server.
          registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
          identityReference: {
            resourceId: string, # Optional. The ARM resource id of the user assigned identity.
          }, # Optional. The reference to a user assigned identity associated with the Batch pool which
a compute node will use.
        }, # Optional. This setting can be omitted if was already provided at Pool creation.
        workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
      }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must
be set as well. If the Pool that will run this Task doesn&apos;t have
containerConfiguration set, this must not be set. When this is specified, all
directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure
Batch directories on the node) are mapped into the container, all Task
environment variables are mapped into the container, and the Task command line
is executed in the container. Files produced in the container outside of
AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that
Batch file APIs will not be able to access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      outputFiles: [OutputFile], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node
on which the primary Task is executed.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Manager Task.
      constraints: {
        maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
        maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task
executable due to a nonzero exit code. The Batch service will try the Task
once, and may then retry up to this limit. For example, if the maximum retry
count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries).
If the maximum retry count is 0, the Batch service does not retry the Task
after the first attempt. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
      }, # Optional. Execution constraints to apply to a Task.
      requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the
node has enough free scheduling slots available. For multi-instance Tasks, this
property is not supported and must not be specified.
      killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job
as complete. If any Tasks are still running at this time (other than Job
Release), those Tasks are terminated. If false, the completion of the Job
Manager Task does not affect the Job status. In this case, you should either
use the onAllTasksComplete attribute to terminate the Job, or have a client or
user terminate the Job explicitly. An example of this is if the Job Manager
creates a set of Tasks but then takes no further role in their execution. The
default value is true. If you are using the onAllTasksComplete and
onTaskFailure attributes to control Job lifetime, and using the Job Manager
Task only to create the Tasks for the Job (not to monitor progress), then it is
important to set killJobOnCompletion to false.
      userIdentity: {
        username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
        autoUser: {
          scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task
should be specified if stricter isolation between tasks is required. For
example, if the task mutates the registry in a way which could impact other
tasks, or if certificates have been specified on the pool which should not be
accessible by normal tasks but should be accessible by StartTasks.
          elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
      }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
      runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job
Manager is running. If false, other Tasks can run simultaneously with the Job
Manager on a Compute Node. The Job Manager Task counts normally against the
Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute
Node allows multiple concurrent Tasks. The default value is true.
      applicationPackageReferences: [ApplicationPackageReference], # Optional. Application Packages are downloaded and deployed to a shared directory, not the
Task working directory. Therefore, if a referenced Application Package is
already on the Compute Node, and is up to date, then it is not re-downloaded;
the existing copy on the Compute Node is used. If a referenced Application
Package cannot be installed, for example because the package has been deleted
or because download failed, the Task fails.
      authenticationTokenSettings: {
        access: [&quot;job&quot;], # Optional. The authentication token grants access to a limited set of Batch service
operations. Currently the only supported value for the access property is
&apos;job&apos;, which grants access to all operations related to the Job which contains
the Task.
      }, # Optional. If this property is set, the Batch service provides the Task with an
authentication token which can be used to authenticate Batch service operations
without requiring an Account access key. The token is provided via the
AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the
Task can carry out using the token depend on the settings. For example, a Task
can request Job permissions in order to add other Tasks to the Job, or check
the status of the Job or of other Tasks under the Job.
      allowLowPriorityNode: boolean, # Optional. The default value is true.
    }, # Optional. If the Job does not specify a Job Manager Task, the user must explicitly add
Tasks to the Job using the Task API. If the Job does specify a Job Manager
Task, the Batch service creates the Job Manager Task when the Job is created,
and will try to schedule the Job Manager Task before scheduling other Tasks in
the Job.
    jobPreparationTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job
Preparation Task. If you try to submit a Task with the same id, the Batch
service rejects the request with error code TaskIdSameAsJobPreparationTask; if
you are calling the REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory. 
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
      constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
      waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries
the Job Preparation Task up to its maximum retry count (as specified in the
constraints element). If the Task has still not completed successfully after
all retries, then the Batch service will not schedule Tasks of the Job to the
Node. The Node remains active and eligible to run Tasks of other Jobs. If
false, the Batch service will not wait for the Job Preparation Task to
complete. In this case, other Tasks of the Job can start executing on the
Compute Node while the Job Preparation Task is still running; and even if the
Job Preparation Task fails, new Tasks will continue to be scheduled on the
Compute Node. The default value is true.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on
Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux
Compute Nodes.
      rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if
the Job Preparation Task did not complete (e.g. because the reboot occurred
while the Task was running). Therefore, you should always write a Job
Preparation Task to be idempotent and to behave correctly if run multiple
times. The default value is true.
    }, # Optional. If a Job has a Job Preparation Task, the Batch service will run the Job
Preparation Task on a Node before starting any Tasks of that Job on that
Compute Node.
    jobReleaseTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release
Task. If you try to submit a Task with the same id, the Batch service rejects
the request with error code TaskIdSameAsJobReleaseTask; if you are calling the
REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute
Node, measured from the time the Task starts. If the Task does not complete
within the time limit, the Batch service terminates it. The default value is 15
minutes. You may not specify a timeout longer than 15 minutes. If you do, the
Batch service rejects it with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
      retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
    }, # Optional. The primary purpose of the Job Release Task is to undo changes to Nodes made by
the Job Preparation Task. Example activities include deleting local files, or
shutting down services that were started as part of Job preparation. A Job
Release Task cannot be specified without also specifying a Job Preparation Task
for the Job. The Batch service runs the Job Release Task on the Compute Nodes
that have run the Job Preparation Task.
    commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by
specifying the same setting name with a different value.
    poolInfo: {
      poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool
does not exist at the time the Batch service tries to schedule a Job, no Tasks
for the Job will run until you create a Pool with that id. Note that the Batch
service will not reject the Job request; it will simply not run Tasks until the
Pool exists. You must specify either the Pool ID or the auto Pool
specification, but not both.
      autoPoolSpecification: {
        autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To
distinguish between Pools created for different purposes, you can specify this
element to add a prefix to the ID that is assigned. The prefix can be up to 20
characters long.
        poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule
are assigned to Pools.
        keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined
by the poolLifetimeOption setting) expires; that is, when the Job or Job
Schedule completes. If true, the Batch service does not delete the Pool
automatically. It is up to the user to delete auto Pools created with this
option.
        pool: {
          displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up
to a maximum length of 1024.
          vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose
a VM size for Compute Nodes in an Azure Batch Pool
(https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
          cloudServiceConfiguration: {
            osFamily: string, # Required. Possible values are:
2 - OS Family 2, equivalent to Windows Server 2008 R2
SP1.
3 - OS Family 3, equivalent to Windows Server 2012.
4 - OS Family 4,
equivalent to Windows Server 2012 R2.
5 - OS Family 5, equivalent to Windows
Server 2016.
6 - OS Family 6, equivalent to Windows Server 2019. For more
information, see Azure Guest OS Releases
(https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
            osVersion: string, # Optional. The default value is * which specifies the latest operating system version for
the specified OS family.
          }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS
VMs. This property and virtualMachineConfiguration are mutually exclusive and
one of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request). This property cannot be specified if the
Batch Account was created with its poolAllocationMode property set to
&apos;UserSubscription&apos;.
          virtualMachineConfiguration: {
            imageReference: {
              publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
              offer: string, # Optional. For example, UbuntuServer or WindowsServer.
              sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
              version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image.
If omitted, the default is &apos;latest&apos;.
              virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The
Shared Image Gallery Image must have replicas in the same region and must be in
the same subscription as the Azure Batch account. If the image version is not
specified in the imageId, the latest version will be used. For information
about the firewall settings for the Batch Compute Node agent to communicate
with the Batch service see
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
              exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create
the node. This read-only field differs from &apos;version&apos; only if the value
specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
            }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image
Gallery Image. To get the list of all Azure Marketplace Image references
verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
            nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the
Pool, and provides the command-and-control interface between the Compute Node
and the Batch service. There are different implementations of the Compute Node
agent, known as SKUs, for different operating systems. You must specify a
Compute Node agent SKU which matches the selected Image reference. To get the
list of supported Compute Node agent SKUs along with their list of verified
Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
            windowsConfiguration: {
              enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
            }, # Optional. This property must not be specified if the imageReference property specifies a
Linux OS Image.
            dataDisks: [DataDisk], # Optional. This property must be specified if the Compute Nodes in the Pool need to have
empty data disks attached to them. This cannot be updated. Each Compute Node
gets its own disk (the disk is not a file share). Existing disks cannot be
attached, each attached disk is empty. When the Compute Node is removed from
the Pool, the disk and all data associated with it is also deleted. The disk is
not formatted after being attached, it must be formatted before use - for more
information see
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux
and
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
            licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and
should only be used when you hold valid on-premises licenses for the Compute
Nodes which will be deployed. If omitted, no on-premises licensing discount is
applied. Values are:

 Windows_Server - The on-premises license is for Windows
Server.
 Windows_Client - The on-premises license is for Windows Client.

            containerConfiguration: {
              type: &quot;dockerCompatible&quot;, # Required. The container technology to be used.
              containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An
Image will be sourced from the default Docker registry unless the Image is
fully qualified with an alternative registry.
              containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires
credentials, then those credentials must be provided here.
            }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow
Tasks to run in containers. All regular Tasks and Job manager Tasks run on this
Pool must specify the containerSettings property, and all other Tasks may
specify it.
            diskEncryptionConfiguration: {
              targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On
Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot;
and &quot;TemporaryDisk&quot; must be specified.
            }, # Optional. If specified, encryption is performed on each node in the pool during node
provisioning.
            nodePlacementConfiguration: {
              policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not
specified, Batch will use the regional policy.
            }, # Optional. This configuration will specify rules on how nodes in the pool will be
physically allocated.
            extensions: [VMExtension], # Optional. If specified, the extensions mentioned in this configuration will be installed
on each node.
            osDisk: {
              ephemeralOSDiskSettings: {
                placement: &quot;cachedisk&quot;, # Optional. This property can be used by user in the request to choose the location e.g.,
cache disk space for Ephemeral OS disk provisioning. For more information on
Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size
requirements for Windows VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements
and Linux VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
              }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the
compute node (VM).
            }, # Optional. Settings for the operating system disk of the compute node (VM).
          }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS
VMs. This property and cloudServiceConfiguration are mutually exclusive and one
of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request).
          taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number
of cores of the vmSize of the pool or 256.
          taskSchedulingPolicy: {
            nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
          }, # Optional. If not specified, the default is spread.
          resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when
enableAutoScale is set to true. The default value is 15 minutes. The minimum
value is 5 minutes. If you specify a value less than 5 minutes, the Batch
service rejects the request with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
          targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must
be specified. If true, the autoScaleFormula element is required. The Pool
automatically resizes according to the formula. The default value is false.
          autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is
required if enableAutoScale is set to true. The formula is checked for validity
before the Pool is created. If the formula is not valid, the Batch service
rejects the request with detailed error information.
          autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes
and 168 hours respectively. If you specify a value less than 5 minutes or
greater than 168 hours, the Batch service rejects the request with an invalid
property value error; if you are calling the REST API directly, the HTTP status
code is 400 (Bad Request).
          enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to
deployment restrictions on the Compute Nodes of the Pool. This may result in
the Pool not reaching its desired size. The default value is false.
          networkConfiguration: {
            subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have
enough free IP addresses, the Pool will partially allocate Nodes and a resize
error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the
&apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for
the specified VNet. The specified subnet must allow communication from the
Azure Batch service to be able to schedule Tasks on the Nodes. This can be
verified by checking if the specified VNet has any associated Network Security
Groups (NSG). If communication to the Nodes in the specified subnet is denied
by an NSG, then the Batch service will set the state of the Compute Nodes to
unusable. For Pools created with virtualMachineConfiguration only ARM virtual
networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools
created with cloudServiceConfiguration both ARM and classic virtual networks
are supported. If the specified VNet has any associated Network Security Groups
(NSG), then a few reserved system ports must be enabled for inbound
communication. For Pools created with a virtual machine configuration, enable
ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows.
For Pools created with a cloud service configuration, enable ports 10100,
20100, and 30100. Also enable outbound connections to Azure Storage on port
443. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
            dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
            endpointConfiguration: {
              inboundNATPools: [InboundNATPool], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum
number of inbound NAT Pools is exceeded the request fails with HTTP status code
400. This cannot be specified if the IPAddressProvisioningType is
NoPublicIPAddresses.
            }, # Optional. Pool endpoint configuration is only supported on Pools with the
virtualMachineConfiguration property.
            publicIPAddressConfiguration: {
              provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
              ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100
dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public
IP. For example, a pool needing 250 dedicated VMs would need at least 3 public
IPs specified. Each element of this collection is of the form:
/subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
            }, # Optional. Public IP configuration property is only supported on Pools with the
virtualMachineConfiguration property.
          }, # Optional. The network configuration for a Pool.
          startTask: {
            commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
            containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
            resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
            environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
            userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
            maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this
value specifically controls the number of retries. The Batch service will try
the Task once, and may then retry up to this limit. For example, if the maximum
retry count is 3, Batch tries the Task up to 4 times (one initial try and 3
retries). If the maximum retry count is 0, the Batch service does not retry the
Task. If the maximum retry count is -1, the Batch service retries the Task
without limit, however this is not recommended for a start task or any task.
The default value is 0 (no retries)
            waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the
StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has
still not completed successfully after all retries, then the Batch service
marks the Node unusable, and will not schedule Tasks to it. This condition can
be detected via the Compute Node state and failure info details. If false, the
Batch service will not wait for the StartTask to complete. In this case, other
Tasks can start executing on the Compute Node while the StartTask is still
running; and even if the StartTask fails, new Tasks will continue to be
scheduled on the Compute Node. The default is true.
          }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node.
Examples of recovery operations include (but are not limited to) when an
unhealthy Node is rebooted or a Compute Node disappeared due to host failure.
Retries due to recovery operations are independent of and are not counted
against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal
retry due to a recovery operation may occur. Because of this, all Tasks should
be idempotent. This means Tasks need to tolerate being interrupted and
restarted without causing any corruption or duplicate data. The best practice
for long running Tasks is to use some form of checkpointing. In some cases the
StartTask may be re-run even though the Compute Node was not rebooted. Special
care should be taken to avoid StartTasks which create breakaway process or
install/launch services from the StartTask working directory, as this will
block Batch from being able to re-run the StartTask.
          certificateReferences: [CertificateReference], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified
Certificate store and location. For Linux Compute Nodes, the Certificates are
stored in a directory inside the Task working directory and an environment
variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this
location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory
is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and
Certificates are placed in that directory.
          applicationPackageReferences: [ApplicationPackageReference], # Optional. When creating a pool, the package&apos;s application ID must be fully qualified
(/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationName}).
Changes to Package references affect all new Nodes joining the Pool, but do not
affect Compute Nodes that are already in the Pool until they are rebooted or
reimaged. There is a maximum of 10 Package references on any given Pool.
          applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service
application licenses. If a license is requested which is not supported, Pool
creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;,
&apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application
license added to the Pool.
          userAccounts: [UserAccount], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
          metadata: [
            {
              name: string, # Required. The name of the metadata item.
              value: string, # Required. The value of the metadata item.
            }
          ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
          mountConfiguration: [MountConfiguration], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
          targetNodeCommunicationMode: &quot;default&quot; | &quot;classic&quot; | &quot;simplified&quot;, # Optional. If omitted, the default value is Default.
        }, # Optional. Specification for creating a new Pool.
      }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed
state, and the Pool creation error is set in the Job&apos;s scheduling error
property. The Batch service manages the lifetime (both creation and, unless
keepAlive is specified, deletion) of the auto Pool. Any user actions that
affect the lifetime of the auto Pool while the Job is active will result in
unexpected behavior. You must specify either the Pool ID or the auto Pool
specification, but not both.
    }, # Required. Specifies how a Job should be assigned to a Pool.
    metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  }, # Optional. Specifies details of the Jobs to be created on a schedule.
  executionInfo: {
    nextRunTime: string (date &amp; time), # Optional. This property is meaningful only if the schedule is in the active state when
the time comes around. For example, if the schedule is disabled, no Job will be
created at nextRunTime unless the Job is enabled before then.
    recentJob: {
      id: string, # Optional. The ID of the Job.
      url: string, # Optional. The URL of the Job.
    }, # Optional. This property is present only if the at least one Job has run under the
schedule.
    endTime: string (date &amp; time), # Optional. This property is set only if the Job Schedule is in the completed state.
  }, # Optional. Contains information about Jobs that have been and will be run under a Job
Schedule.
  metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  stats: {
    url: string, # Required. The URL of the statistics.
    startTime: string (date &amp; time), # Required. The start time of the time range covered by the statistics.
    lastUpdateTime: string (date &amp; time), # Required. The time at which the statistics were last updated. All statistics are limited
to the range between startTime and lastUpdateTime.
    userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    wallClockTime: string (duration ISO 8601 Format), # Required. The wall clock time is the elapsed time from when the Task started running on a
Compute Node to when it finished (or to the last time the statistics were
updated, if the Task had not finished by then). If a Task was retried, this
includes the wall clock time of all the Task retries.
    readIOps: number, # Required. The total number of disk read operations made by all Tasks in all Jobs created
under the schedule.
    writeIOps: number, # Required. The total number of disk write operations made by all Tasks in all Jobs created
under the schedule.
    readIOGiB: number, # Required. The total gibibytes read from disk by all Tasks in all Jobs created under the
schedule.
    writeIOGiB: number, # Required. The total gibibytes written to disk by all Tasks in all Jobs created under the
schedule.
    numSucceededTasks: number, # Required. The total number of Tasks successfully completed during the given time range in
Jobs created under the schedule. A Task completes successfully if it returns
exit code 0.
    numFailedTasks: number, # Required. The total number of Tasks that failed during the given time range in Jobs
created under the schedule. A Task fails if it exhausts its maximum retry count
without returning exit code 0.
    numTaskRetries: number, # Required. The total number of retries during the given time range on all Tasks in all
Jobs created under the schedule.
    waitTime: string (duration ISO 8601 Format), # Required. This value is only reported in the Account lifetime statistics; it is not
included in the Job statistics.
  }, # Optional. Resource usage statistics for a Job Schedule.
}
</code>

</remarks>
    </member>
    <member name="PatchAsync(String,RequestContent,Int32,String,Boolean,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call PatchAsync with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

var data = new {};

Response response = await client.PatchAsync("<jobScheduleId>", RequestContent.Create(data));
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call PatchAsync with all parameters and request content.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

var data = new {
    id = "<id>",
    displayName = "<displayName>",
    schedule = new {
        doNotRunUntil = "2022-05-10T14:57:31.2311892-04:00",
        doNotRunAfter = "2022-05-10T14:57:31.2311892-04:00",
        startWindow = PT1H23M45S,
        recurrenceInterval = PT1H23M45S,
    },
    jobSpecification = new {
        priority = 1234,
        allowTaskPreemption = true,
        maxParallelTasks = 1234,
        displayName = "<displayName>",
        usesTaskDependencies = true,
        onAllTasksComplete = "noaction",
        onTaskFailure = "noaction",
        networkConfiguration = new {
            subnetId = "<subnetId>",
        },
        constraints = new {
            maxWallClockTime = PT1H23M45S,
            maxTaskRetryCount = 1234,
        },
        jobManagerTask = new {
            id = "<id>",
            displayName = "<displayName>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            outputFiles = new[] {
                new {
                    filePattern = "<filePattern>",
                    destination = new {
                        container = new {
                            path = "<path>",
                            containerUrl = "<containerUrl>",
                            identityReference = new {
                                resourceId = "<resourceId>",
                            },
                            uploadHeaders = new[] {
                                new {
                                    name = "<name>",
                                    value = "<value>",
                                }
                            },
                        },
                    },
                    uploadOptions = new {
                        uploadCondition = "tasksuccess",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            constraints = new {
                maxWallClockTime = PT1H23M45S,
                retentionTime = PT1H23M45S,
                maxTaskRetryCount = 1234,
            },
            requiredSlots = 1234,
            killJobOnCompletion = true,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
            runExclusive = true,
            applicationPackageReferences = new[] {
                new {
                    applicationId = "<applicationId>",
                    version = "<version>",
                }
            },
            authenticationTokenSettings = new {
                access = new[] {
                    "job"
                },
            },
            allowLowPriorityNode = true,
        },
        jobPreparationTask = new {
            id = "<id>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            constraints = new {
                maxWallClockTime = PT1H23M45S,
                retentionTime = PT1H23M45S,
                maxTaskRetryCount = 1234,
            },
            waitForSuccess = true,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
            rerunOnNodeRebootAfterSuccess = true,
        },
        jobReleaseTask = new {
            id = "<id>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            maxWallClockTime = PT1H23M45S,
            retentionTime = PT1H23M45S,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
        },
        commonEnvironmentSettings = new[] {
            new {
                name = "<name>",
                value = "<value>",
            }
        },
        poolInfo = new {
            poolId = "<poolId>",
            autoPoolSpecification = new {
                autoPoolIdPrefix = "<autoPoolIdPrefix>",
                poolLifetimeOption = "jobschedule",
                keepAlive = true,
                pool = new {
                    displayName = "<displayName>",
                    vmSize = "<vmSize>",
                    cloudServiceConfiguration = new {
                        osFamily = "<osFamily>",
                        osVersion = "<osVersion>",
                    },
                    virtualMachineConfiguration = new {
                        imageReference = new {
                            publisher = "<publisher>",
                            offer = "<offer>",
                            sku = "<sku>",
                            version = "<version>",
                            virtualMachineImageId = "<virtualMachineImageId>",
                        },
                        nodeAgentSKUId = "<nodeAgentSKUId>",
                        windowsConfiguration = new {
                            enableAutomaticUpdates = true,
                        },
                        dataDisks = new[] {
                            new {
                                lun = 1234,
                                caching = "none",
                                diskSizeGB = 1234,
                                storageAccountType = "standard_lrs",
                            }
                        },
                        licenseType = "<licenseType>",
                        containerConfiguration = new {
                            type = "dockerCompatible",
                            containerImageNames = new[] {
                                "<String>"
                            },
                            containerRegistries = new[] {
                                new {
                                    username = "<username>",
                                    password = "<password>",
                                    registryServer = "<registryServer>",
                                    identityReference = new {
                                        resourceId = "<resourceId>",
                                    },
                                }
                            },
                        },
                        diskEncryptionConfiguration = new {
                            targets = new[] {
                                "osdisk"
                            },
                        },
                        nodePlacementConfiguration = new {
                            policy = "regional",
                        },
                        extensions = new[] {
                            new {
                                name = "<name>",
                                publisher = "<publisher>",
                                type = "<type>",
                                typeHandlerVersion = "<typeHandlerVersion>",
                                autoUpgradeMinorVersion = true,
                                settings = new {},
                                protectedSettings = new {},
                                provisionAfterExtensions = new[] {
                                    "<String>"
                                },
                            }
                        },
                        osDisk = new {
                            ephemeralOSDiskSettings = new {
                                placement = "cachedisk",
                            },
                        },
                    },
                    taskSlotsPerNode = 1234,
                    taskSchedulingPolicy = new {
                        nodeFillType = "spread",
                    },
                    resizeTimeout = PT1H23M45S,
                    targetDedicatedNodes = 1234,
                    targetLowPriorityNodes = 1234,
                    enableAutoScale = true,
                    autoScaleFormula = "<autoScaleFormula>",
                    autoScaleEvaluationInterval = PT1H23M45S,
                    enableInterNodeCommunication = true,
                    networkConfiguration = new {
                        subnetId = "<subnetId>",
                        dynamicVNetAssignmentScope = "none",
                        endpointConfiguration = new {
                            inboundNATPools = new[] {
                                new {
                                    name = "<name>",
                                    protocol = "tcp",
                                    backendPort = 1234,
                                    frontendPortRangeStart = 1234,
                                    frontendPortRangeEnd = 1234,
                                    networkSecurityGroupRules = new[] {
                                        new {
                                            priority = 1234,
                                            access = "allow",
                                            sourceAddressPrefix = "<sourceAddressPrefix>",
                                            sourcePortRanges = new[] {
                                                "<String>"
                                            },
                                        }
                                    },
                                }
                            },
                        },
                        publicIPAddressConfiguration = new {
                            provision = "batchmanaged",
                            ipAddressIds = new[] {
                                "<String>"
                            },
                        },
                    },
                    startTask = new {
                        commandLine = "<commandLine>",
                        containerSettings = new {
                            containerRunOptions = "<containerRunOptions>",
                            imageName = "<imageName>",
                            registry = new {
                                username = "<username>",
                                password = "<password>",
                                registryServer = "<registryServer>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            },
                            workingDirectory = "taskWorkingDirectory",
                        },
                        resourceFiles = new[] {
                            new {
                                autoStorageContainerName = "<autoStorageContainerName>",
                                storageContainerUrl = "<storageContainerUrl>",
                                httpUrl = "<httpUrl>",
                                blobPrefix = "<blobPrefix>",
                                filePath = "<filePath>",
                                fileMode = "<fileMode>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            }
                        },
                        environmentSettings = new[] {
                            new {
                                name = "<name>",
                                value = "<value>",
                            }
                        },
                        userIdentity = new {
                            username = "<username>",
                            autoUser = new {
                                scope = "task",
                                elevationLevel = "nonadmin",
                            },
                        },
                        maxTaskRetryCount = 1234,
                        waitForSuccess = true,
                    },
                    certificateReferences = new[] {
                        new {
                            thumbprint = "<thumbprint>",
                            thumbprintAlgorithm = "<thumbprintAlgorithm>",
                            storeLocation = "currentuser",
                            storeName = "<storeName>",
                            visibility = new[] {
                                "starttask"
                            },
                        }
                    },
                    applicationPackageReferences = new[] {
                        new {
                            applicationId = "<applicationId>",
                            version = "<version>",
                        }
                    },
                    applicationLicenses = new[] {
                        "<String>"
                    },
                    userAccounts = new[] {
                        new {
                            name = "<name>",
                            password = "<password>",
                            elevationLevel = "nonadmin",
                            linuxUserConfiguration = new {
                                uid = 1234,
                                gid = 1234,
                                sshPrivateKey = "<sshPrivateKey>",
                            },
                            windowsUserConfiguration = new {
                                loginMode = "batch",
                            },
                        }
                    },
                    metadata = new[] {
                        new {
                            name = "<name>",
                            value = "<value>",
                        }
                    },
                    mountConfiguration = new[] {
                        new {
                            azureBlobFileSystemConfiguration = new {
                                accountName = "<accountName>",
                                containerName = "<containerName>",
                                accountKey = "<accountKey>",
                                sasKey = "<sasKey>",
                                blobfuseOptions = "<blobfuseOptions>",
                                relativeMountPath = "<relativeMountPath>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            },
                            nfsMountConfiguration = new {
                                source = "<source>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                            },
                            cifsMountConfiguration = new {
                                username = "<username>",
                                source = "<source>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                                password = "<password>",
                            },
                            azureFileShareConfiguration = new {
                                accountName = "<accountName>",
                                azureFileUrl = "<azureFileUrl>",
                                accountKey = "<accountKey>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                            },
                        }
                    },
                    targetNodeCommunicationMode = "default",
                },
            },
        },
        metadata = new[] {
            new {
                name = "<name>",
                value = "<value>",
            }
        },
    },
    metadata = new[] {
        new {
            name = "<name>",
            value = "<value>",
        }
    },
};

Response response = await client.PatchAsync("<jobScheduleId>", RequestContent.Create(data), 1234, "<clientRequestId>", true, "<ocpDate>", null);
Console.WriteLine(response.Status);
]]></code>
</example>
<remarks>
This replaces only the Job Schedule properties specified in the request. For
example, if the schedule property is not specified with this request, then the
Batch service will keep the existing schedule. Changes to a Job Schedule only
impact Jobs created by the schedule after the update has taken place; currently
running Jobs are unaffected.

Below is the JSON schema for the request payload.

Request Body:

Schema for <c>BatchJobSchedule</c>:
<code>{
  id: string, # Optional. A string that uniquely identifies the schedule within the Account.
  displayName: string, # Optional. The display name for the schedule.
  url: string, # Optional. The URL of the Job Schedule.
  eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job Schedule has
changed between requests. In particular, you can be pass the ETag with an
Update Job Schedule request to specify that your changes should take effect
only if nobody else has modified the schedule in the meantime.
  lastModified: string (date &amp; time), # Optional. This is the last time at which the schedule level data, such as the Job
specification or recurrence information, changed. It does not factor in
job-level changes such as new Jobs being created or Jobs changing state.
  creationTime: string (date &amp; time), # Optional. The creation time of the Job Schedule.
  state: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. The state of the Job Schedule.
  stateTransitionTime: string (date &amp; time), # Optional. The time at which the Job Schedule entered the current state.
  previousState: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. This property is not present if the Job Schedule is in its initial active state.
  previousStateTransitionTime: string (date &amp; time), # Optional. This property is not present if the Job Schedule is in its initial active state.
  schedule: {
    doNotRunUntil: string (date &amp; time), # Optional. If you do not specify a doNotRunUntil time, the schedule becomes ready to
create Jobs immediately.
    doNotRunAfter: string (date &amp; time), # Optional. If you do not specify a doNotRunAfter time, and you are creating a recurring
Job Schedule, the Job Schedule will remain active until you explicitly
terminate it.
    startWindow: string (duration ISO 8601 Format), # Optional. If a Job is not created within the startWindow interval, then the &apos;opportunity&apos;
is lost; no Job will be created until the next recurrence of the schedule. If
the schedule is recurring, and the startWindow is longer than the recurrence
interval, then this is equivalent to an infinite startWindow, because the Job
that is &apos;due&apos; in one recurrenceInterval is not carried forward into the next
recurrence interval. The default is infinite. The minimum value is 1 minute. If
you specify a lower value, the Batch service rejects the schedule with an
error; if you are calling the REST API directly, the HTTP status code is 400
(Bad Request).
    recurrenceInterval: string (duration ISO 8601 Format), # Optional. Because a Job Schedule can have at most one active Job under it at any given
time, if it is time to create a new Job under a Job Schedule, but the previous
Job is still running, the Batch service will not create the new Job until the
previous Job finishes. If the previous Job does not finish within the
startWindow period of the new recurrenceInterval, then no new Job will be
scheduled for that interval. For recurring Jobs, you should normally specify a
jobManagerTask in the jobSpecification. If you do not use jobManagerTask, you
will need an external process to monitor when Jobs are created, add Tasks to
the Jobs and terminate the Jobs ready for the next recurrence. The default is
that the schedule does not recur: one Job is created, within the startWindow
after the doNotRunUntil time, and the schedule is complete as soon as that Job
finishes. The minimum value is 1 minute. If you specify a lower value, the
Batch service rejects the schedule with an error; if you are calling the REST
API directly, the HTTP status code is 400 (Bad Request).
  }, # Optional. All times are fixed respective to UTC and are not impacted by daylight saving
time.
  jobSpecification: {
    priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest
priority and 1000 being the highest priority. The default value is 0. This
priority is used as the default for all Jobs under the Job Schedule. You can
update a Job&apos;s priority after it has been created using by using the update Job
API.
    allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system
will take precedence and will be able requeue tasks from this job. You can
update a job&apos;s allowTaskPreemption after it has been created using the update
job API.
    maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not
specified, the default value is -1, which means there&apos;s no limit to the number
of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after
it has been created using the update job API.
    displayName: string, # Optional. The name need not be unique and can contain any Unicode characters up to a
maximum length of 1024.
    usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is
false.
    onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. Note that if a Job contains no Tasks, then all Tasks are considered complete.
This option is therefore most commonly used with a Job Manager task; if you
want to use automatic Job termination without a Job Manager, you should
initially set onAllTasksComplete to noaction and update the Job properties to
set onAllTasksComplete to terminatejob once you have finished adding Tasks. The
default is noaction.
    onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. The default is noaction.
    networkConfiguration: {
      subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes which will run Tasks from the Job. This
can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos;
service principal must have the &apos;Classic Virtual Machine Contributor&apos;
Role-Based Access Control (RBAC) role for the specified VNet so that Azure
Batch service can schedule Tasks on the Nodes. This can be verified by checking
if the specified VNet has any associated Network Security Groups (NSG). If
communication to the Nodes in the specified subnet is denied by an NSG, then
the Batch service will set the state of the Compute Nodes to unusable. This is
of the form
/subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}.
If the specified VNet has any associated Network Security Groups (NSG), then a
few reserved system ports must be enabled for inbound communication from the
Azure Batch service. For Pools created with a Virtual Machine configuration,
enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for
Windows. Port 443 is also required to be open for outbound connections for
communications to Azure Storage. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
    }, # Optional. The network configuration for the Job.
    constraints: {
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service
terminates it and any Tasks that are still running. In this case, the
termination reason will be MaxWallClockTimeExpiry. If this property is not
specified, there is no time limit on how long the Job may run.
      maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch
service will try each Task once, and may then retry up to this limit. For
example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one
initial try and 3 retries). If the maximum retry count is 0, the Batch service
does not retry Tasks. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
    }, # Optional. The execution constraints for a Job.
    jobManagerTask: {
      id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters.
      displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum
length of 1024.
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: {
        containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot;
command, in addition to those controlled by the Batch Service.
        imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If
no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a
default.
        registry: {
          username: string, # Optional. The user name to log into the registry server.
          password: string, # Optional. The password to log into the registry server.
          registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
          identityReference: {
            resourceId: string, # Optional. The ARM resource id of the user assigned identity.
          }, # Optional. The reference to a user assigned identity associated with the Batch pool which
a compute node will use.
        }, # Optional. This setting can be omitted if was already provided at Pool creation.
        workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
      }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must
be set as well. If the Pool that will run this Task doesn&apos;t have
containerConfiguration set, this must not be set. When this is specified, all
directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure
Batch directories on the node) are mapped into the container, all Task
environment variables are mapped into the container, and the Task command line
is executed in the container. Files produced in the container outside of
AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that
Batch file APIs will not be able to access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      outputFiles: [OutputFile], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node
on which the primary Task is executed.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Manager Task.
      constraints: {
        maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
        maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task
executable due to a nonzero exit code. The Batch service will try the Task
once, and may then retry up to this limit. For example, if the maximum retry
count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries).
If the maximum retry count is 0, the Batch service does not retry the Task
after the first attempt. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
      }, # Optional. Execution constraints to apply to a Task.
      requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the
node has enough free scheduling slots available. For multi-instance Tasks, this
property is not supported and must not be specified.
      killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job
as complete. If any Tasks are still running at this time (other than Job
Release), those Tasks are terminated. If false, the completion of the Job
Manager Task does not affect the Job status. In this case, you should either
use the onAllTasksComplete attribute to terminate the Job, or have a client or
user terminate the Job explicitly. An example of this is if the Job Manager
creates a set of Tasks but then takes no further role in their execution. The
default value is true. If you are using the onAllTasksComplete and
onTaskFailure attributes to control Job lifetime, and using the Job Manager
Task only to create the Tasks for the Job (not to monitor progress), then it is
important to set killJobOnCompletion to false.
      userIdentity: {
        username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
        autoUser: {
          scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task
should be specified if stricter isolation between tasks is required. For
example, if the task mutates the registry in a way which could impact other
tasks, or if certificates have been specified on the pool which should not be
accessible by normal tasks but should be accessible by StartTasks.
          elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
      }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
      runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job
Manager is running. If false, other Tasks can run simultaneously with the Job
Manager on a Compute Node. The Job Manager Task counts normally against the
Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute
Node allows multiple concurrent Tasks. The default value is true.
      applicationPackageReferences: [ApplicationPackageReference], # Optional. Application Packages are downloaded and deployed to a shared directory, not the
Task working directory. Therefore, if a referenced Application Package is
already on the Compute Node, and is up to date, then it is not re-downloaded;
the existing copy on the Compute Node is used. If a referenced Application
Package cannot be installed, for example because the package has been deleted
or because download failed, the Task fails.
      authenticationTokenSettings: {
        access: [&quot;job&quot;], # Optional. The authentication token grants access to a limited set of Batch service
operations. Currently the only supported value for the access property is
&apos;job&apos;, which grants access to all operations related to the Job which contains
the Task.
      }, # Optional. If this property is set, the Batch service provides the Task with an
authentication token which can be used to authenticate Batch service operations
without requiring an Account access key. The token is provided via the
AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the
Task can carry out using the token depend on the settings. For example, a Task
can request Job permissions in order to add other Tasks to the Job, or check
the status of the Job or of other Tasks under the Job.
      allowLowPriorityNode: boolean, # Optional. The default value is true.
    }, # Optional. If the Job does not specify a Job Manager Task, the user must explicitly add
Tasks to the Job using the Task API. If the Job does specify a Job Manager
Task, the Batch service creates the Job Manager Task when the Job is created,
and will try to schedule the Job Manager Task before scheduling other Tasks in
the Job.
    jobPreparationTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job
Preparation Task. If you try to submit a Task with the same id, the Batch
service rejects the request with error code TaskIdSameAsJobPreparationTask; if
you are calling the REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory. 
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
      constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
      waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries
the Job Preparation Task up to its maximum retry count (as specified in the
constraints element). If the Task has still not completed successfully after
all retries, then the Batch service will not schedule Tasks of the Job to the
Node. The Node remains active and eligible to run Tasks of other Jobs. If
false, the Batch service will not wait for the Job Preparation Task to
complete. In this case, other Tasks of the Job can start executing on the
Compute Node while the Job Preparation Task is still running; and even if the
Job Preparation Task fails, new Tasks will continue to be scheduled on the
Compute Node. The default value is true.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on
Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux
Compute Nodes.
      rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if
the Job Preparation Task did not complete (e.g. because the reboot occurred
while the Task was running). Therefore, you should always write a Job
Preparation Task to be idempotent and to behave correctly if run multiple
times. The default value is true.
    }, # Optional. If a Job has a Job Preparation Task, the Batch service will run the Job
Preparation Task on a Node before starting any Tasks of that Job on that
Compute Node.
    jobReleaseTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release
Task. If you try to submit a Task with the same id, the Batch service rejects
the request with error code TaskIdSameAsJobReleaseTask; if you are calling the
REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute
Node, measured from the time the Task starts. If the Task does not complete
within the time limit, the Batch service terminates it. The default value is 15
minutes. You may not specify a timeout longer than 15 minutes. If you do, the
Batch service rejects it with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
      retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
    }, # Optional. The primary purpose of the Job Release Task is to undo changes to Nodes made by
the Job Preparation Task. Example activities include deleting local files, or
shutting down services that were started as part of Job preparation. A Job
Release Task cannot be specified without also specifying a Job Preparation Task
for the Job. The Batch service runs the Job Release Task on the Compute Nodes
that have run the Job Preparation Task.
    commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by
specifying the same setting name with a different value.
    poolInfo: {
      poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool
does not exist at the time the Batch service tries to schedule a Job, no Tasks
for the Job will run until you create a Pool with that id. Note that the Batch
service will not reject the Job request; it will simply not run Tasks until the
Pool exists. You must specify either the Pool ID or the auto Pool
specification, but not both.
      autoPoolSpecification: {
        autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To
distinguish between Pools created for different purposes, you can specify this
element to add a prefix to the ID that is assigned. The prefix can be up to 20
characters long.
        poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule
are assigned to Pools.
        keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined
by the poolLifetimeOption setting) expires; that is, when the Job or Job
Schedule completes. If true, the Batch service does not delete the Pool
automatically. It is up to the user to delete auto Pools created with this
option.
        pool: {
          displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up
to a maximum length of 1024.
          vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose
a VM size for Compute Nodes in an Azure Batch Pool
(https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
          cloudServiceConfiguration: {
            osFamily: string, # Required. Possible values are:
2 - OS Family 2, equivalent to Windows Server 2008 R2
SP1.
3 - OS Family 3, equivalent to Windows Server 2012.
4 - OS Family 4,
equivalent to Windows Server 2012 R2.
5 - OS Family 5, equivalent to Windows
Server 2016.
6 - OS Family 6, equivalent to Windows Server 2019. For more
information, see Azure Guest OS Releases
(https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
            osVersion: string, # Optional. The default value is * which specifies the latest operating system version for
the specified OS family.
          }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS
VMs. This property and virtualMachineConfiguration are mutually exclusive and
one of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request). This property cannot be specified if the
Batch Account was created with its poolAllocationMode property set to
&apos;UserSubscription&apos;.
          virtualMachineConfiguration: {
            imageReference: {
              publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
              offer: string, # Optional. For example, UbuntuServer or WindowsServer.
              sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
              version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image.
If omitted, the default is &apos;latest&apos;.
              virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The
Shared Image Gallery Image must have replicas in the same region and must be in
the same subscription as the Azure Batch account. If the image version is not
specified in the imageId, the latest version will be used. For information
about the firewall settings for the Batch Compute Node agent to communicate
with the Batch service see
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
              exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create
the node. This read-only field differs from &apos;version&apos; only if the value
specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
            }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image
Gallery Image. To get the list of all Azure Marketplace Image references
verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
            nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the
Pool, and provides the command-and-control interface between the Compute Node
and the Batch service. There are different implementations of the Compute Node
agent, known as SKUs, for different operating systems. You must specify a
Compute Node agent SKU which matches the selected Image reference. To get the
list of supported Compute Node agent SKUs along with their list of verified
Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
            windowsConfiguration: {
              enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
            }, # Optional. This property must not be specified if the imageReference property specifies a
Linux OS Image.
            dataDisks: [DataDisk], # Optional. This property must be specified if the Compute Nodes in the Pool need to have
empty data disks attached to them. This cannot be updated. Each Compute Node
gets its own disk (the disk is not a file share). Existing disks cannot be
attached, each attached disk is empty. When the Compute Node is removed from
the Pool, the disk and all data associated with it is also deleted. The disk is
not formatted after being attached, it must be formatted before use - for more
information see
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux
and
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
            licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and
should only be used when you hold valid on-premises licenses for the Compute
Nodes which will be deployed. If omitted, no on-premises licensing discount is
applied. Values are:

 Windows_Server - The on-premises license is for Windows
Server.
 Windows_Client - The on-premises license is for Windows Client.

            containerConfiguration: {
              type: &quot;dockerCompatible&quot;, # Required. The container technology to be used.
              containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An
Image will be sourced from the default Docker registry unless the Image is
fully qualified with an alternative registry.
              containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires
credentials, then those credentials must be provided here.
            }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow
Tasks to run in containers. All regular Tasks and Job manager Tasks run on this
Pool must specify the containerSettings property, and all other Tasks may
specify it.
            diskEncryptionConfiguration: {
              targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On
Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot;
and &quot;TemporaryDisk&quot; must be specified.
            }, # Optional. If specified, encryption is performed on each node in the pool during node
provisioning.
            nodePlacementConfiguration: {
              policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not
specified, Batch will use the regional policy.
            }, # Optional. This configuration will specify rules on how nodes in the pool will be
physically allocated.
            extensions: [VMExtension], # Optional. If specified, the extensions mentioned in this configuration will be installed
on each node.
            osDisk: {
              ephemeralOSDiskSettings: {
                placement: &quot;cachedisk&quot;, # Optional. This property can be used by user in the request to choose the location e.g.,
cache disk space for Ephemeral OS disk provisioning. For more information on
Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size
requirements for Windows VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements
and Linux VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
              }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the
compute node (VM).
            }, # Optional. Settings for the operating system disk of the compute node (VM).
          }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS
VMs. This property and cloudServiceConfiguration are mutually exclusive and one
of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request).
          taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number
of cores of the vmSize of the pool or 256.
          taskSchedulingPolicy: {
            nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
          }, # Optional. If not specified, the default is spread.
          resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when
enableAutoScale is set to true. The default value is 15 minutes. The minimum
value is 5 minutes. If you specify a value less than 5 minutes, the Batch
service rejects the request with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
          targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must
be specified. If true, the autoScaleFormula element is required. The Pool
automatically resizes according to the formula. The default value is false.
          autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is
required if enableAutoScale is set to true. The formula is checked for validity
before the Pool is created. If the formula is not valid, the Batch service
rejects the request with detailed error information.
          autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes
and 168 hours respectively. If you specify a value less than 5 minutes or
greater than 168 hours, the Batch service rejects the request with an invalid
property value error; if you are calling the REST API directly, the HTTP status
code is 400 (Bad Request).
          enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to
deployment restrictions on the Compute Nodes of the Pool. This may result in
the Pool not reaching its desired size. The default value is false.
          networkConfiguration: {
            subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have
enough free IP addresses, the Pool will partially allocate Nodes and a resize
error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the
&apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for
the specified VNet. The specified subnet must allow communication from the
Azure Batch service to be able to schedule Tasks on the Nodes. This can be
verified by checking if the specified VNet has any associated Network Security
Groups (NSG). If communication to the Nodes in the specified subnet is denied
by an NSG, then the Batch service will set the state of the Compute Nodes to
unusable. For Pools created with virtualMachineConfiguration only ARM virtual
networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools
created with cloudServiceConfiguration both ARM and classic virtual networks
are supported. If the specified VNet has any associated Network Security Groups
(NSG), then a few reserved system ports must be enabled for inbound
communication. For Pools created with a virtual machine configuration, enable
ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows.
For Pools created with a cloud service configuration, enable ports 10100,
20100, and 30100. Also enable outbound connections to Azure Storage on port
443. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
            dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
            endpointConfiguration: {
              inboundNATPools: [InboundNATPool], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum
number of inbound NAT Pools is exceeded the request fails with HTTP status code
400. This cannot be specified if the IPAddressProvisioningType is
NoPublicIPAddresses.
            }, # Optional. Pool endpoint configuration is only supported on Pools with the
virtualMachineConfiguration property.
            publicIPAddressConfiguration: {
              provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
              ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100
dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public
IP. For example, a pool needing 250 dedicated VMs would need at least 3 public
IPs specified. Each element of this collection is of the form:
/subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
            }, # Optional. Public IP configuration property is only supported on Pools with the
virtualMachineConfiguration property.
          }, # Optional. The network configuration for a Pool.
          startTask: {
            commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
            containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
            resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
            environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
            userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
            maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this
value specifically controls the number of retries. The Batch service will try
the Task once, and may then retry up to this limit. For example, if the maximum
retry count is 3, Batch tries the Task up to 4 times (one initial try and 3
retries). If the maximum retry count is 0, the Batch service does not retry the
Task. If the maximum retry count is -1, the Batch service retries the Task
without limit, however this is not recommended for a start task or any task.
The default value is 0 (no retries)
            waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the
StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has
still not completed successfully after all retries, then the Batch service
marks the Node unusable, and will not schedule Tasks to it. This condition can
be detected via the Compute Node state and failure info details. If false, the
Batch service will not wait for the StartTask to complete. In this case, other
Tasks can start executing on the Compute Node while the StartTask is still
running; and even if the StartTask fails, new Tasks will continue to be
scheduled on the Compute Node. The default is true.
          }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node.
Examples of recovery operations include (but are not limited to) when an
unhealthy Node is rebooted or a Compute Node disappeared due to host failure.
Retries due to recovery operations are independent of and are not counted
against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal
retry due to a recovery operation may occur. Because of this, all Tasks should
be idempotent. This means Tasks need to tolerate being interrupted and
restarted without causing any corruption or duplicate data. The best practice
for long running Tasks is to use some form of checkpointing. In some cases the
StartTask may be re-run even though the Compute Node was not rebooted. Special
care should be taken to avoid StartTasks which create breakaway process or
install/launch services from the StartTask working directory, as this will
block Batch from being able to re-run the StartTask.
          certificateReferences: [CertificateReference], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified
Certificate store and location. For Linux Compute Nodes, the Certificates are
stored in a directory inside the Task working directory and an environment
variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this
location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory
is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and
Certificates are placed in that directory.
          applicationPackageReferences: [ApplicationPackageReference], # Optional. When creating a pool, the package&apos;s application ID must be fully qualified
(/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationName}).
Changes to Package references affect all new Nodes joining the Pool, but do not
affect Compute Nodes that are already in the Pool until they are rebooted or
reimaged. There is a maximum of 10 Package references on any given Pool.
          applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service
application licenses. If a license is requested which is not supported, Pool
creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;,
&apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application
license added to the Pool.
          userAccounts: [UserAccount], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
          metadata: [
            {
              name: string, # Required. The name of the metadata item.
              value: string, # Required. The value of the metadata item.
            }
          ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
          mountConfiguration: [MountConfiguration], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
          targetNodeCommunicationMode: &quot;default&quot; | &quot;classic&quot; | &quot;simplified&quot;, # Optional. If omitted, the default value is Default.
        }, # Optional. Specification for creating a new Pool.
      }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed
state, and the Pool creation error is set in the Job&apos;s scheduling error
property. The Batch service manages the lifetime (both creation and, unless
keepAlive is specified, deletion) of the auto Pool. Any user actions that
affect the lifetime of the auto Pool while the Job is active will result in
unexpected behavior. You must specify either the Pool ID or the auto Pool
specification, but not both.
    }, # Required. Specifies how a Job should be assigned to a Pool.
    metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  }, # Optional. Specifies details of the Jobs to be created on a schedule.
  executionInfo: {
    nextRunTime: string (date &amp; time), # Optional. This property is meaningful only if the schedule is in the active state when
the time comes around. For example, if the schedule is disabled, no Job will be
created at nextRunTime unless the Job is enabled before then.
    recentJob: {
      id: string, # Optional. The ID of the Job.
      url: string, # Optional. The URL of the Job.
    }, # Optional. This property is present only if the at least one Job has run under the
schedule.
    endTime: string (date &amp; time), # Optional. This property is set only if the Job Schedule is in the completed state.
  }, # Optional. Contains information about Jobs that have been and will be run under a Job
Schedule.
  metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  stats: {
    url: string, # Required. The URL of the statistics.
    startTime: string (date &amp; time), # Required. The start time of the time range covered by the statistics.
    lastUpdateTime: string (date &amp; time), # Required. The time at which the statistics were last updated. All statistics are limited
to the range between startTime and lastUpdateTime.
    userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    wallClockTime: string (duration ISO 8601 Format), # Required. The wall clock time is the elapsed time from when the Task started running on a
Compute Node to when it finished (or to the last time the statistics were
updated, if the Task had not finished by then). If a Task was retried, this
includes the wall clock time of all the Task retries.
    readIOps: number, # Required. The total number of disk read operations made by all Tasks in all Jobs created
under the schedule.
    writeIOps: number, # Required. The total number of disk write operations made by all Tasks in all Jobs created
under the schedule.
    readIOGiB: number, # Required. The total gibibytes read from disk by all Tasks in all Jobs created under the
schedule.
    writeIOGiB: number, # Required. The total gibibytes written to disk by all Tasks in all Jobs created under the
schedule.
    numSucceededTasks: number, # Required. The total number of Tasks successfully completed during the given time range in
Jobs created under the schedule. A Task completes successfully if it returns
exit code 0.
    numFailedTasks: number, # Required. The total number of Tasks that failed during the given time range in Jobs
created under the schedule. A Task fails if it exhausts its maximum retry count
without returning exit code 0.
    numTaskRetries: number, # Required. The total number of retries during the given time range on all Tasks in all
Jobs created under the schedule.
    waitTime: string (duration ISO 8601 Format), # Required. This value is only reported in the Account lifetime statistics; it is not
included in the Job statistics.
  }, # Optional. Resource usage statistics for a Job Schedule.
}
</code>

</remarks>
    </member>
    <member name="Patch(String,RequestContent,Int32,String,Boolean,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call Patch with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

var data = new {};

Response response = client.Patch("<jobScheduleId>", RequestContent.Create(data));
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call Patch with all parameters and request content.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

var data = new {
    id = "<id>",
    displayName = "<displayName>",
    schedule = new {
        doNotRunUntil = "2022-05-10T14:57:31.2311892-04:00",
        doNotRunAfter = "2022-05-10T14:57:31.2311892-04:00",
        startWindow = PT1H23M45S,
        recurrenceInterval = PT1H23M45S,
    },
    jobSpecification = new {
        priority = 1234,
        allowTaskPreemption = true,
        maxParallelTasks = 1234,
        displayName = "<displayName>",
        usesTaskDependencies = true,
        onAllTasksComplete = "noaction",
        onTaskFailure = "noaction",
        networkConfiguration = new {
            subnetId = "<subnetId>",
        },
        constraints = new {
            maxWallClockTime = PT1H23M45S,
            maxTaskRetryCount = 1234,
        },
        jobManagerTask = new {
            id = "<id>",
            displayName = "<displayName>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            outputFiles = new[] {
                new {
                    filePattern = "<filePattern>",
                    destination = new {
                        container = new {
                            path = "<path>",
                            containerUrl = "<containerUrl>",
                            identityReference = new {
                                resourceId = "<resourceId>",
                            },
                            uploadHeaders = new[] {
                                new {
                                    name = "<name>",
                                    value = "<value>",
                                }
                            },
                        },
                    },
                    uploadOptions = new {
                        uploadCondition = "tasksuccess",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            constraints = new {
                maxWallClockTime = PT1H23M45S,
                retentionTime = PT1H23M45S,
                maxTaskRetryCount = 1234,
            },
            requiredSlots = 1234,
            killJobOnCompletion = true,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
            runExclusive = true,
            applicationPackageReferences = new[] {
                new {
                    applicationId = "<applicationId>",
                    version = "<version>",
                }
            },
            authenticationTokenSettings = new {
                access = new[] {
                    "job"
                },
            },
            allowLowPriorityNode = true,
        },
        jobPreparationTask = new {
            id = "<id>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            constraints = new {
                maxWallClockTime = PT1H23M45S,
                retentionTime = PT1H23M45S,
                maxTaskRetryCount = 1234,
            },
            waitForSuccess = true,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
            rerunOnNodeRebootAfterSuccess = true,
        },
        jobReleaseTask = new {
            id = "<id>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            maxWallClockTime = PT1H23M45S,
            retentionTime = PT1H23M45S,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
        },
        commonEnvironmentSettings = new[] {
            new {
                name = "<name>",
                value = "<value>",
            }
        },
        poolInfo = new {
            poolId = "<poolId>",
            autoPoolSpecification = new {
                autoPoolIdPrefix = "<autoPoolIdPrefix>",
                poolLifetimeOption = "jobschedule",
                keepAlive = true,
                pool = new {
                    displayName = "<displayName>",
                    vmSize = "<vmSize>",
                    cloudServiceConfiguration = new {
                        osFamily = "<osFamily>",
                        osVersion = "<osVersion>",
                    },
                    virtualMachineConfiguration = new {
                        imageReference = new {
                            publisher = "<publisher>",
                            offer = "<offer>",
                            sku = "<sku>",
                            version = "<version>",
                            virtualMachineImageId = "<virtualMachineImageId>",
                        },
                        nodeAgentSKUId = "<nodeAgentSKUId>",
                        windowsConfiguration = new {
                            enableAutomaticUpdates = true,
                        },
                        dataDisks = new[] {
                            new {
                                lun = 1234,
                                caching = "none",
                                diskSizeGB = 1234,
                                storageAccountType = "standard_lrs",
                            }
                        },
                        licenseType = "<licenseType>",
                        containerConfiguration = new {
                            type = "dockerCompatible",
                            containerImageNames = new[] {
                                "<String>"
                            },
                            containerRegistries = new[] {
                                new {
                                    username = "<username>",
                                    password = "<password>",
                                    registryServer = "<registryServer>",
                                    identityReference = new {
                                        resourceId = "<resourceId>",
                                    },
                                }
                            },
                        },
                        diskEncryptionConfiguration = new {
                            targets = new[] {
                                "osdisk"
                            },
                        },
                        nodePlacementConfiguration = new {
                            policy = "regional",
                        },
                        extensions = new[] {
                            new {
                                name = "<name>",
                                publisher = "<publisher>",
                                type = "<type>",
                                typeHandlerVersion = "<typeHandlerVersion>",
                                autoUpgradeMinorVersion = true,
                                settings = new {},
                                protectedSettings = new {},
                                provisionAfterExtensions = new[] {
                                    "<String>"
                                },
                            }
                        },
                        osDisk = new {
                            ephemeralOSDiskSettings = new {
                                placement = "cachedisk",
                            },
                        },
                    },
                    taskSlotsPerNode = 1234,
                    taskSchedulingPolicy = new {
                        nodeFillType = "spread",
                    },
                    resizeTimeout = PT1H23M45S,
                    targetDedicatedNodes = 1234,
                    targetLowPriorityNodes = 1234,
                    enableAutoScale = true,
                    autoScaleFormula = "<autoScaleFormula>",
                    autoScaleEvaluationInterval = PT1H23M45S,
                    enableInterNodeCommunication = true,
                    networkConfiguration = new {
                        subnetId = "<subnetId>",
                        dynamicVNetAssignmentScope = "none",
                        endpointConfiguration = new {
                            inboundNATPools = new[] {
                                new {
                                    name = "<name>",
                                    protocol = "tcp",
                                    backendPort = 1234,
                                    frontendPortRangeStart = 1234,
                                    frontendPortRangeEnd = 1234,
                                    networkSecurityGroupRules = new[] {
                                        new {
                                            priority = 1234,
                                            access = "allow",
                                            sourceAddressPrefix = "<sourceAddressPrefix>",
                                            sourcePortRanges = new[] {
                                                "<String>"
                                            },
                                        }
                                    },
                                }
                            },
                        },
                        publicIPAddressConfiguration = new {
                            provision = "batchmanaged",
                            ipAddressIds = new[] {
                                "<String>"
                            },
                        },
                    },
                    startTask = new {
                        commandLine = "<commandLine>",
                        containerSettings = new {
                            containerRunOptions = "<containerRunOptions>",
                            imageName = "<imageName>",
                            registry = new {
                                username = "<username>",
                                password = "<password>",
                                registryServer = "<registryServer>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            },
                            workingDirectory = "taskWorkingDirectory",
                        },
                        resourceFiles = new[] {
                            new {
                                autoStorageContainerName = "<autoStorageContainerName>",
                                storageContainerUrl = "<storageContainerUrl>",
                                httpUrl = "<httpUrl>",
                                blobPrefix = "<blobPrefix>",
                                filePath = "<filePath>",
                                fileMode = "<fileMode>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            }
                        },
                        environmentSettings = new[] {
                            new {
                                name = "<name>",
                                value = "<value>",
                            }
                        },
                        userIdentity = new {
                            username = "<username>",
                            autoUser = new {
                                scope = "task",
                                elevationLevel = "nonadmin",
                            },
                        },
                        maxTaskRetryCount = 1234,
                        waitForSuccess = true,
                    },
                    certificateReferences = new[] {
                        new {
                            thumbprint = "<thumbprint>",
                            thumbprintAlgorithm = "<thumbprintAlgorithm>",
                            storeLocation = "currentuser",
                            storeName = "<storeName>",
                            visibility = new[] {
                                "starttask"
                            },
                        }
                    },
                    applicationPackageReferences = new[] {
                        new {
                            applicationId = "<applicationId>",
                            version = "<version>",
                        }
                    },
                    applicationLicenses = new[] {
                        "<String>"
                    },
                    userAccounts = new[] {
                        new {
                            name = "<name>",
                            password = "<password>",
                            elevationLevel = "nonadmin",
                            linuxUserConfiguration = new {
                                uid = 1234,
                                gid = 1234,
                                sshPrivateKey = "<sshPrivateKey>",
                            },
                            windowsUserConfiguration = new {
                                loginMode = "batch",
                            },
                        }
                    },
                    metadata = new[] {
                        new {
                            name = "<name>",
                            value = "<value>",
                        }
                    },
                    mountConfiguration = new[] {
                        new {
                            azureBlobFileSystemConfiguration = new {
                                accountName = "<accountName>",
                                containerName = "<containerName>",
                                accountKey = "<accountKey>",
                                sasKey = "<sasKey>",
                                blobfuseOptions = "<blobfuseOptions>",
                                relativeMountPath = "<relativeMountPath>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            },
                            nfsMountConfiguration = new {
                                source = "<source>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                            },
                            cifsMountConfiguration = new {
                                username = "<username>",
                                source = "<source>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                                password = "<password>",
                            },
                            azureFileShareConfiguration = new {
                                accountName = "<accountName>",
                                azureFileUrl = "<azureFileUrl>",
                                accountKey = "<accountKey>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                            },
                        }
                    },
                    targetNodeCommunicationMode = "default",
                },
            },
        },
        metadata = new[] {
            new {
                name = "<name>",
                value = "<value>",
            }
        },
    },
    metadata = new[] {
        new {
            name = "<name>",
            value = "<value>",
        }
    },
};

Response response = client.Patch("<jobScheduleId>", RequestContent.Create(data), 1234, "<clientRequestId>", true, "<ocpDate>", null);
Console.WriteLine(response.Status);
]]></code>
</example>
<remarks>
This replaces only the Job Schedule properties specified in the request. For
example, if the schedule property is not specified with this request, then the
Batch service will keep the existing schedule. Changes to a Job Schedule only
impact Jobs created by the schedule after the update has taken place; currently
running Jobs are unaffected.

Below is the JSON schema for the request payload.

Request Body:

Schema for <c>BatchJobSchedule</c>:
<code>{
  id: string, # Optional. A string that uniquely identifies the schedule within the Account.
  displayName: string, # Optional. The display name for the schedule.
  url: string, # Optional. The URL of the Job Schedule.
  eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job Schedule has
changed between requests. In particular, you can be pass the ETag with an
Update Job Schedule request to specify that your changes should take effect
only if nobody else has modified the schedule in the meantime.
  lastModified: string (date &amp; time), # Optional. This is the last time at which the schedule level data, such as the Job
specification or recurrence information, changed. It does not factor in
job-level changes such as new Jobs being created or Jobs changing state.
  creationTime: string (date &amp; time), # Optional. The creation time of the Job Schedule.
  state: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. The state of the Job Schedule.
  stateTransitionTime: string (date &amp; time), # Optional. The time at which the Job Schedule entered the current state.
  previousState: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. This property is not present if the Job Schedule is in its initial active state.
  previousStateTransitionTime: string (date &amp; time), # Optional. This property is not present if the Job Schedule is in its initial active state.
  schedule: {
    doNotRunUntil: string (date &amp; time), # Optional. If you do not specify a doNotRunUntil time, the schedule becomes ready to
create Jobs immediately.
    doNotRunAfter: string (date &amp; time), # Optional. If you do not specify a doNotRunAfter time, and you are creating a recurring
Job Schedule, the Job Schedule will remain active until you explicitly
terminate it.
    startWindow: string (duration ISO 8601 Format), # Optional. If a Job is not created within the startWindow interval, then the &apos;opportunity&apos;
is lost; no Job will be created until the next recurrence of the schedule. If
the schedule is recurring, and the startWindow is longer than the recurrence
interval, then this is equivalent to an infinite startWindow, because the Job
that is &apos;due&apos; in one recurrenceInterval is not carried forward into the next
recurrence interval. The default is infinite. The minimum value is 1 minute. If
you specify a lower value, the Batch service rejects the schedule with an
error; if you are calling the REST API directly, the HTTP status code is 400
(Bad Request).
    recurrenceInterval: string (duration ISO 8601 Format), # Optional. Because a Job Schedule can have at most one active Job under it at any given
time, if it is time to create a new Job under a Job Schedule, but the previous
Job is still running, the Batch service will not create the new Job until the
previous Job finishes. If the previous Job does not finish within the
startWindow period of the new recurrenceInterval, then no new Job will be
scheduled for that interval. For recurring Jobs, you should normally specify a
jobManagerTask in the jobSpecification. If you do not use jobManagerTask, you
will need an external process to monitor when Jobs are created, add Tasks to
the Jobs and terminate the Jobs ready for the next recurrence. The default is
that the schedule does not recur: one Job is created, within the startWindow
after the doNotRunUntil time, and the schedule is complete as soon as that Job
finishes. The minimum value is 1 minute. If you specify a lower value, the
Batch service rejects the schedule with an error; if you are calling the REST
API directly, the HTTP status code is 400 (Bad Request).
  }, # Optional. All times are fixed respective to UTC and are not impacted by daylight saving
time.
  jobSpecification: {
    priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest
priority and 1000 being the highest priority. The default value is 0. This
priority is used as the default for all Jobs under the Job Schedule. You can
update a Job&apos;s priority after it has been created using by using the update Job
API.
    allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system
will take precedence and will be able requeue tasks from this job. You can
update a job&apos;s allowTaskPreemption after it has been created using the update
job API.
    maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not
specified, the default value is -1, which means there&apos;s no limit to the number
of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after
it has been created using the update job API.
    displayName: string, # Optional. The name need not be unique and can contain any Unicode characters up to a
maximum length of 1024.
    usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is
false.
    onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. Note that if a Job contains no Tasks, then all Tasks are considered complete.
This option is therefore most commonly used with a Job Manager task; if you
want to use automatic Job termination without a Job Manager, you should
initially set onAllTasksComplete to noaction and update the Job properties to
set onAllTasksComplete to terminatejob once you have finished adding Tasks. The
default is noaction.
    onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. The default is noaction.
    networkConfiguration: {
      subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes which will run Tasks from the Job. This
can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos;
service principal must have the &apos;Classic Virtual Machine Contributor&apos;
Role-Based Access Control (RBAC) role for the specified VNet so that Azure
Batch service can schedule Tasks on the Nodes. This can be verified by checking
if the specified VNet has any associated Network Security Groups (NSG). If
communication to the Nodes in the specified subnet is denied by an NSG, then
the Batch service will set the state of the Compute Nodes to unusable. This is
of the form
/subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}.
If the specified VNet has any associated Network Security Groups (NSG), then a
few reserved system ports must be enabled for inbound communication from the
Azure Batch service. For Pools created with a Virtual Machine configuration,
enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for
Windows. Port 443 is also required to be open for outbound connections for
communications to Azure Storage. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
    }, # Optional. The network configuration for the Job.
    constraints: {
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service
terminates it and any Tasks that are still running. In this case, the
termination reason will be MaxWallClockTimeExpiry. If this property is not
specified, there is no time limit on how long the Job may run.
      maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch
service will try each Task once, and may then retry up to this limit. For
example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one
initial try and 3 retries). If the maximum retry count is 0, the Batch service
does not retry Tasks. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
    }, # Optional. The execution constraints for a Job.
    jobManagerTask: {
      id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters.
      displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum
length of 1024.
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: {
        containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot;
command, in addition to those controlled by the Batch Service.
        imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If
no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a
default.
        registry: {
          username: string, # Optional. The user name to log into the registry server.
          password: string, # Optional. The password to log into the registry server.
          registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
          identityReference: {
            resourceId: string, # Optional. The ARM resource id of the user assigned identity.
          }, # Optional. The reference to a user assigned identity associated with the Batch pool which
a compute node will use.
        }, # Optional. This setting can be omitted if was already provided at Pool creation.
        workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
      }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must
be set as well. If the Pool that will run this Task doesn&apos;t have
containerConfiguration set, this must not be set. When this is specified, all
directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure
Batch directories on the node) are mapped into the container, all Task
environment variables are mapped into the container, and the Task command line
is executed in the container. Files produced in the container outside of
AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that
Batch file APIs will not be able to access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      outputFiles: [OutputFile], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node
on which the primary Task is executed.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Manager Task.
      constraints: {
        maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
        maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task
executable due to a nonzero exit code. The Batch service will try the Task
once, and may then retry up to this limit. For example, if the maximum retry
count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries).
If the maximum retry count is 0, the Batch service does not retry the Task
after the first attempt. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
      }, # Optional. Execution constraints to apply to a Task.
      requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the
node has enough free scheduling slots available. For multi-instance Tasks, this
property is not supported and must not be specified.
      killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job
as complete. If any Tasks are still running at this time (other than Job
Release), those Tasks are terminated. If false, the completion of the Job
Manager Task does not affect the Job status. In this case, you should either
use the onAllTasksComplete attribute to terminate the Job, or have a client or
user terminate the Job explicitly. An example of this is if the Job Manager
creates a set of Tasks but then takes no further role in their execution. The
default value is true. If you are using the onAllTasksComplete and
onTaskFailure attributes to control Job lifetime, and using the Job Manager
Task only to create the Tasks for the Job (not to monitor progress), then it is
important to set killJobOnCompletion to false.
      userIdentity: {
        username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
        autoUser: {
          scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task
should be specified if stricter isolation between tasks is required. For
example, if the task mutates the registry in a way which could impact other
tasks, or if certificates have been specified on the pool which should not be
accessible by normal tasks but should be accessible by StartTasks.
          elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
      }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
      runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job
Manager is running. If false, other Tasks can run simultaneously with the Job
Manager on a Compute Node. The Job Manager Task counts normally against the
Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute
Node allows multiple concurrent Tasks. The default value is true.
      applicationPackageReferences: [ApplicationPackageReference], # Optional. Application Packages are downloaded and deployed to a shared directory, not the
Task working directory. Therefore, if a referenced Application Package is
already on the Compute Node, and is up to date, then it is not re-downloaded;
the existing copy on the Compute Node is used. If a referenced Application
Package cannot be installed, for example because the package has been deleted
or because download failed, the Task fails.
      authenticationTokenSettings: {
        access: [&quot;job&quot;], # Optional. The authentication token grants access to a limited set of Batch service
operations. Currently the only supported value for the access property is
&apos;job&apos;, which grants access to all operations related to the Job which contains
the Task.
      }, # Optional. If this property is set, the Batch service provides the Task with an
authentication token which can be used to authenticate Batch service operations
without requiring an Account access key. The token is provided via the
AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the
Task can carry out using the token depend on the settings. For example, a Task
can request Job permissions in order to add other Tasks to the Job, or check
the status of the Job or of other Tasks under the Job.
      allowLowPriorityNode: boolean, # Optional. The default value is true.
    }, # Optional. If the Job does not specify a Job Manager Task, the user must explicitly add
Tasks to the Job using the Task API. If the Job does specify a Job Manager
Task, the Batch service creates the Job Manager Task when the Job is created,
and will try to schedule the Job Manager Task before scheduling other Tasks in
the Job.
    jobPreparationTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job
Preparation Task. If you try to submit a Task with the same id, the Batch
service rejects the request with error code TaskIdSameAsJobPreparationTask; if
you are calling the REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory. 
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
      constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
      waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries
the Job Preparation Task up to its maximum retry count (as specified in the
constraints element). If the Task has still not completed successfully after
all retries, then the Batch service will not schedule Tasks of the Job to the
Node. The Node remains active and eligible to run Tasks of other Jobs. If
false, the Batch service will not wait for the Job Preparation Task to
complete. In this case, other Tasks of the Job can start executing on the
Compute Node while the Job Preparation Task is still running; and even if the
Job Preparation Task fails, new Tasks will continue to be scheduled on the
Compute Node. The default value is true.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on
Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux
Compute Nodes.
      rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if
the Job Preparation Task did not complete (e.g. because the reboot occurred
while the Task was running). Therefore, you should always write a Job
Preparation Task to be idempotent and to behave correctly if run multiple
times. The default value is true.
    }, # Optional. If a Job has a Job Preparation Task, the Batch service will run the Job
Preparation Task on a Node before starting any Tasks of that Job on that
Compute Node.
    jobReleaseTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release
Task. If you try to submit a Task with the same id, the Batch service rejects
the request with error code TaskIdSameAsJobReleaseTask; if you are calling the
REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute
Node, measured from the time the Task starts. If the Task does not complete
within the time limit, the Batch service terminates it. The default value is 15
minutes. You may not specify a timeout longer than 15 minutes. If you do, the
Batch service rejects it with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
      retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
    }, # Optional. The primary purpose of the Job Release Task is to undo changes to Nodes made by
the Job Preparation Task. Example activities include deleting local files, or
shutting down services that were started as part of Job preparation. A Job
Release Task cannot be specified without also specifying a Job Preparation Task
for the Job. The Batch service runs the Job Release Task on the Compute Nodes
that have run the Job Preparation Task.
    commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by
specifying the same setting name with a different value.
    poolInfo: {
      poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool
does not exist at the time the Batch service tries to schedule a Job, no Tasks
for the Job will run until you create a Pool with that id. Note that the Batch
service will not reject the Job request; it will simply not run Tasks until the
Pool exists. You must specify either the Pool ID or the auto Pool
specification, but not both.
      autoPoolSpecification: {
        autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To
distinguish between Pools created for different purposes, you can specify this
element to add a prefix to the ID that is assigned. The prefix can be up to 20
characters long.
        poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule
are assigned to Pools.
        keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined
by the poolLifetimeOption setting) expires; that is, when the Job or Job
Schedule completes. If true, the Batch service does not delete the Pool
automatically. It is up to the user to delete auto Pools created with this
option.
        pool: {
          displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up
to a maximum length of 1024.
          vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose
a VM size for Compute Nodes in an Azure Batch Pool
(https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
          cloudServiceConfiguration: {
            osFamily: string, # Required. Possible values are:
2 - OS Family 2, equivalent to Windows Server 2008 R2
SP1.
3 - OS Family 3, equivalent to Windows Server 2012.
4 - OS Family 4,
equivalent to Windows Server 2012 R2.
5 - OS Family 5, equivalent to Windows
Server 2016.
6 - OS Family 6, equivalent to Windows Server 2019. For more
information, see Azure Guest OS Releases
(https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
            osVersion: string, # Optional. The default value is * which specifies the latest operating system version for
the specified OS family.
          }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS
VMs. This property and virtualMachineConfiguration are mutually exclusive and
one of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request). This property cannot be specified if the
Batch Account was created with its poolAllocationMode property set to
&apos;UserSubscription&apos;.
          virtualMachineConfiguration: {
            imageReference: {
              publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
              offer: string, # Optional. For example, UbuntuServer or WindowsServer.
              sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
              version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image.
If omitted, the default is &apos;latest&apos;.
              virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The
Shared Image Gallery Image must have replicas in the same region and must be in
the same subscription as the Azure Batch account. If the image version is not
specified in the imageId, the latest version will be used. For information
about the firewall settings for the Batch Compute Node agent to communicate
with the Batch service see
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
              exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create
the node. This read-only field differs from &apos;version&apos; only if the value
specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
            }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image
Gallery Image. To get the list of all Azure Marketplace Image references
verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
            nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the
Pool, and provides the command-and-control interface between the Compute Node
and the Batch service. There are different implementations of the Compute Node
agent, known as SKUs, for different operating systems. You must specify a
Compute Node agent SKU which matches the selected Image reference. To get the
list of supported Compute Node agent SKUs along with their list of verified
Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
            windowsConfiguration: {
              enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
            }, # Optional. This property must not be specified if the imageReference property specifies a
Linux OS Image.
            dataDisks: [DataDisk], # Optional. This property must be specified if the Compute Nodes in the Pool need to have
empty data disks attached to them. This cannot be updated. Each Compute Node
gets its own disk (the disk is not a file share). Existing disks cannot be
attached, each attached disk is empty. When the Compute Node is removed from
the Pool, the disk and all data associated with it is also deleted. The disk is
not formatted after being attached, it must be formatted before use - for more
information see
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux
and
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
            licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and
should only be used when you hold valid on-premises licenses for the Compute
Nodes which will be deployed. If omitted, no on-premises licensing discount is
applied. Values are:

 Windows_Server - The on-premises license is for Windows
Server.
 Windows_Client - The on-premises license is for Windows Client.

            containerConfiguration: {
              type: &quot;dockerCompatible&quot;, # Required. The container technology to be used.
              containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An
Image will be sourced from the default Docker registry unless the Image is
fully qualified with an alternative registry.
              containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires
credentials, then those credentials must be provided here.
            }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow
Tasks to run in containers. All regular Tasks and Job manager Tasks run on this
Pool must specify the containerSettings property, and all other Tasks may
specify it.
            diskEncryptionConfiguration: {
              targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On
Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot;
and &quot;TemporaryDisk&quot; must be specified.
            }, # Optional. If specified, encryption is performed on each node in the pool during node
provisioning.
            nodePlacementConfiguration: {
              policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not
specified, Batch will use the regional policy.
            }, # Optional. This configuration will specify rules on how nodes in the pool will be
physically allocated.
            extensions: [VMExtension], # Optional. If specified, the extensions mentioned in this configuration will be installed
on each node.
            osDisk: {
              ephemeralOSDiskSettings: {
                placement: &quot;cachedisk&quot;, # Optional. This property can be used by user in the request to choose the location e.g.,
cache disk space for Ephemeral OS disk provisioning. For more information on
Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size
requirements for Windows VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements
and Linux VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
              }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the
compute node (VM).
            }, # Optional. Settings for the operating system disk of the compute node (VM).
          }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS
VMs. This property and cloudServiceConfiguration are mutually exclusive and one
of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request).
          taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number
of cores of the vmSize of the pool or 256.
          taskSchedulingPolicy: {
            nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
          }, # Optional. If not specified, the default is spread.
          resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when
enableAutoScale is set to true. The default value is 15 minutes. The minimum
value is 5 minutes. If you specify a value less than 5 minutes, the Batch
service rejects the request with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
          targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must
be specified. If true, the autoScaleFormula element is required. The Pool
automatically resizes according to the formula. The default value is false.
          autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is
required if enableAutoScale is set to true. The formula is checked for validity
before the Pool is created. If the formula is not valid, the Batch service
rejects the request with detailed error information.
          autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes
and 168 hours respectively. If you specify a value less than 5 minutes or
greater than 168 hours, the Batch service rejects the request with an invalid
property value error; if you are calling the REST API directly, the HTTP status
code is 400 (Bad Request).
          enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to
deployment restrictions on the Compute Nodes of the Pool. This may result in
the Pool not reaching its desired size. The default value is false.
          networkConfiguration: {
            subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have
enough free IP addresses, the Pool will partially allocate Nodes and a resize
error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the
&apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for
the specified VNet. The specified subnet must allow communication from the
Azure Batch service to be able to schedule Tasks on the Nodes. This can be
verified by checking if the specified VNet has any associated Network Security
Groups (NSG). If communication to the Nodes in the specified subnet is denied
by an NSG, then the Batch service will set the state of the Compute Nodes to
unusable. For Pools created with virtualMachineConfiguration only ARM virtual
networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools
created with cloudServiceConfiguration both ARM and classic virtual networks
are supported. If the specified VNet has any associated Network Security Groups
(NSG), then a few reserved system ports must be enabled for inbound
communication. For Pools created with a virtual machine configuration, enable
ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows.
For Pools created with a cloud service configuration, enable ports 10100,
20100, and 30100. Also enable outbound connections to Azure Storage on port
443. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
            dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
            endpointConfiguration: {
              inboundNATPools: [InboundNATPool], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum
number of inbound NAT Pools is exceeded the request fails with HTTP status code
400. This cannot be specified if the IPAddressProvisioningType is
NoPublicIPAddresses.
            }, # Optional. Pool endpoint configuration is only supported on Pools with the
virtualMachineConfiguration property.
            publicIPAddressConfiguration: {
              provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
              ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100
dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public
IP. For example, a pool needing 250 dedicated VMs would need at least 3 public
IPs specified. Each element of this collection is of the form:
/subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
            }, # Optional. Public IP configuration property is only supported on Pools with the
virtualMachineConfiguration property.
          }, # Optional. The network configuration for a Pool.
          startTask: {
            commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
            containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
            resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
            environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
            userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
            maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this
value specifically controls the number of retries. The Batch service will try
the Task once, and may then retry up to this limit. For example, if the maximum
retry count is 3, Batch tries the Task up to 4 times (one initial try and 3
retries). If the maximum retry count is 0, the Batch service does not retry the
Task. If the maximum retry count is -1, the Batch service retries the Task
without limit, however this is not recommended for a start task or any task.
The default value is 0 (no retries)
            waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the
StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has
still not completed successfully after all retries, then the Batch service
marks the Node unusable, and will not schedule Tasks to it. This condition can
be detected via the Compute Node state and failure info details. If false, the
Batch service will not wait for the StartTask to complete. In this case, other
Tasks can start executing on the Compute Node while the StartTask is still
running; and even if the StartTask fails, new Tasks will continue to be
scheduled on the Compute Node. The default is true.
          }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node.
Examples of recovery operations include (but are not limited to) when an
unhealthy Node is rebooted or a Compute Node disappeared due to host failure.
Retries due to recovery operations are independent of and are not counted
against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal
retry due to a recovery operation may occur. Because of this, all Tasks should
be idempotent. This means Tasks need to tolerate being interrupted and
restarted without causing any corruption or duplicate data. The best practice
for long running Tasks is to use some form of checkpointing. In some cases the
StartTask may be re-run even though the Compute Node was not rebooted. Special
care should be taken to avoid StartTasks which create breakaway process or
install/launch services from the StartTask working directory, as this will
block Batch from being able to re-run the StartTask.
          certificateReferences: [CertificateReference], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified
Certificate store and location. For Linux Compute Nodes, the Certificates are
stored in a directory inside the Task working directory and an environment
variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this
location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory
is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and
Certificates are placed in that directory.
          applicationPackageReferences: [ApplicationPackageReference], # Optional. When creating a pool, the package&apos;s application ID must be fully qualified
(/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationName}).
Changes to Package references affect all new Nodes joining the Pool, but do not
affect Compute Nodes that are already in the Pool until they are rebooted or
reimaged. There is a maximum of 10 Package references on any given Pool.
          applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service
application licenses. If a license is requested which is not supported, Pool
creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;,
&apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application
license added to the Pool.
          userAccounts: [UserAccount], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
          metadata: [
            {
              name: string, # Required. The name of the metadata item.
              value: string, # Required. The value of the metadata item.
            }
          ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
          mountConfiguration: [MountConfiguration], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
          targetNodeCommunicationMode: &quot;default&quot; | &quot;classic&quot; | &quot;simplified&quot;, # Optional. If omitted, the default value is Default.
        }, # Optional. Specification for creating a new Pool.
      }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed
state, and the Pool creation error is set in the Job&apos;s scheduling error
property. The Batch service manages the lifetime (both creation and, unless
keepAlive is specified, deletion) of the auto Pool. Any user actions that
affect the lifetime of the auto Pool while the Job is active will result in
unexpected behavior. You must specify either the Pool ID or the auto Pool
specification, but not both.
    }, # Required. Specifies how a Job should be assigned to a Pool.
    metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  }, # Optional. Specifies details of the Jobs to be created on a schedule.
  executionInfo: {
    nextRunTime: string (date &amp; time), # Optional. This property is meaningful only if the schedule is in the active state when
the time comes around. For example, if the schedule is disabled, no Job will be
created at nextRunTime unless the Job is enabled before then.
    recentJob: {
      id: string, # Optional. The ID of the Job.
      url: string, # Optional. The URL of the Job.
    }, # Optional. This property is present only if the at least one Job has run under the
schedule.
    endTime: string (date &amp; time), # Optional. This property is set only if the Job Schedule is in the completed state.
  }, # Optional. Contains information about Jobs that have been and will be run under a Job
Schedule.
  metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  stats: {
    url: string, # Required. The URL of the statistics.
    startTime: string (date &amp; time), # Required. The start time of the time range covered by the statistics.
    lastUpdateTime: string (date &amp; time), # Required. The time at which the statistics were last updated. All statistics are limited
to the range between startTime and lastUpdateTime.
    userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    wallClockTime: string (duration ISO 8601 Format), # Required. The wall clock time is the elapsed time from when the Task started running on a
Compute Node to when it finished (or to the last time the statistics were
updated, if the Task had not finished by then). If a Task was retried, this
includes the wall clock time of all the Task retries.
    readIOps: number, # Required. The total number of disk read operations made by all Tasks in all Jobs created
under the schedule.
    writeIOps: number, # Required. The total number of disk write operations made by all Tasks in all Jobs created
under the schedule.
    readIOGiB: number, # Required. The total gibibytes read from disk by all Tasks in all Jobs created under the
schedule.
    writeIOGiB: number, # Required. The total gibibytes written to disk by all Tasks in all Jobs created under the
schedule.
    numSucceededTasks: number, # Required. The total number of Tasks successfully completed during the given time range in
Jobs created under the schedule. A Task completes successfully if it returns
exit code 0.
    numFailedTasks: number, # Required. The total number of Tasks that failed during the given time range in Jobs
created under the schedule. A Task fails if it exhausts its maximum retry count
without returning exit code 0.
    numTaskRetries: number, # Required. The total number of retries during the given time range on all Tasks in all
Jobs created under the schedule.
    waitTime: string (duration ISO 8601 Format), # Required. This value is only reported in the Account lifetime statistics; it is not
included in the Job statistics.
  }, # Optional. Resource usage statistics for a Job Schedule.
}
</code>

</remarks>
    </member>
    <member name="UpdateAsync(String,RequestContent,Int32,String,Boolean,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call UpdateAsync with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

var data = new {};

Response response = await client.UpdateAsync("<jobScheduleId>", RequestContent.Create(data));
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call UpdateAsync with all parameters and request content.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

var data = new {
    id = "<id>",
    displayName = "<displayName>",
    schedule = new {
        doNotRunUntil = "2022-05-10T14:57:31.2311892-04:00",
        doNotRunAfter = "2022-05-10T14:57:31.2311892-04:00",
        startWindow = PT1H23M45S,
        recurrenceInterval = PT1H23M45S,
    },
    jobSpecification = new {
        priority = 1234,
        allowTaskPreemption = true,
        maxParallelTasks = 1234,
        displayName = "<displayName>",
        usesTaskDependencies = true,
        onAllTasksComplete = "noaction",
        onTaskFailure = "noaction",
        networkConfiguration = new {
            subnetId = "<subnetId>",
        },
        constraints = new {
            maxWallClockTime = PT1H23M45S,
            maxTaskRetryCount = 1234,
        },
        jobManagerTask = new {
            id = "<id>",
            displayName = "<displayName>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            outputFiles = new[] {
                new {
                    filePattern = "<filePattern>",
                    destination = new {
                        container = new {
                            path = "<path>",
                            containerUrl = "<containerUrl>",
                            identityReference = new {
                                resourceId = "<resourceId>",
                            },
                            uploadHeaders = new[] {
                                new {
                                    name = "<name>",
                                    value = "<value>",
                                }
                            },
                        },
                    },
                    uploadOptions = new {
                        uploadCondition = "tasksuccess",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            constraints = new {
                maxWallClockTime = PT1H23M45S,
                retentionTime = PT1H23M45S,
                maxTaskRetryCount = 1234,
            },
            requiredSlots = 1234,
            killJobOnCompletion = true,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
            runExclusive = true,
            applicationPackageReferences = new[] {
                new {
                    applicationId = "<applicationId>",
                    version = "<version>",
                }
            },
            authenticationTokenSettings = new {
                access = new[] {
                    "job"
                },
            },
            allowLowPriorityNode = true,
        },
        jobPreparationTask = new {
            id = "<id>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            constraints = new {
                maxWallClockTime = PT1H23M45S,
                retentionTime = PT1H23M45S,
                maxTaskRetryCount = 1234,
            },
            waitForSuccess = true,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
            rerunOnNodeRebootAfterSuccess = true,
        },
        jobReleaseTask = new {
            id = "<id>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            maxWallClockTime = PT1H23M45S,
            retentionTime = PT1H23M45S,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
        },
        commonEnvironmentSettings = new[] {
            new {
                name = "<name>",
                value = "<value>",
            }
        },
        poolInfo = new {
            poolId = "<poolId>",
            autoPoolSpecification = new {
                autoPoolIdPrefix = "<autoPoolIdPrefix>",
                poolLifetimeOption = "jobschedule",
                keepAlive = true,
                pool = new {
                    displayName = "<displayName>",
                    vmSize = "<vmSize>",
                    cloudServiceConfiguration = new {
                        osFamily = "<osFamily>",
                        osVersion = "<osVersion>",
                    },
                    virtualMachineConfiguration = new {
                        imageReference = new {
                            publisher = "<publisher>",
                            offer = "<offer>",
                            sku = "<sku>",
                            version = "<version>",
                            virtualMachineImageId = "<virtualMachineImageId>",
                        },
                        nodeAgentSKUId = "<nodeAgentSKUId>",
                        windowsConfiguration = new {
                            enableAutomaticUpdates = true,
                        },
                        dataDisks = new[] {
                            new {
                                lun = 1234,
                                caching = "none",
                                diskSizeGB = 1234,
                                storageAccountType = "standard_lrs",
                            }
                        },
                        licenseType = "<licenseType>",
                        containerConfiguration = new {
                            type = "dockerCompatible",
                            containerImageNames = new[] {
                                "<String>"
                            },
                            containerRegistries = new[] {
                                new {
                                    username = "<username>",
                                    password = "<password>",
                                    registryServer = "<registryServer>",
                                    identityReference = new {
                                        resourceId = "<resourceId>",
                                    },
                                }
                            },
                        },
                        diskEncryptionConfiguration = new {
                            targets = new[] {
                                "osdisk"
                            },
                        },
                        nodePlacementConfiguration = new {
                            policy = "regional",
                        },
                        extensions = new[] {
                            new {
                                name = "<name>",
                                publisher = "<publisher>",
                                type = "<type>",
                                typeHandlerVersion = "<typeHandlerVersion>",
                                autoUpgradeMinorVersion = true,
                                settings = new {},
                                protectedSettings = new {},
                                provisionAfterExtensions = new[] {
                                    "<String>"
                                },
                            }
                        },
                        osDisk = new {
                            ephemeralOSDiskSettings = new {
                                placement = "cachedisk",
                            },
                        },
                    },
                    taskSlotsPerNode = 1234,
                    taskSchedulingPolicy = new {
                        nodeFillType = "spread",
                    },
                    resizeTimeout = PT1H23M45S,
                    targetDedicatedNodes = 1234,
                    targetLowPriorityNodes = 1234,
                    enableAutoScale = true,
                    autoScaleFormula = "<autoScaleFormula>",
                    autoScaleEvaluationInterval = PT1H23M45S,
                    enableInterNodeCommunication = true,
                    networkConfiguration = new {
                        subnetId = "<subnetId>",
                        dynamicVNetAssignmentScope = "none",
                        endpointConfiguration = new {
                            inboundNATPools = new[] {
                                new {
                                    name = "<name>",
                                    protocol = "tcp",
                                    backendPort = 1234,
                                    frontendPortRangeStart = 1234,
                                    frontendPortRangeEnd = 1234,
                                    networkSecurityGroupRules = new[] {
                                        new {
                                            priority = 1234,
                                            access = "allow",
                                            sourceAddressPrefix = "<sourceAddressPrefix>",
                                            sourcePortRanges = new[] {
                                                "<String>"
                                            },
                                        }
                                    },
                                }
                            },
                        },
                        publicIPAddressConfiguration = new {
                            provision = "batchmanaged",
                            ipAddressIds = new[] {
                                "<String>"
                            },
                        },
                    },
                    startTask = new {
                        commandLine = "<commandLine>",
                        containerSettings = new {
                            containerRunOptions = "<containerRunOptions>",
                            imageName = "<imageName>",
                            registry = new {
                                username = "<username>",
                                password = "<password>",
                                registryServer = "<registryServer>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            },
                            workingDirectory = "taskWorkingDirectory",
                        },
                        resourceFiles = new[] {
                            new {
                                autoStorageContainerName = "<autoStorageContainerName>",
                                storageContainerUrl = "<storageContainerUrl>",
                                httpUrl = "<httpUrl>",
                                blobPrefix = "<blobPrefix>",
                                filePath = "<filePath>",
                                fileMode = "<fileMode>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            }
                        },
                        environmentSettings = new[] {
                            new {
                                name = "<name>",
                                value = "<value>",
                            }
                        },
                        userIdentity = new {
                            username = "<username>",
                            autoUser = new {
                                scope = "task",
                                elevationLevel = "nonadmin",
                            },
                        },
                        maxTaskRetryCount = 1234,
                        waitForSuccess = true,
                    },
                    certificateReferences = new[] {
                        new {
                            thumbprint = "<thumbprint>",
                            thumbprintAlgorithm = "<thumbprintAlgorithm>",
                            storeLocation = "currentuser",
                            storeName = "<storeName>",
                            visibility = new[] {
                                "starttask"
                            },
                        }
                    },
                    applicationPackageReferences = new[] {
                        new {
                            applicationId = "<applicationId>",
                            version = "<version>",
                        }
                    },
                    applicationLicenses = new[] {
                        "<String>"
                    },
                    userAccounts = new[] {
                        new {
                            name = "<name>",
                            password = "<password>",
                            elevationLevel = "nonadmin",
                            linuxUserConfiguration = new {
                                uid = 1234,
                                gid = 1234,
                                sshPrivateKey = "<sshPrivateKey>",
                            },
                            windowsUserConfiguration = new {
                                loginMode = "batch",
                            },
                        }
                    },
                    metadata = new[] {
                        new {
                            name = "<name>",
                            value = "<value>",
                        }
                    },
                    mountConfiguration = new[] {
                        new {
                            azureBlobFileSystemConfiguration = new {
                                accountName = "<accountName>",
                                containerName = "<containerName>",
                                accountKey = "<accountKey>",
                                sasKey = "<sasKey>",
                                blobfuseOptions = "<blobfuseOptions>",
                                relativeMountPath = "<relativeMountPath>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            },
                            nfsMountConfiguration = new {
                                source = "<source>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                            },
                            cifsMountConfiguration = new {
                                username = "<username>",
                                source = "<source>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                                password = "<password>",
                            },
                            azureFileShareConfiguration = new {
                                accountName = "<accountName>",
                                azureFileUrl = "<azureFileUrl>",
                                accountKey = "<accountKey>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                            },
                        }
                    },
                    targetNodeCommunicationMode = "default",
                },
            },
        },
        metadata = new[] {
            new {
                name = "<name>",
                value = "<value>",
            }
        },
    },
    metadata = new[] {
        new {
            name = "<name>",
            value = "<value>",
        }
    },
};

Response response = await client.UpdateAsync("<jobScheduleId>", RequestContent.Create(data), 1234, "<clientRequestId>", true, "<ocpDate>", null);
Console.WriteLine(response.Status);
]]></code>
</example>
<remarks>
This fully replaces all the updatable properties of the Job Schedule. For
example, if the schedule property is not specified with this request, then the
Batch service will remove the existing schedule. Changes to a Job Schedule only
impact Jobs created by the schedule after the update has taken place; currently
running Jobs are unaffected.

Below is the JSON schema for the request payload.

Request Body:

Schema for <c>BatchJobSchedule</c>:
<code>{
  id: string, # Optional. A string that uniquely identifies the schedule within the Account.
  displayName: string, # Optional. The display name for the schedule.
  url: string, # Optional. The URL of the Job Schedule.
  eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job Schedule has
changed between requests. In particular, you can be pass the ETag with an
Update Job Schedule request to specify that your changes should take effect
only if nobody else has modified the schedule in the meantime.
  lastModified: string (date &amp; time), # Optional. This is the last time at which the schedule level data, such as the Job
specification or recurrence information, changed. It does not factor in
job-level changes such as new Jobs being created or Jobs changing state.
  creationTime: string (date &amp; time), # Optional. The creation time of the Job Schedule.
  state: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. The state of the Job Schedule.
  stateTransitionTime: string (date &amp; time), # Optional. The time at which the Job Schedule entered the current state.
  previousState: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. This property is not present if the Job Schedule is in its initial active state.
  previousStateTransitionTime: string (date &amp; time), # Optional. This property is not present if the Job Schedule is in its initial active state.
  schedule: {
    doNotRunUntil: string (date &amp; time), # Optional. If you do not specify a doNotRunUntil time, the schedule becomes ready to
create Jobs immediately.
    doNotRunAfter: string (date &amp; time), # Optional. If you do not specify a doNotRunAfter time, and you are creating a recurring
Job Schedule, the Job Schedule will remain active until you explicitly
terminate it.
    startWindow: string (duration ISO 8601 Format), # Optional. If a Job is not created within the startWindow interval, then the &apos;opportunity&apos;
is lost; no Job will be created until the next recurrence of the schedule. If
the schedule is recurring, and the startWindow is longer than the recurrence
interval, then this is equivalent to an infinite startWindow, because the Job
that is &apos;due&apos; in one recurrenceInterval is not carried forward into the next
recurrence interval. The default is infinite. The minimum value is 1 minute. If
you specify a lower value, the Batch service rejects the schedule with an
error; if you are calling the REST API directly, the HTTP status code is 400
(Bad Request).
    recurrenceInterval: string (duration ISO 8601 Format), # Optional. Because a Job Schedule can have at most one active Job under it at any given
time, if it is time to create a new Job under a Job Schedule, but the previous
Job is still running, the Batch service will not create the new Job until the
previous Job finishes. If the previous Job does not finish within the
startWindow period of the new recurrenceInterval, then no new Job will be
scheduled for that interval. For recurring Jobs, you should normally specify a
jobManagerTask in the jobSpecification. If you do not use jobManagerTask, you
will need an external process to monitor when Jobs are created, add Tasks to
the Jobs and terminate the Jobs ready for the next recurrence. The default is
that the schedule does not recur: one Job is created, within the startWindow
after the doNotRunUntil time, and the schedule is complete as soon as that Job
finishes. The minimum value is 1 minute. If you specify a lower value, the
Batch service rejects the schedule with an error; if you are calling the REST
API directly, the HTTP status code is 400 (Bad Request).
  }, # Optional. All times are fixed respective to UTC and are not impacted by daylight saving
time.
  jobSpecification: {
    priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest
priority and 1000 being the highest priority. The default value is 0. This
priority is used as the default for all Jobs under the Job Schedule. You can
update a Job&apos;s priority after it has been created using by using the update Job
API.
    allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system
will take precedence and will be able requeue tasks from this job. You can
update a job&apos;s allowTaskPreemption after it has been created using the update
job API.
    maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not
specified, the default value is -1, which means there&apos;s no limit to the number
of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after
it has been created using the update job API.
    displayName: string, # Optional. The name need not be unique and can contain any Unicode characters up to a
maximum length of 1024.
    usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is
false.
    onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. Note that if a Job contains no Tasks, then all Tasks are considered complete.
This option is therefore most commonly used with a Job Manager task; if you
want to use automatic Job termination without a Job Manager, you should
initially set onAllTasksComplete to noaction and update the Job properties to
set onAllTasksComplete to terminatejob once you have finished adding Tasks. The
default is noaction.
    onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. The default is noaction.
    networkConfiguration: {
      subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes which will run Tasks from the Job. This
can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos;
service principal must have the &apos;Classic Virtual Machine Contributor&apos;
Role-Based Access Control (RBAC) role for the specified VNet so that Azure
Batch service can schedule Tasks on the Nodes. This can be verified by checking
if the specified VNet has any associated Network Security Groups (NSG). If
communication to the Nodes in the specified subnet is denied by an NSG, then
the Batch service will set the state of the Compute Nodes to unusable. This is
of the form
/subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}.
If the specified VNet has any associated Network Security Groups (NSG), then a
few reserved system ports must be enabled for inbound communication from the
Azure Batch service. For Pools created with a Virtual Machine configuration,
enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for
Windows. Port 443 is also required to be open for outbound connections for
communications to Azure Storage. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
    }, # Optional. The network configuration for the Job.
    constraints: {
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service
terminates it and any Tasks that are still running. In this case, the
termination reason will be MaxWallClockTimeExpiry. If this property is not
specified, there is no time limit on how long the Job may run.
      maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch
service will try each Task once, and may then retry up to this limit. For
example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one
initial try and 3 retries). If the maximum retry count is 0, the Batch service
does not retry Tasks. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
    }, # Optional. The execution constraints for a Job.
    jobManagerTask: {
      id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters.
      displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum
length of 1024.
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: {
        containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot;
command, in addition to those controlled by the Batch Service.
        imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If
no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a
default.
        registry: {
          username: string, # Optional. The user name to log into the registry server.
          password: string, # Optional. The password to log into the registry server.
          registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
          identityReference: {
            resourceId: string, # Optional. The ARM resource id of the user assigned identity.
          }, # Optional. The reference to a user assigned identity associated with the Batch pool which
a compute node will use.
        }, # Optional. This setting can be omitted if was already provided at Pool creation.
        workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
      }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must
be set as well. If the Pool that will run this Task doesn&apos;t have
containerConfiguration set, this must not be set. When this is specified, all
directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure
Batch directories on the node) are mapped into the container, all Task
environment variables are mapped into the container, and the Task command line
is executed in the container. Files produced in the container outside of
AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that
Batch file APIs will not be able to access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      outputFiles: [OutputFile], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node
on which the primary Task is executed.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Manager Task.
      constraints: {
        maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
        maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task
executable due to a nonzero exit code. The Batch service will try the Task
once, and may then retry up to this limit. For example, if the maximum retry
count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries).
If the maximum retry count is 0, the Batch service does not retry the Task
after the first attempt. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
      }, # Optional. Execution constraints to apply to a Task.
      requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the
node has enough free scheduling slots available. For multi-instance Tasks, this
property is not supported and must not be specified.
      killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job
as complete. If any Tasks are still running at this time (other than Job
Release), those Tasks are terminated. If false, the completion of the Job
Manager Task does not affect the Job status. In this case, you should either
use the onAllTasksComplete attribute to terminate the Job, or have a client or
user terminate the Job explicitly. An example of this is if the Job Manager
creates a set of Tasks but then takes no further role in their execution. The
default value is true. If you are using the onAllTasksComplete and
onTaskFailure attributes to control Job lifetime, and using the Job Manager
Task only to create the Tasks for the Job (not to monitor progress), then it is
important to set killJobOnCompletion to false.
      userIdentity: {
        username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
        autoUser: {
          scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task
should be specified if stricter isolation between tasks is required. For
example, if the task mutates the registry in a way which could impact other
tasks, or if certificates have been specified on the pool which should not be
accessible by normal tasks but should be accessible by StartTasks.
          elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
      }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
      runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job
Manager is running. If false, other Tasks can run simultaneously with the Job
Manager on a Compute Node. The Job Manager Task counts normally against the
Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute
Node allows multiple concurrent Tasks. The default value is true.
      applicationPackageReferences: [ApplicationPackageReference], # Optional. Application Packages are downloaded and deployed to a shared directory, not the
Task working directory. Therefore, if a referenced Application Package is
already on the Compute Node, and is up to date, then it is not re-downloaded;
the existing copy on the Compute Node is used. If a referenced Application
Package cannot be installed, for example because the package has been deleted
or because download failed, the Task fails.
      authenticationTokenSettings: {
        access: [&quot;job&quot;], # Optional. The authentication token grants access to a limited set of Batch service
operations. Currently the only supported value for the access property is
&apos;job&apos;, which grants access to all operations related to the Job which contains
the Task.
      }, # Optional. If this property is set, the Batch service provides the Task with an
authentication token which can be used to authenticate Batch service operations
without requiring an Account access key. The token is provided via the
AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the
Task can carry out using the token depend on the settings. For example, a Task
can request Job permissions in order to add other Tasks to the Job, or check
the status of the Job or of other Tasks under the Job.
      allowLowPriorityNode: boolean, # Optional. The default value is true.
    }, # Optional. If the Job does not specify a Job Manager Task, the user must explicitly add
Tasks to the Job using the Task API. If the Job does specify a Job Manager
Task, the Batch service creates the Job Manager Task when the Job is created,
and will try to schedule the Job Manager Task before scheduling other Tasks in
the Job.
    jobPreparationTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job
Preparation Task. If you try to submit a Task with the same id, the Batch
service rejects the request with error code TaskIdSameAsJobPreparationTask; if
you are calling the REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory. 
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
      constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
      waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries
the Job Preparation Task up to its maximum retry count (as specified in the
constraints element). If the Task has still not completed successfully after
all retries, then the Batch service will not schedule Tasks of the Job to the
Node. The Node remains active and eligible to run Tasks of other Jobs. If
false, the Batch service will not wait for the Job Preparation Task to
complete. In this case, other Tasks of the Job can start executing on the
Compute Node while the Job Preparation Task is still running; and even if the
Job Preparation Task fails, new Tasks will continue to be scheduled on the
Compute Node. The default value is true.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on
Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux
Compute Nodes.
      rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if
the Job Preparation Task did not complete (e.g. because the reboot occurred
while the Task was running). Therefore, you should always write a Job
Preparation Task to be idempotent and to behave correctly if run multiple
times. The default value is true.
    }, # Optional. If a Job has a Job Preparation Task, the Batch service will run the Job
Preparation Task on a Node before starting any Tasks of that Job on that
Compute Node.
    jobReleaseTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release
Task. If you try to submit a Task with the same id, the Batch service rejects
the request with error code TaskIdSameAsJobReleaseTask; if you are calling the
REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute
Node, measured from the time the Task starts. If the Task does not complete
within the time limit, the Batch service terminates it. The default value is 15
minutes. You may not specify a timeout longer than 15 minutes. If you do, the
Batch service rejects it with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
      retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
    }, # Optional. The primary purpose of the Job Release Task is to undo changes to Nodes made by
the Job Preparation Task. Example activities include deleting local files, or
shutting down services that were started as part of Job preparation. A Job
Release Task cannot be specified without also specifying a Job Preparation Task
for the Job. The Batch service runs the Job Release Task on the Compute Nodes
that have run the Job Preparation Task.
    commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by
specifying the same setting name with a different value.
    poolInfo: {
      poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool
does not exist at the time the Batch service tries to schedule a Job, no Tasks
for the Job will run until you create a Pool with that id. Note that the Batch
service will not reject the Job request; it will simply not run Tasks until the
Pool exists. You must specify either the Pool ID or the auto Pool
specification, but not both.
      autoPoolSpecification: {
        autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To
distinguish between Pools created for different purposes, you can specify this
element to add a prefix to the ID that is assigned. The prefix can be up to 20
characters long.
        poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule
are assigned to Pools.
        keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined
by the poolLifetimeOption setting) expires; that is, when the Job or Job
Schedule completes. If true, the Batch service does not delete the Pool
automatically. It is up to the user to delete auto Pools created with this
option.
        pool: {
          displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up
to a maximum length of 1024.
          vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose
a VM size for Compute Nodes in an Azure Batch Pool
(https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
          cloudServiceConfiguration: {
            osFamily: string, # Required. Possible values are:
2 - OS Family 2, equivalent to Windows Server 2008 R2
SP1.
3 - OS Family 3, equivalent to Windows Server 2012.
4 - OS Family 4,
equivalent to Windows Server 2012 R2.
5 - OS Family 5, equivalent to Windows
Server 2016.
6 - OS Family 6, equivalent to Windows Server 2019. For more
information, see Azure Guest OS Releases
(https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
            osVersion: string, # Optional. The default value is * which specifies the latest operating system version for
the specified OS family.
          }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS
VMs. This property and virtualMachineConfiguration are mutually exclusive and
one of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request). This property cannot be specified if the
Batch Account was created with its poolAllocationMode property set to
&apos;UserSubscription&apos;.
          virtualMachineConfiguration: {
            imageReference: {
              publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
              offer: string, # Optional. For example, UbuntuServer or WindowsServer.
              sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
              version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image.
If omitted, the default is &apos;latest&apos;.
              virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The
Shared Image Gallery Image must have replicas in the same region and must be in
the same subscription as the Azure Batch account. If the image version is not
specified in the imageId, the latest version will be used. For information
about the firewall settings for the Batch Compute Node agent to communicate
with the Batch service see
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
              exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create
the node. This read-only field differs from &apos;version&apos; only if the value
specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
            }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image
Gallery Image. To get the list of all Azure Marketplace Image references
verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
            nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the
Pool, and provides the command-and-control interface between the Compute Node
and the Batch service. There are different implementations of the Compute Node
agent, known as SKUs, for different operating systems. You must specify a
Compute Node agent SKU which matches the selected Image reference. To get the
list of supported Compute Node agent SKUs along with their list of verified
Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
            windowsConfiguration: {
              enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
            }, # Optional. This property must not be specified if the imageReference property specifies a
Linux OS Image.
            dataDisks: [DataDisk], # Optional. This property must be specified if the Compute Nodes in the Pool need to have
empty data disks attached to them. This cannot be updated. Each Compute Node
gets its own disk (the disk is not a file share). Existing disks cannot be
attached, each attached disk is empty. When the Compute Node is removed from
the Pool, the disk and all data associated with it is also deleted. The disk is
not formatted after being attached, it must be formatted before use - for more
information see
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux
and
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
            licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and
should only be used when you hold valid on-premises licenses for the Compute
Nodes which will be deployed. If omitted, no on-premises licensing discount is
applied. Values are:

 Windows_Server - The on-premises license is for Windows
Server.
 Windows_Client - The on-premises license is for Windows Client.

            containerConfiguration: {
              type: &quot;dockerCompatible&quot;, # Required. The container technology to be used.
              containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An
Image will be sourced from the default Docker registry unless the Image is
fully qualified with an alternative registry.
              containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires
credentials, then those credentials must be provided here.
            }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow
Tasks to run in containers. All regular Tasks and Job manager Tasks run on this
Pool must specify the containerSettings property, and all other Tasks may
specify it.
            diskEncryptionConfiguration: {
              targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On
Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot;
and &quot;TemporaryDisk&quot; must be specified.
            }, # Optional. If specified, encryption is performed on each node in the pool during node
provisioning.
            nodePlacementConfiguration: {
              policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not
specified, Batch will use the regional policy.
            }, # Optional. This configuration will specify rules on how nodes in the pool will be
physically allocated.
            extensions: [VMExtension], # Optional. If specified, the extensions mentioned in this configuration will be installed
on each node.
            osDisk: {
              ephemeralOSDiskSettings: {
                placement: &quot;cachedisk&quot;, # Optional. This property can be used by user in the request to choose the location e.g.,
cache disk space for Ephemeral OS disk provisioning. For more information on
Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size
requirements for Windows VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements
and Linux VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
              }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the
compute node (VM).
            }, # Optional. Settings for the operating system disk of the compute node (VM).
          }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS
VMs. This property and cloudServiceConfiguration are mutually exclusive and one
of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request).
          taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number
of cores of the vmSize of the pool or 256.
          taskSchedulingPolicy: {
            nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
          }, # Optional. If not specified, the default is spread.
          resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when
enableAutoScale is set to true. The default value is 15 minutes. The minimum
value is 5 minutes. If you specify a value less than 5 minutes, the Batch
service rejects the request with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
          targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must
be specified. If true, the autoScaleFormula element is required. The Pool
automatically resizes according to the formula. The default value is false.
          autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is
required if enableAutoScale is set to true. The formula is checked for validity
before the Pool is created. If the formula is not valid, the Batch service
rejects the request with detailed error information.
          autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes
and 168 hours respectively. If you specify a value less than 5 minutes or
greater than 168 hours, the Batch service rejects the request with an invalid
property value error; if you are calling the REST API directly, the HTTP status
code is 400 (Bad Request).
          enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to
deployment restrictions on the Compute Nodes of the Pool. This may result in
the Pool not reaching its desired size. The default value is false.
          networkConfiguration: {
            subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have
enough free IP addresses, the Pool will partially allocate Nodes and a resize
error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the
&apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for
the specified VNet. The specified subnet must allow communication from the
Azure Batch service to be able to schedule Tasks on the Nodes. This can be
verified by checking if the specified VNet has any associated Network Security
Groups (NSG). If communication to the Nodes in the specified subnet is denied
by an NSG, then the Batch service will set the state of the Compute Nodes to
unusable. For Pools created with virtualMachineConfiguration only ARM virtual
networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools
created with cloudServiceConfiguration both ARM and classic virtual networks
are supported. If the specified VNet has any associated Network Security Groups
(NSG), then a few reserved system ports must be enabled for inbound
communication. For Pools created with a virtual machine configuration, enable
ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows.
For Pools created with a cloud service configuration, enable ports 10100,
20100, and 30100. Also enable outbound connections to Azure Storage on port
443. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
            dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
            endpointConfiguration: {
              inboundNATPools: [InboundNATPool], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum
number of inbound NAT Pools is exceeded the request fails with HTTP status code
400. This cannot be specified if the IPAddressProvisioningType is
NoPublicIPAddresses.
            }, # Optional. Pool endpoint configuration is only supported on Pools with the
virtualMachineConfiguration property.
            publicIPAddressConfiguration: {
              provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
              ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100
dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public
IP. For example, a pool needing 250 dedicated VMs would need at least 3 public
IPs specified. Each element of this collection is of the form:
/subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
            }, # Optional. Public IP configuration property is only supported on Pools with the
virtualMachineConfiguration property.
          }, # Optional. The network configuration for a Pool.
          startTask: {
            commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
            containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
            resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
            environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
            userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
            maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this
value specifically controls the number of retries. The Batch service will try
the Task once, and may then retry up to this limit. For example, if the maximum
retry count is 3, Batch tries the Task up to 4 times (one initial try and 3
retries). If the maximum retry count is 0, the Batch service does not retry the
Task. If the maximum retry count is -1, the Batch service retries the Task
without limit, however this is not recommended for a start task or any task.
The default value is 0 (no retries)
            waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the
StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has
still not completed successfully after all retries, then the Batch service
marks the Node unusable, and will not schedule Tasks to it. This condition can
be detected via the Compute Node state and failure info details. If false, the
Batch service will not wait for the StartTask to complete. In this case, other
Tasks can start executing on the Compute Node while the StartTask is still
running; and even if the StartTask fails, new Tasks will continue to be
scheduled on the Compute Node. The default is true.
          }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node.
Examples of recovery operations include (but are not limited to) when an
unhealthy Node is rebooted or a Compute Node disappeared due to host failure.
Retries due to recovery operations are independent of and are not counted
against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal
retry due to a recovery operation may occur. Because of this, all Tasks should
be idempotent. This means Tasks need to tolerate being interrupted and
restarted without causing any corruption or duplicate data. The best practice
for long running Tasks is to use some form of checkpointing. In some cases the
StartTask may be re-run even though the Compute Node was not rebooted. Special
care should be taken to avoid StartTasks which create breakaway process or
install/launch services from the StartTask working directory, as this will
block Batch from being able to re-run the StartTask.
          certificateReferences: [CertificateReference], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified
Certificate store and location. For Linux Compute Nodes, the Certificates are
stored in a directory inside the Task working directory and an environment
variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this
location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory
is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and
Certificates are placed in that directory.
          applicationPackageReferences: [ApplicationPackageReference], # Optional. When creating a pool, the package&apos;s application ID must be fully qualified
(/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationName}).
Changes to Package references affect all new Nodes joining the Pool, but do not
affect Compute Nodes that are already in the Pool until they are rebooted or
reimaged. There is a maximum of 10 Package references on any given Pool.
          applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service
application licenses. If a license is requested which is not supported, Pool
creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;,
&apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application
license added to the Pool.
          userAccounts: [UserAccount], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
          metadata: [
            {
              name: string, # Required. The name of the metadata item.
              value: string, # Required. The value of the metadata item.
            }
          ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
          mountConfiguration: [MountConfiguration], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
          targetNodeCommunicationMode: &quot;default&quot; | &quot;classic&quot; | &quot;simplified&quot;, # Optional. If omitted, the default value is Default.
        }, # Optional. Specification for creating a new Pool.
      }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed
state, and the Pool creation error is set in the Job&apos;s scheduling error
property. The Batch service manages the lifetime (both creation and, unless
keepAlive is specified, deletion) of the auto Pool. Any user actions that
affect the lifetime of the auto Pool while the Job is active will result in
unexpected behavior. You must specify either the Pool ID or the auto Pool
specification, but not both.
    }, # Required. Specifies how a Job should be assigned to a Pool.
    metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  }, # Optional. Specifies details of the Jobs to be created on a schedule.
  executionInfo: {
    nextRunTime: string (date &amp; time), # Optional. This property is meaningful only if the schedule is in the active state when
the time comes around. For example, if the schedule is disabled, no Job will be
created at nextRunTime unless the Job is enabled before then.
    recentJob: {
      id: string, # Optional. The ID of the Job.
      url: string, # Optional. The URL of the Job.
    }, # Optional. This property is present only if the at least one Job has run under the
schedule.
    endTime: string (date &amp; time), # Optional. This property is set only if the Job Schedule is in the completed state.
  }, # Optional. Contains information about Jobs that have been and will be run under a Job
Schedule.
  metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  stats: {
    url: string, # Required. The URL of the statistics.
    startTime: string (date &amp; time), # Required. The start time of the time range covered by the statistics.
    lastUpdateTime: string (date &amp; time), # Required. The time at which the statistics were last updated. All statistics are limited
to the range between startTime and lastUpdateTime.
    userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    wallClockTime: string (duration ISO 8601 Format), # Required. The wall clock time is the elapsed time from when the Task started running on a
Compute Node to when it finished (or to the last time the statistics were
updated, if the Task had not finished by then). If a Task was retried, this
includes the wall clock time of all the Task retries.
    readIOps: number, # Required. The total number of disk read operations made by all Tasks in all Jobs created
under the schedule.
    writeIOps: number, # Required. The total number of disk write operations made by all Tasks in all Jobs created
under the schedule.
    readIOGiB: number, # Required. The total gibibytes read from disk by all Tasks in all Jobs created under the
schedule.
    writeIOGiB: number, # Required. The total gibibytes written to disk by all Tasks in all Jobs created under the
schedule.
    numSucceededTasks: number, # Required. The total number of Tasks successfully completed during the given time range in
Jobs created under the schedule. A Task completes successfully if it returns
exit code 0.
    numFailedTasks: number, # Required. The total number of Tasks that failed during the given time range in Jobs
created under the schedule. A Task fails if it exhausts its maximum retry count
without returning exit code 0.
    numTaskRetries: number, # Required. The total number of retries during the given time range on all Tasks in all
Jobs created under the schedule.
    waitTime: string (duration ISO 8601 Format), # Required. This value is only reported in the Account lifetime statistics; it is not
included in the Job statistics.
  }, # Optional. Resource usage statistics for a Job Schedule.
}
</code>

</remarks>
    </member>
    <member name="Update(String,RequestContent,Int32,String,Boolean,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call Update with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

var data = new {};

Response response = client.Update("<jobScheduleId>", RequestContent.Create(data));
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call Update with all parameters and request content.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

var data = new {
    id = "<id>",
    displayName = "<displayName>",
    schedule = new {
        doNotRunUntil = "2022-05-10T14:57:31.2311892-04:00",
        doNotRunAfter = "2022-05-10T14:57:31.2311892-04:00",
        startWindow = PT1H23M45S,
        recurrenceInterval = PT1H23M45S,
    },
    jobSpecification = new {
        priority = 1234,
        allowTaskPreemption = true,
        maxParallelTasks = 1234,
        displayName = "<displayName>",
        usesTaskDependencies = true,
        onAllTasksComplete = "noaction",
        onTaskFailure = "noaction",
        networkConfiguration = new {
            subnetId = "<subnetId>",
        },
        constraints = new {
            maxWallClockTime = PT1H23M45S,
            maxTaskRetryCount = 1234,
        },
        jobManagerTask = new {
            id = "<id>",
            displayName = "<displayName>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            outputFiles = new[] {
                new {
                    filePattern = "<filePattern>",
                    destination = new {
                        container = new {
                            path = "<path>",
                            containerUrl = "<containerUrl>",
                            identityReference = new {
                                resourceId = "<resourceId>",
                            },
                            uploadHeaders = new[] {
                                new {
                                    name = "<name>",
                                    value = "<value>",
                                }
                            },
                        },
                    },
                    uploadOptions = new {
                        uploadCondition = "tasksuccess",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            constraints = new {
                maxWallClockTime = PT1H23M45S,
                retentionTime = PT1H23M45S,
                maxTaskRetryCount = 1234,
            },
            requiredSlots = 1234,
            killJobOnCompletion = true,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
            runExclusive = true,
            applicationPackageReferences = new[] {
                new {
                    applicationId = "<applicationId>",
                    version = "<version>",
                }
            },
            authenticationTokenSettings = new {
                access = new[] {
                    "job"
                },
            },
            allowLowPriorityNode = true,
        },
        jobPreparationTask = new {
            id = "<id>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            constraints = new {
                maxWallClockTime = PT1H23M45S,
                retentionTime = PT1H23M45S,
                maxTaskRetryCount = 1234,
            },
            waitForSuccess = true,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
            rerunOnNodeRebootAfterSuccess = true,
        },
        jobReleaseTask = new {
            id = "<id>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            maxWallClockTime = PT1H23M45S,
            retentionTime = PT1H23M45S,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
        },
        commonEnvironmentSettings = new[] {
            new {
                name = "<name>",
                value = "<value>",
            }
        },
        poolInfo = new {
            poolId = "<poolId>",
            autoPoolSpecification = new {
                autoPoolIdPrefix = "<autoPoolIdPrefix>",
                poolLifetimeOption = "jobschedule",
                keepAlive = true,
                pool = new {
                    displayName = "<displayName>",
                    vmSize = "<vmSize>",
                    cloudServiceConfiguration = new {
                        osFamily = "<osFamily>",
                        osVersion = "<osVersion>",
                    },
                    virtualMachineConfiguration = new {
                        imageReference = new {
                            publisher = "<publisher>",
                            offer = "<offer>",
                            sku = "<sku>",
                            version = "<version>",
                            virtualMachineImageId = "<virtualMachineImageId>",
                        },
                        nodeAgentSKUId = "<nodeAgentSKUId>",
                        windowsConfiguration = new {
                            enableAutomaticUpdates = true,
                        },
                        dataDisks = new[] {
                            new {
                                lun = 1234,
                                caching = "none",
                                diskSizeGB = 1234,
                                storageAccountType = "standard_lrs",
                            }
                        },
                        licenseType = "<licenseType>",
                        containerConfiguration = new {
                            type = "dockerCompatible",
                            containerImageNames = new[] {
                                "<String>"
                            },
                            containerRegistries = new[] {
                                new {
                                    username = "<username>",
                                    password = "<password>",
                                    registryServer = "<registryServer>",
                                    identityReference = new {
                                        resourceId = "<resourceId>",
                                    },
                                }
                            },
                        },
                        diskEncryptionConfiguration = new {
                            targets = new[] {
                                "osdisk"
                            },
                        },
                        nodePlacementConfiguration = new {
                            policy = "regional",
                        },
                        extensions = new[] {
                            new {
                                name = "<name>",
                                publisher = "<publisher>",
                                type = "<type>",
                                typeHandlerVersion = "<typeHandlerVersion>",
                                autoUpgradeMinorVersion = true,
                                settings = new {},
                                protectedSettings = new {},
                                provisionAfterExtensions = new[] {
                                    "<String>"
                                },
                            }
                        },
                        osDisk = new {
                            ephemeralOSDiskSettings = new {
                                placement = "cachedisk",
                            },
                        },
                    },
                    taskSlotsPerNode = 1234,
                    taskSchedulingPolicy = new {
                        nodeFillType = "spread",
                    },
                    resizeTimeout = PT1H23M45S,
                    targetDedicatedNodes = 1234,
                    targetLowPriorityNodes = 1234,
                    enableAutoScale = true,
                    autoScaleFormula = "<autoScaleFormula>",
                    autoScaleEvaluationInterval = PT1H23M45S,
                    enableInterNodeCommunication = true,
                    networkConfiguration = new {
                        subnetId = "<subnetId>",
                        dynamicVNetAssignmentScope = "none",
                        endpointConfiguration = new {
                            inboundNATPools = new[] {
                                new {
                                    name = "<name>",
                                    protocol = "tcp",
                                    backendPort = 1234,
                                    frontendPortRangeStart = 1234,
                                    frontendPortRangeEnd = 1234,
                                    networkSecurityGroupRules = new[] {
                                        new {
                                            priority = 1234,
                                            access = "allow",
                                            sourceAddressPrefix = "<sourceAddressPrefix>",
                                            sourcePortRanges = new[] {
                                                "<String>"
                                            },
                                        }
                                    },
                                }
                            },
                        },
                        publicIPAddressConfiguration = new {
                            provision = "batchmanaged",
                            ipAddressIds = new[] {
                                "<String>"
                            },
                        },
                    },
                    startTask = new {
                        commandLine = "<commandLine>",
                        containerSettings = new {
                            containerRunOptions = "<containerRunOptions>",
                            imageName = "<imageName>",
                            registry = new {
                                username = "<username>",
                                password = "<password>",
                                registryServer = "<registryServer>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            },
                            workingDirectory = "taskWorkingDirectory",
                        },
                        resourceFiles = new[] {
                            new {
                                autoStorageContainerName = "<autoStorageContainerName>",
                                storageContainerUrl = "<storageContainerUrl>",
                                httpUrl = "<httpUrl>",
                                blobPrefix = "<blobPrefix>",
                                filePath = "<filePath>",
                                fileMode = "<fileMode>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            }
                        },
                        environmentSettings = new[] {
                            new {
                                name = "<name>",
                                value = "<value>",
                            }
                        },
                        userIdentity = new {
                            username = "<username>",
                            autoUser = new {
                                scope = "task",
                                elevationLevel = "nonadmin",
                            },
                        },
                        maxTaskRetryCount = 1234,
                        waitForSuccess = true,
                    },
                    certificateReferences = new[] {
                        new {
                            thumbprint = "<thumbprint>",
                            thumbprintAlgorithm = "<thumbprintAlgorithm>",
                            storeLocation = "currentuser",
                            storeName = "<storeName>",
                            visibility = new[] {
                                "starttask"
                            },
                        }
                    },
                    applicationPackageReferences = new[] {
                        new {
                            applicationId = "<applicationId>",
                            version = "<version>",
                        }
                    },
                    applicationLicenses = new[] {
                        "<String>"
                    },
                    userAccounts = new[] {
                        new {
                            name = "<name>",
                            password = "<password>",
                            elevationLevel = "nonadmin",
                            linuxUserConfiguration = new {
                                uid = 1234,
                                gid = 1234,
                                sshPrivateKey = "<sshPrivateKey>",
                            },
                            windowsUserConfiguration = new {
                                loginMode = "batch",
                            },
                        }
                    },
                    metadata = new[] {
                        new {
                            name = "<name>",
                            value = "<value>",
                        }
                    },
                    mountConfiguration = new[] {
                        new {
                            azureBlobFileSystemConfiguration = new {
                                accountName = "<accountName>",
                                containerName = "<containerName>",
                                accountKey = "<accountKey>",
                                sasKey = "<sasKey>",
                                blobfuseOptions = "<blobfuseOptions>",
                                relativeMountPath = "<relativeMountPath>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            },
                            nfsMountConfiguration = new {
                                source = "<source>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                            },
                            cifsMountConfiguration = new {
                                username = "<username>",
                                source = "<source>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                                password = "<password>",
                            },
                            azureFileShareConfiguration = new {
                                accountName = "<accountName>",
                                azureFileUrl = "<azureFileUrl>",
                                accountKey = "<accountKey>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                            },
                        }
                    },
                    targetNodeCommunicationMode = "default",
                },
            },
        },
        metadata = new[] {
            new {
                name = "<name>",
                value = "<value>",
            }
        },
    },
    metadata = new[] {
        new {
            name = "<name>",
            value = "<value>",
        }
    },
};

Response response = client.Update("<jobScheduleId>", RequestContent.Create(data), 1234, "<clientRequestId>", true, "<ocpDate>", null);
Console.WriteLine(response.Status);
]]></code>
</example>
<remarks>
This fully replaces all the updatable properties of the Job Schedule. For
example, if the schedule property is not specified with this request, then the
Batch service will remove the existing schedule. Changes to a Job Schedule only
impact Jobs created by the schedule after the update has taken place; currently
running Jobs are unaffected.

Below is the JSON schema for the request payload.

Request Body:

Schema for <c>BatchJobSchedule</c>:
<code>{
  id: string, # Optional. A string that uniquely identifies the schedule within the Account.
  displayName: string, # Optional. The display name for the schedule.
  url: string, # Optional. The URL of the Job Schedule.
  eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job Schedule has
changed between requests. In particular, you can be pass the ETag with an
Update Job Schedule request to specify that your changes should take effect
only if nobody else has modified the schedule in the meantime.
  lastModified: string (date &amp; time), # Optional. This is the last time at which the schedule level data, such as the Job
specification or recurrence information, changed. It does not factor in
job-level changes such as new Jobs being created or Jobs changing state.
  creationTime: string (date &amp; time), # Optional. The creation time of the Job Schedule.
  state: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. The state of the Job Schedule.
  stateTransitionTime: string (date &amp; time), # Optional. The time at which the Job Schedule entered the current state.
  previousState: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. This property is not present if the Job Schedule is in its initial active state.
  previousStateTransitionTime: string (date &amp; time), # Optional. This property is not present if the Job Schedule is in its initial active state.
  schedule: {
    doNotRunUntil: string (date &amp; time), # Optional. If you do not specify a doNotRunUntil time, the schedule becomes ready to
create Jobs immediately.
    doNotRunAfter: string (date &amp; time), # Optional. If you do not specify a doNotRunAfter time, and you are creating a recurring
Job Schedule, the Job Schedule will remain active until you explicitly
terminate it.
    startWindow: string (duration ISO 8601 Format), # Optional. If a Job is not created within the startWindow interval, then the &apos;opportunity&apos;
is lost; no Job will be created until the next recurrence of the schedule. If
the schedule is recurring, and the startWindow is longer than the recurrence
interval, then this is equivalent to an infinite startWindow, because the Job
that is &apos;due&apos; in one recurrenceInterval is not carried forward into the next
recurrence interval. The default is infinite. The minimum value is 1 minute. If
you specify a lower value, the Batch service rejects the schedule with an
error; if you are calling the REST API directly, the HTTP status code is 400
(Bad Request).
    recurrenceInterval: string (duration ISO 8601 Format), # Optional. Because a Job Schedule can have at most one active Job under it at any given
time, if it is time to create a new Job under a Job Schedule, but the previous
Job is still running, the Batch service will not create the new Job until the
previous Job finishes. If the previous Job does not finish within the
startWindow period of the new recurrenceInterval, then no new Job will be
scheduled for that interval. For recurring Jobs, you should normally specify a
jobManagerTask in the jobSpecification. If you do not use jobManagerTask, you
will need an external process to monitor when Jobs are created, add Tasks to
the Jobs and terminate the Jobs ready for the next recurrence. The default is
that the schedule does not recur: one Job is created, within the startWindow
after the doNotRunUntil time, and the schedule is complete as soon as that Job
finishes. The minimum value is 1 minute. If you specify a lower value, the
Batch service rejects the schedule with an error; if you are calling the REST
API directly, the HTTP status code is 400 (Bad Request).
  }, # Optional. All times are fixed respective to UTC and are not impacted by daylight saving
time.
  jobSpecification: {
    priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest
priority and 1000 being the highest priority. The default value is 0. This
priority is used as the default for all Jobs under the Job Schedule. You can
update a Job&apos;s priority after it has been created using by using the update Job
API.
    allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system
will take precedence and will be able requeue tasks from this job. You can
update a job&apos;s allowTaskPreemption after it has been created using the update
job API.
    maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not
specified, the default value is -1, which means there&apos;s no limit to the number
of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after
it has been created using the update job API.
    displayName: string, # Optional. The name need not be unique and can contain any Unicode characters up to a
maximum length of 1024.
    usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is
false.
    onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. Note that if a Job contains no Tasks, then all Tasks are considered complete.
This option is therefore most commonly used with a Job Manager task; if you
want to use automatic Job termination without a Job Manager, you should
initially set onAllTasksComplete to noaction and update the Job properties to
set onAllTasksComplete to terminatejob once you have finished adding Tasks. The
default is noaction.
    onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. The default is noaction.
    networkConfiguration: {
      subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes which will run Tasks from the Job. This
can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos;
service principal must have the &apos;Classic Virtual Machine Contributor&apos;
Role-Based Access Control (RBAC) role for the specified VNet so that Azure
Batch service can schedule Tasks on the Nodes. This can be verified by checking
if the specified VNet has any associated Network Security Groups (NSG). If
communication to the Nodes in the specified subnet is denied by an NSG, then
the Batch service will set the state of the Compute Nodes to unusable. This is
of the form
/subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}.
If the specified VNet has any associated Network Security Groups (NSG), then a
few reserved system ports must be enabled for inbound communication from the
Azure Batch service. For Pools created with a Virtual Machine configuration,
enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for
Windows. Port 443 is also required to be open for outbound connections for
communications to Azure Storage. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
    }, # Optional. The network configuration for the Job.
    constraints: {
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service
terminates it and any Tasks that are still running. In this case, the
termination reason will be MaxWallClockTimeExpiry. If this property is not
specified, there is no time limit on how long the Job may run.
      maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch
service will try each Task once, and may then retry up to this limit. For
example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one
initial try and 3 retries). If the maximum retry count is 0, the Batch service
does not retry Tasks. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
    }, # Optional. The execution constraints for a Job.
    jobManagerTask: {
      id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters.
      displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum
length of 1024.
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: {
        containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot;
command, in addition to those controlled by the Batch Service.
        imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If
no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a
default.
        registry: {
          username: string, # Optional. The user name to log into the registry server.
          password: string, # Optional. The password to log into the registry server.
          registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
          identityReference: {
            resourceId: string, # Optional. The ARM resource id of the user assigned identity.
          }, # Optional. The reference to a user assigned identity associated with the Batch pool which
a compute node will use.
        }, # Optional. This setting can be omitted if was already provided at Pool creation.
        workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
      }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must
be set as well. If the Pool that will run this Task doesn&apos;t have
containerConfiguration set, this must not be set. When this is specified, all
directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure
Batch directories on the node) are mapped into the container, all Task
environment variables are mapped into the container, and the Task command line
is executed in the container. Files produced in the container outside of
AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that
Batch file APIs will not be able to access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      outputFiles: [OutputFile], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node
on which the primary Task is executed.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Manager Task.
      constraints: {
        maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
        maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task
executable due to a nonzero exit code. The Batch service will try the Task
once, and may then retry up to this limit. For example, if the maximum retry
count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries).
If the maximum retry count is 0, the Batch service does not retry the Task
after the first attempt. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
      }, # Optional. Execution constraints to apply to a Task.
      requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the
node has enough free scheduling slots available. For multi-instance Tasks, this
property is not supported and must not be specified.
      killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job
as complete. If any Tasks are still running at this time (other than Job
Release), those Tasks are terminated. If false, the completion of the Job
Manager Task does not affect the Job status. In this case, you should either
use the onAllTasksComplete attribute to terminate the Job, or have a client or
user terminate the Job explicitly. An example of this is if the Job Manager
creates a set of Tasks but then takes no further role in their execution. The
default value is true. If you are using the onAllTasksComplete and
onTaskFailure attributes to control Job lifetime, and using the Job Manager
Task only to create the Tasks for the Job (not to monitor progress), then it is
important to set killJobOnCompletion to false.
      userIdentity: {
        username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
        autoUser: {
          scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task
should be specified if stricter isolation between tasks is required. For
example, if the task mutates the registry in a way which could impact other
tasks, or if certificates have been specified on the pool which should not be
accessible by normal tasks but should be accessible by StartTasks.
          elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
      }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
      runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job
Manager is running. If false, other Tasks can run simultaneously with the Job
Manager on a Compute Node. The Job Manager Task counts normally against the
Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute
Node allows multiple concurrent Tasks. The default value is true.
      applicationPackageReferences: [ApplicationPackageReference], # Optional. Application Packages are downloaded and deployed to a shared directory, not the
Task working directory. Therefore, if a referenced Application Package is
already on the Compute Node, and is up to date, then it is not re-downloaded;
the existing copy on the Compute Node is used. If a referenced Application
Package cannot be installed, for example because the package has been deleted
or because download failed, the Task fails.
      authenticationTokenSettings: {
        access: [&quot;job&quot;], # Optional. The authentication token grants access to a limited set of Batch service
operations. Currently the only supported value for the access property is
&apos;job&apos;, which grants access to all operations related to the Job which contains
the Task.
      }, # Optional. If this property is set, the Batch service provides the Task with an
authentication token which can be used to authenticate Batch service operations
without requiring an Account access key. The token is provided via the
AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the
Task can carry out using the token depend on the settings. For example, a Task
can request Job permissions in order to add other Tasks to the Job, or check
the status of the Job or of other Tasks under the Job.
      allowLowPriorityNode: boolean, # Optional. The default value is true.
    }, # Optional. If the Job does not specify a Job Manager Task, the user must explicitly add
Tasks to the Job using the Task API. If the Job does specify a Job Manager
Task, the Batch service creates the Job Manager Task when the Job is created,
and will try to schedule the Job Manager Task before scheduling other Tasks in
the Job.
    jobPreparationTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job
Preparation Task. If you try to submit a Task with the same id, the Batch
service rejects the request with error code TaskIdSameAsJobPreparationTask; if
you are calling the REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory. 
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
      constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
      waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries
the Job Preparation Task up to its maximum retry count (as specified in the
constraints element). If the Task has still not completed successfully after
all retries, then the Batch service will not schedule Tasks of the Job to the
Node. The Node remains active and eligible to run Tasks of other Jobs. If
false, the Batch service will not wait for the Job Preparation Task to
complete. In this case, other Tasks of the Job can start executing on the
Compute Node while the Job Preparation Task is still running; and even if the
Job Preparation Task fails, new Tasks will continue to be scheduled on the
Compute Node. The default value is true.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on
Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux
Compute Nodes.
      rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if
the Job Preparation Task did not complete (e.g. because the reboot occurred
while the Task was running). Therefore, you should always write a Job
Preparation Task to be idempotent and to behave correctly if run multiple
times. The default value is true.
    }, # Optional. If a Job has a Job Preparation Task, the Batch service will run the Job
Preparation Task on a Node before starting any Tasks of that Job on that
Compute Node.
    jobReleaseTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release
Task. If you try to submit a Task with the same id, the Batch service rejects
the request with error code TaskIdSameAsJobReleaseTask; if you are calling the
REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute
Node, measured from the time the Task starts. If the Task does not complete
within the time limit, the Batch service terminates it. The default value is 15
minutes. You may not specify a timeout longer than 15 minutes. If you do, the
Batch service rejects it with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
      retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
    }, # Optional. The primary purpose of the Job Release Task is to undo changes to Nodes made by
the Job Preparation Task. Example activities include deleting local files, or
shutting down services that were started as part of Job preparation. A Job
Release Task cannot be specified without also specifying a Job Preparation Task
for the Job. The Batch service runs the Job Release Task on the Compute Nodes
that have run the Job Preparation Task.
    commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by
specifying the same setting name with a different value.
    poolInfo: {
      poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool
does not exist at the time the Batch service tries to schedule a Job, no Tasks
for the Job will run until you create a Pool with that id. Note that the Batch
service will not reject the Job request; it will simply not run Tasks until the
Pool exists. You must specify either the Pool ID or the auto Pool
specification, but not both.
      autoPoolSpecification: {
        autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To
distinguish between Pools created for different purposes, you can specify this
element to add a prefix to the ID that is assigned. The prefix can be up to 20
characters long.
        poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule
are assigned to Pools.
        keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined
by the poolLifetimeOption setting) expires; that is, when the Job or Job
Schedule completes. If true, the Batch service does not delete the Pool
automatically. It is up to the user to delete auto Pools created with this
option.
        pool: {
          displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up
to a maximum length of 1024.
          vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose
a VM size for Compute Nodes in an Azure Batch Pool
(https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
          cloudServiceConfiguration: {
            osFamily: string, # Required. Possible values are:
2 - OS Family 2, equivalent to Windows Server 2008 R2
SP1.
3 - OS Family 3, equivalent to Windows Server 2012.
4 - OS Family 4,
equivalent to Windows Server 2012 R2.
5 - OS Family 5, equivalent to Windows
Server 2016.
6 - OS Family 6, equivalent to Windows Server 2019. For more
information, see Azure Guest OS Releases
(https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
            osVersion: string, # Optional. The default value is * which specifies the latest operating system version for
the specified OS family.
          }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS
VMs. This property and virtualMachineConfiguration are mutually exclusive and
one of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request). This property cannot be specified if the
Batch Account was created with its poolAllocationMode property set to
&apos;UserSubscription&apos;.
          virtualMachineConfiguration: {
            imageReference: {
              publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
              offer: string, # Optional. For example, UbuntuServer or WindowsServer.
              sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
              version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image.
If omitted, the default is &apos;latest&apos;.
              virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The
Shared Image Gallery Image must have replicas in the same region and must be in
the same subscription as the Azure Batch account. If the image version is not
specified in the imageId, the latest version will be used. For information
about the firewall settings for the Batch Compute Node agent to communicate
with the Batch service see
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
              exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create
the node. This read-only field differs from &apos;version&apos; only if the value
specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
            }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image
Gallery Image. To get the list of all Azure Marketplace Image references
verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
            nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the
Pool, and provides the command-and-control interface between the Compute Node
and the Batch service. There are different implementations of the Compute Node
agent, known as SKUs, for different operating systems. You must specify a
Compute Node agent SKU which matches the selected Image reference. To get the
list of supported Compute Node agent SKUs along with their list of verified
Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
            windowsConfiguration: {
              enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
            }, # Optional. This property must not be specified if the imageReference property specifies a
Linux OS Image.
            dataDisks: [DataDisk], # Optional. This property must be specified if the Compute Nodes in the Pool need to have
empty data disks attached to them. This cannot be updated. Each Compute Node
gets its own disk (the disk is not a file share). Existing disks cannot be
attached, each attached disk is empty. When the Compute Node is removed from
the Pool, the disk and all data associated with it is also deleted. The disk is
not formatted after being attached, it must be formatted before use - for more
information see
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux
and
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
            licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and
should only be used when you hold valid on-premises licenses for the Compute
Nodes which will be deployed. If omitted, no on-premises licensing discount is
applied. Values are:

 Windows_Server - The on-premises license is for Windows
Server.
 Windows_Client - The on-premises license is for Windows Client.

            containerConfiguration: {
              type: &quot;dockerCompatible&quot;, # Required. The container technology to be used.
              containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An
Image will be sourced from the default Docker registry unless the Image is
fully qualified with an alternative registry.
              containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires
credentials, then those credentials must be provided here.
            }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow
Tasks to run in containers. All regular Tasks and Job manager Tasks run on this
Pool must specify the containerSettings property, and all other Tasks may
specify it.
            diskEncryptionConfiguration: {
              targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On
Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot;
and &quot;TemporaryDisk&quot; must be specified.
            }, # Optional. If specified, encryption is performed on each node in the pool during node
provisioning.
            nodePlacementConfiguration: {
              policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not
specified, Batch will use the regional policy.
            }, # Optional. This configuration will specify rules on how nodes in the pool will be
physically allocated.
            extensions: [VMExtension], # Optional. If specified, the extensions mentioned in this configuration will be installed
on each node.
            osDisk: {
              ephemeralOSDiskSettings: {
                placement: &quot;cachedisk&quot;, # Optional. This property can be used by user in the request to choose the location e.g.,
cache disk space for Ephemeral OS disk provisioning. For more information on
Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size
requirements for Windows VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements
and Linux VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
              }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the
compute node (VM).
            }, # Optional. Settings for the operating system disk of the compute node (VM).
          }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS
VMs. This property and cloudServiceConfiguration are mutually exclusive and one
of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request).
          taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number
of cores of the vmSize of the pool or 256.
          taskSchedulingPolicy: {
            nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
          }, # Optional. If not specified, the default is spread.
          resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when
enableAutoScale is set to true. The default value is 15 minutes. The minimum
value is 5 minutes. If you specify a value less than 5 minutes, the Batch
service rejects the request with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
          targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must
be specified. If true, the autoScaleFormula element is required. The Pool
automatically resizes according to the formula. The default value is false.
          autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is
required if enableAutoScale is set to true. The formula is checked for validity
before the Pool is created. If the formula is not valid, the Batch service
rejects the request with detailed error information.
          autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes
and 168 hours respectively. If you specify a value less than 5 minutes or
greater than 168 hours, the Batch service rejects the request with an invalid
property value error; if you are calling the REST API directly, the HTTP status
code is 400 (Bad Request).
          enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to
deployment restrictions on the Compute Nodes of the Pool. This may result in
the Pool not reaching its desired size. The default value is false.
          networkConfiguration: {
            subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have
enough free IP addresses, the Pool will partially allocate Nodes and a resize
error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the
&apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for
the specified VNet. The specified subnet must allow communication from the
Azure Batch service to be able to schedule Tasks on the Nodes. This can be
verified by checking if the specified VNet has any associated Network Security
Groups (NSG). If communication to the Nodes in the specified subnet is denied
by an NSG, then the Batch service will set the state of the Compute Nodes to
unusable. For Pools created with virtualMachineConfiguration only ARM virtual
networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools
created with cloudServiceConfiguration both ARM and classic virtual networks
are supported. If the specified VNet has any associated Network Security Groups
(NSG), then a few reserved system ports must be enabled for inbound
communication. For Pools created with a virtual machine configuration, enable
ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows.
For Pools created with a cloud service configuration, enable ports 10100,
20100, and 30100. Also enable outbound connections to Azure Storage on port
443. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
            dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
            endpointConfiguration: {
              inboundNATPools: [InboundNATPool], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum
number of inbound NAT Pools is exceeded the request fails with HTTP status code
400. This cannot be specified if the IPAddressProvisioningType is
NoPublicIPAddresses.
            }, # Optional. Pool endpoint configuration is only supported on Pools with the
virtualMachineConfiguration property.
            publicIPAddressConfiguration: {
              provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
              ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100
dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public
IP. For example, a pool needing 250 dedicated VMs would need at least 3 public
IPs specified. Each element of this collection is of the form:
/subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
            }, # Optional. Public IP configuration property is only supported on Pools with the
virtualMachineConfiguration property.
          }, # Optional. The network configuration for a Pool.
          startTask: {
            commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
            containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
            resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
            environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
            userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
            maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this
value specifically controls the number of retries. The Batch service will try
the Task once, and may then retry up to this limit. For example, if the maximum
retry count is 3, Batch tries the Task up to 4 times (one initial try and 3
retries). If the maximum retry count is 0, the Batch service does not retry the
Task. If the maximum retry count is -1, the Batch service retries the Task
without limit, however this is not recommended for a start task or any task.
The default value is 0 (no retries)
            waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the
StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has
still not completed successfully after all retries, then the Batch service
marks the Node unusable, and will not schedule Tasks to it. This condition can
be detected via the Compute Node state and failure info details. If false, the
Batch service will not wait for the StartTask to complete. In this case, other
Tasks can start executing on the Compute Node while the StartTask is still
running; and even if the StartTask fails, new Tasks will continue to be
scheduled on the Compute Node. The default is true.
          }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node.
Examples of recovery operations include (but are not limited to) when an
unhealthy Node is rebooted or a Compute Node disappeared due to host failure.
Retries due to recovery operations are independent of and are not counted
against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal
retry due to a recovery operation may occur. Because of this, all Tasks should
be idempotent. This means Tasks need to tolerate being interrupted and
restarted without causing any corruption or duplicate data. The best practice
for long running Tasks is to use some form of checkpointing. In some cases the
StartTask may be re-run even though the Compute Node was not rebooted. Special
care should be taken to avoid StartTasks which create breakaway process or
install/launch services from the StartTask working directory, as this will
block Batch from being able to re-run the StartTask.
          certificateReferences: [CertificateReference], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified
Certificate store and location. For Linux Compute Nodes, the Certificates are
stored in a directory inside the Task working directory and an environment
variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this
location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory
is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and
Certificates are placed in that directory.
          applicationPackageReferences: [ApplicationPackageReference], # Optional. When creating a pool, the package&apos;s application ID must be fully qualified
(/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationName}).
Changes to Package references affect all new Nodes joining the Pool, but do not
affect Compute Nodes that are already in the Pool until they are rebooted or
reimaged. There is a maximum of 10 Package references on any given Pool.
          applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service
application licenses. If a license is requested which is not supported, Pool
creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;,
&apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application
license added to the Pool.
          userAccounts: [UserAccount], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
          metadata: [
            {
              name: string, # Required. The name of the metadata item.
              value: string, # Required. The value of the metadata item.
            }
          ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
          mountConfiguration: [MountConfiguration], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
          targetNodeCommunicationMode: &quot;default&quot; | &quot;classic&quot; | &quot;simplified&quot;, # Optional. If omitted, the default value is Default.
        }, # Optional. Specification for creating a new Pool.
      }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed
state, and the Pool creation error is set in the Job&apos;s scheduling error
property. The Batch service manages the lifetime (both creation and, unless
keepAlive is specified, deletion) of the auto Pool. Any user actions that
affect the lifetime of the auto Pool while the Job is active will result in
unexpected behavior. You must specify either the Pool ID or the auto Pool
specification, but not both.
    }, # Required. Specifies how a Job should be assigned to a Pool.
    metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  }, # Optional. Specifies details of the Jobs to be created on a schedule.
  executionInfo: {
    nextRunTime: string (date &amp; time), # Optional. This property is meaningful only if the schedule is in the active state when
the time comes around. For example, if the schedule is disabled, no Job will be
created at nextRunTime unless the Job is enabled before then.
    recentJob: {
      id: string, # Optional. The ID of the Job.
      url: string, # Optional. The URL of the Job.
    }, # Optional. This property is present only if the at least one Job has run under the
schedule.
    endTime: string (date &amp; time), # Optional. This property is set only if the Job Schedule is in the completed state.
  }, # Optional. Contains information about Jobs that have been and will be run under a Job
Schedule.
  metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  stats: {
    url: string, # Required. The URL of the statistics.
    startTime: string (date &amp; time), # Required. The start time of the time range covered by the statistics.
    lastUpdateTime: string (date &amp; time), # Required. The time at which the statistics were last updated. All statistics are limited
to the range between startTime and lastUpdateTime.
    userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    wallClockTime: string (duration ISO 8601 Format), # Required. The wall clock time is the elapsed time from when the Task started running on a
Compute Node to when it finished (or to the last time the statistics were
updated, if the Task had not finished by then). If a Task was retried, this
includes the wall clock time of all the Task retries.
    readIOps: number, # Required. The total number of disk read operations made by all Tasks in all Jobs created
under the schedule.
    writeIOps: number, # Required. The total number of disk write operations made by all Tasks in all Jobs created
under the schedule.
    readIOGiB: number, # Required. The total gibibytes read from disk by all Tasks in all Jobs created under the
schedule.
    writeIOGiB: number, # Required. The total gibibytes written to disk by all Tasks in all Jobs created under the
schedule.
    numSucceededTasks: number, # Required. The total number of Tasks successfully completed during the given time range in
Jobs created under the schedule. A Task completes successfully if it returns
exit code 0.
    numFailedTasks: number, # Required. The total number of Tasks that failed during the given time range in Jobs
created under the schedule. A Task fails if it exhausts its maximum retry count
without returning exit code 0.
    numTaskRetries: number, # Required. The total number of retries during the given time range on all Tasks in all
Jobs created under the schedule.
    waitTime: string (duration ISO 8601 Format), # Required. This value is only reported in the Account lifetime statistics; it is not
included in the Job statistics.
  }, # Optional. Resource usage statistics for a Job Schedule.
}
</code>

</remarks>
    </member>
    <member name="DisableAsync(String,Int32,String,Boolean,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call DisableAsync with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = await client.DisableAsync("<jobScheduleId>");
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call DisableAsync with all parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = await client.DisableAsync("<jobScheduleId>", 1234, "<clientRequestId>", true, "<ocpDate>", null);
Console.WriteLine(response.Status);
]]></code>
</example>
<remarks>
No new Jobs will be created until the Job Schedule is enabled again.
</remarks>
    </member>
    <member name="Disable(String,Int32,String,Boolean,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call Disable with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = client.Disable("<jobScheduleId>");
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call Disable with all parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = client.Disable("<jobScheduleId>", 1234, "<clientRequestId>", true, "<ocpDate>", null);
Console.WriteLine(response.Status);
]]></code>
</example>
<remarks>
No new Jobs will be created until the Job Schedule is enabled again.
</remarks>
    </member>
    <member name="EnableAsync(String,Int32,String,Boolean,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call EnableAsync with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = await client.EnableAsync("<jobScheduleId>");
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call EnableAsync with all parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = await client.EnableAsync("<jobScheduleId>", 1234, "<clientRequestId>", true, "<ocpDate>", null);
Console.WriteLine(response.Status);
]]></code>
</example>
    </member>
    <member name="Enable(String,Int32,String,Boolean,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call Enable with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = client.Enable("<jobScheduleId>");
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call Enable with all parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = client.Enable("<jobScheduleId>", 1234, "<clientRequestId>", true, "<ocpDate>", null);
Console.WriteLine(response.Status);
]]></code>
</example>
    </member>
    <member name="TerminateAsync(String,Int32,String,Boolean,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call TerminateAsync with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = await client.TerminateAsync("<jobScheduleId>");
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call TerminateAsync with all parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = await client.TerminateAsync("<jobScheduleId>", 1234, "<clientRequestId>", true, "<ocpDate>", null);
Console.WriteLine(response.Status);
]]></code>
</example>
    </member>
    <member name="Terminate(String,Int32,String,Boolean,String,RequestConditions,RequestContext)">
<example>
This sample shows how to call Terminate with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = client.Terminate("<jobScheduleId>");
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call Terminate with all parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = client.Terminate("<jobScheduleId>", 1234, "<clientRequestId>", true, "<ocpDate>", null);
Console.WriteLine(response.Status);
]]></code>
</example>
    </member>
    <member name="AddAsync(RequestContent,Int32,String,Boolean,String,RequestContext)">
<example>
This sample shows how to call AddAsync with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

var data = new {};

Response response = await client.AddAsync(RequestContent.Create(data));
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call AddAsync with all parameters and request content.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

var data = new {
    id = "<id>",
    displayName = "<displayName>",
    schedule = new {
        doNotRunUntil = "2022-05-10T14:57:31.2311892-04:00",
        doNotRunAfter = "2022-05-10T14:57:31.2311892-04:00",
        startWindow = PT1H23M45S,
        recurrenceInterval = PT1H23M45S,
    },
    jobSpecification = new {
        priority = 1234,
        allowTaskPreemption = true,
        maxParallelTasks = 1234,
        displayName = "<displayName>",
        usesTaskDependencies = true,
        onAllTasksComplete = "noaction",
        onTaskFailure = "noaction",
        networkConfiguration = new {
            subnetId = "<subnetId>",
        },
        constraints = new {
            maxWallClockTime = PT1H23M45S,
            maxTaskRetryCount = 1234,
        },
        jobManagerTask = new {
            id = "<id>",
            displayName = "<displayName>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            outputFiles = new[] {
                new {
                    filePattern = "<filePattern>",
                    destination = new {
                        container = new {
                            path = "<path>",
                            containerUrl = "<containerUrl>",
                            identityReference = new {
                                resourceId = "<resourceId>",
                            },
                            uploadHeaders = new[] {
                                new {
                                    name = "<name>",
                                    value = "<value>",
                                }
                            },
                        },
                    },
                    uploadOptions = new {
                        uploadCondition = "tasksuccess",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            constraints = new {
                maxWallClockTime = PT1H23M45S,
                retentionTime = PT1H23M45S,
                maxTaskRetryCount = 1234,
            },
            requiredSlots = 1234,
            killJobOnCompletion = true,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
            runExclusive = true,
            applicationPackageReferences = new[] {
                new {
                    applicationId = "<applicationId>",
                    version = "<version>",
                }
            },
            authenticationTokenSettings = new {
                access = new[] {
                    "job"
                },
            },
            allowLowPriorityNode = true,
        },
        jobPreparationTask = new {
            id = "<id>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            constraints = new {
                maxWallClockTime = PT1H23M45S,
                retentionTime = PT1H23M45S,
                maxTaskRetryCount = 1234,
            },
            waitForSuccess = true,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
            rerunOnNodeRebootAfterSuccess = true,
        },
        jobReleaseTask = new {
            id = "<id>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            maxWallClockTime = PT1H23M45S,
            retentionTime = PT1H23M45S,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
        },
        commonEnvironmentSettings = new[] {
            new {
                name = "<name>",
                value = "<value>",
            }
        },
        poolInfo = new {
            poolId = "<poolId>",
            autoPoolSpecification = new {
                autoPoolIdPrefix = "<autoPoolIdPrefix>",
                poolLifetimeOption = "jobschedule",
                keepAlive = true,
                pool = new {
                    displayName = "<displayName>",
                    vmSize = "<vmSize>",
                    cloudServiceConfiguration = new {
                        osFamily = "<osFamily>",
                        osVersion = "<osVersion>",
                    },
                    virtualMachineConfiguration = new {
                        imageReference = new {
                            publisher = "<publisher>",
                            offer = "<offer>",
                            sku = "<sku>",
                            version = "<version>",
                            virtualMachineImageId = "<virtualMachineImageId>",
                        },
                        nodeAgentSKUId = "<nodeAgentSKUId>",
                        windowsConfiguration = new {
                            enableAutomaticUpdates = true,
                        },
                        dataDisks = new[] {
                            new {
                                lun = 1234,
                                caching = "none",
                                diskSizeGB = 1234,
                                storageAccountType = "standard_lrs",
                            }
                        },
                        licenseType = "<licenseType>",
                        containerConfiguration = new {
                            type = "dockerCompatible",
                            containerImageNames = new[] {
                                "<String>"
                            },
                            containerRegistries = new[] {
                                new {
                                    username = "<username>",
                                    password = "<password>",
                                    registryServer = "<registryServer>",
                                    identityReference = new {
                                        resourceId = "<resourceId>",
                                    },
                                }
                            },
                        },
                        diskEncryptionConfiguration = new {
                            targets = new[] {
                                "osdisk"
                            },
                        },
                        nodePlacementConfiguration = new {
                            policy = "regional",
                        },
                        extensions = new[] {
                            new {
                                name = "<name>",
                                publisher = "<publisher>",
                                type = "<type>",
                                typeHandlerVersion = "<typeHandlerVersion>",
                                autoUpgradeMinorVersion = true,
                                settings = new {},
                                protectedSettings = new {},
                                provisionAfterExtensions = new[] {
                                    "<String>"
                                },
                            }
                        },
                        osDisk = new {
                            ephemeralOSDiskSettings = new {
                                placement = "cachedisk",
                            },
                        },
                    },
                    taskSlotsPerNode = 1234,
                    taskSchedulingPolicy = new {
                        nodeFillType = "spread",
                    },
                    resizeTimeout = PT1H23M45S,
                    targetDedicatedNodes = 1234,
                    targetLowPriorityNodes = 1234,
                    enableAutoScale = true,
                    autoScaleFormula = "<autoScaleFormula>",
                    autoScaleEvaluationInterval = PT1H23M45S,
                    enableInterNodeCommunication = true,
                    networkConfiguration = new {
                        subnetId = "<subnetId>",
                        dynamicVNetAssignmentScope = "none",
                        endpointConfiguration = new {
                            inboundNATPools = new[] {
                                new {
                                    name = "<name>",
                                    protocol = "tcp",
                                    backendPort = 1234,
                                    frontendPortRangeStart = 1234,
                                    frontendPortRangeEnd = 1234,
                                    networkSecurityGroupRules = new[] {
                                        new {
                                            priority = 1234,
                                            access = "allow",
                                            sourceAddressPrefix = "<sourceAddressPrefix>",
                                            sourcePortRanges = new[] {
                                                "<String>"
                                            },
                                        }
                                    },
                                }
                            },
                        },
                        publicIPAddressConfiguration = new {
                            provision = "batchmanaged",
                            ipAddressIds = new[] {
                                "<String>"
                            },
                        },
                    },
                    startTask = new {
                        commandLine = "<commandLine>",
                        containerSettings = new {
                            containerRunOptions = "<containerRunOptions>",
                            imageName = "<imageName>",
                            registry = new {
                                username = "<username>",
                                password = "<password>",
                                registryServer = "<registryServer>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            },
                            workingDirectory = "taskWorkingDirectory",
                        },
                        resourceFiles = new[] {
                            new {
                                autoStorageContainerName = "<autoStorageContainerName>",
                                storageContainerUrl = "<storageContainerUrl>",
                                httpUrl = "<httpUrl>",
                                blobPrefix = "<blobPrefix>",
                                filePath = "<filePath>",
                                fileMode = "<fileMode>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            }
                        },
                        environmentSettings = new[] {
                            new {
                                name = "<name>",
                                value = "<value>",
                            }
                        },
                        userIdentity = new {
                            username = "<username>",
                            autoUser = new {
                                scope = "task",
                                elevationLevel = "nonadmin",
                            },
                        },
                        maxTaskRetryCount = 1234,
                        waitForSuccess = true,
                    },
                    certificateReferences = new[] {
                        new {
                            thumbprint = "<thumbprint>",
                            thumbprintAlgorithm = "<thumbprintAlgorithm>",
                            storeLocation = "currentuser",
                            storeName = "<storeName>",
                            visibility = new[] {
                                "starttask"
                            },
                        }
                    },
                    applicationPackageReferences = new[] {
                        new {
                            applicationId = "<applicationId>",
                            version = "<version>",
                        }
                    },
                    applicationLicenses = new[] {
                        "<String>"
                    },
                    userAccounts = new[] {
                        new {
                            name = "<name>",
                            password = "<password>",
                            elevationLevel = "nonadmin",
                            linuxUserConfiguration = new {
                                uid = 1234,
                                gid = 1234,
                                sshPrivateKey = "<sshPrivateKey>",
                            },
                            windowsUserConfiguration = new {
                                loginMode = "batch",
                            },
                        }
                    },
                    metadata = new[] {
                        new {
                            name = "<name>",
                            value = "<value>",
                        }
                    },
                    mountConfiguration = new[] {
                        new {
                            azureBlobFileSystemConfiguration = new {
                                accountName = "<accountName>",
                                containerName = "<containerName>",
                                accountKey = "<accountKey>",
                                sasKey = "<sasKey>",
                                blobfuseOptions = "<blobfuseOptions>",
                                relativeMountPath = "<relativeMountPath>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            },
                            nfsMountConfiguration = new {
                                source = "<source>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                            },
                            cifsMountConfiguration = new {
                                username = "<username>",
                                source = "<source>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                                password = "<password>",
                            },
                            azureFileShareConfiguration = new {
                                accountName = "<accountName>",
                                azureFileUrl = "<azureFileUrl>",
                                accountKey = "<accountKey>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                            },
                        }
                    },
                    targetNodeCommunicationMode = "default",
                },
            },
        },
        metadata = new[] {
            new {
                name = "<name>",
                value = "<value>",
            }
        },
    },
    metadata = new[] {
        new {
            name = "<name>",
            value = "<value>",
        }
    },
};

Response response = await client.AddAsync(RequestContent.Create(data), 1234, "<clientRequestId>", true, "<ocpDate>");
Console.WriteLine(response.Status);
]]></code>
</example>
<remarks>
Below is the JSON schema for the request payload.

Request Body:

Schema for <c>BatchJobSchedule</c>:
<code>{
  id: string, # Optional. A string that uniquely identifies the schedule within the Account.
  displayName: string, # Optional. The display name for the schedule.
  url: string, # Optional. The URL of the Job Schedule.
  eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job Schedule has
changed between requests. In particular, you can be pass the ETag with an
Update Job Schedule request to specify that your changes should take effect
only if nobody else has modified the schedule in the meantime.
  lastModified: string (date &amp; time), # Optional. This is the last time at which the schedule level data, such as the Job
specification or recurrence information, changed. It does not factor in
job-level changes such as new Jobs being created or Jobs changing state.
  creationTime: string (date &amp; time), # Optional. The creation time of the Job Schedule.
  state: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. The state of the Job Schedule.
  stateTransitionTime: string (date &amp; time), # Optional. The time at which the Job Schedule entered the current state.
  previousState: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. This property is not present if the Job Schedule is in its initial active state.
  previousStateTransitionTime: string (date &amp; time), # Optional. This property is not present if the Job Schedule is in its initial active state.
  schedule: {
    doNotRunUntil: string (date &amp; time), # Optional. If you do not specify a doNotRunUntil time, the schedule becomes ready to
create Jobs immediately.
    doNotRunAfter: string (date &amp; time), # Optional. If you do not specify a doNotRunAfter time, and you are creating a recurring
Job Schedule, the Job Schedule will remain active until you explicitly
terminate it.
    startWindow: string (duration ISO 8601 Format), # Optional. If a Job is not created within the startWindow interval, then the &apos;opportunity&apos;
is lost; no Job will be created until the next recurrence of the schedule. If
the schedule is recurring, and the startWindow is longer than the recurrence
interval, then this is equivalent to an infinite startWindow, because the Job
that is &apos;due&apos; in one recurrenceInterval is not carried forward into the next
recurrence interval. The default is infinite. The minimum value is 1 minute. If
you specify a lower value, the Batch service rejects the schedule with an
error; if you are calling the REST API directly, the HTTP status code is 400
(Bad Request).
    recurrenceInterval: string (duration ISO 8601 Format), # Optional. Because a Job Schedule can have at most one active Job under it at any given
time, if it is time to create a new Job under a Job Schedule, but the previous
Job is still running, the Batch service will not create the new Job until the
previous Job finishes. If the previous Job does not finish within the
startWindow period of the new recurrenceInterval, then no new Job will be
scheduled for that interval. For recurring Jobs, you should normally specify a
jobManagerTask in the jobSpecification. If you do not use jobManagerTask, you
will need an external process to monitor when Jobs are created, add Tasks to
the Jobs and terminate the Jobs ready for the next recurrence. The default is
that the schedule does not recur: one Job is created, within the startWindow
after the doNotRunUntil time, and the schedule is complete as soon as that Job
finishes. The minimum value is 1 minute. If you specify a lower value, the
Batch service rejects the schedule with an error; if you are calling the REST
API directly, the HTTP status code is 400 (Bad Request).
  }, # Optional. All times are fixed respective to UTC and are not impacted by daylight saving
time.
  jobSpecification: {
    priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest
priority and 1000 being the highest priority. The default value is 0. This
priority is used as the default for all Jobs under the Job Schedule. You can
update a Job&apos;s priority after it has been created using by using the update Job
API.
    allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system
will take precedence and will be able requeue tasks from this job. You can
update a job&apos;s allowTaskPreemption after it has been created using the update
job API.
    maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not
specified, the default value is -1, which means there&apos;s no limit to the number
of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after
it has been created using the update job API.
    displayName: string, # Optional. The name need not be unique and can contain any Unicode characters up to a
maximum length of 1024.
    usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is
false.
    onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. Note that if a Job contains no Tasks, then all Tasks are considered complete.
This option is therefore most commonly used with a Job Manager task; if you
want to use automatic Job termination without a Job Manager, you should
initially set onAllTasksComplete to noaction and update the Job properties to
set onAllTasksComplete to terminatejob once you have finished adding Tasks. The
default is noaction.
    onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. The default is noaction.
    networkConfiguration: {
      subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes which will run Tasks from the Job. This
can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos;
service principal must have the &apos;Classic Virtual Machine Contributor&apos;
Role-Based Access Control (RBAC) role for the specified VNet so that Azure
Batch service can schedule Tasks on the Nodes. This can be verified by checking
if the specified VNet has any associated Network Security Groups (NSG). If
communication to the Nodes in the specified subnet is denied by an NSG, then
the Batch service will set the state of the Compute Nodes to unusable. This is
of the form
/subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}.
If the specified VNet has any associated Network Security Groups (NSG), then a
few reserved system ports must be enabled for inbound communication from the
Azure Batch service. For Pools created with a Virtual Machine configuration,
enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for
Windows. Port 443 is also required to be open for outbound connections for
communications to Azure Storage. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
    }, # Optional. The network configuration for the Job.
    constraints: {
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service
terminates it and any Tasks that are still running. In this case, the
termination reason will be MaxWallClockTimeExpiry. If this property is not
specified, there is no time limit on how long the Job may run.
      maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch
service will try each Task once, and may then retry up to this limit. For
example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one
initial try and 3 retries). If the maximum retry count is 0, the Batch service
does not retry Tasks. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
    }, # Optional. The execution constraints for a Job.
    jobManagerTask: {
      id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters.
      displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum
length of 1024.
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: {
        containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot;
command, in addition to those controlled by the Batch Service.
        imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If
no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a
default.
        registry: {
          username: string, # Optional. The user name to log into the registry server.
          password: string, # Optional. The password to log into the registry server.
          registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
          identityReference: {
            resourceId: string, # Optional. The ARM resource id of the user assigned identity.
          }, # Optional. The reference to a user assigned identity associated with the Batch pool which
a compute node will use.
        }, # Optional. This setting can be omitted if was already provided at Pool creation.
        workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
      }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must
be set as well. If the Pool that will run this Task doesn&apos;t have
containerConfiguration set, this must not be set. When this is specified, all
directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure
Batch directories on the node) are mapped into the container, all Task
environment variables are mapped into the container, and the Task command line
is executed in the container. Files produced in the container outside of
AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that
Batch file APIs will not be able to access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      outputFiles: [OutputFile], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node
on which the primary Task is executed.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Manager Task.
      constraints: {
        maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
        maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task
executable due to a nonzero exit code. The Batch service will try the Task
once, and may then retry up to this limit. For example, if the maximum retry
count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries).
If the maximum retry count is 0, the Batch service does not retry the Task
after the first attempt. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
      }, # Optional. Execution constraints to apply to a Task.
      requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the
node has enough free scheduling slots available. For multi-instance Tasks, this
property is not supported and must not be specified.
      killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job
as complete. If any Tasks are still running at this time (other than Job
Release), those Tasks are terminated. If false, the completion of the Job
Manager Task does not affect the Job status. In this case, you should either
use the onAllTasksComplete attribute to terminate the Job, or have a client or
user terminate the Job explicitly. An example of this is if the Job Manager
creates a set of Tasks but then takes no further role in their execution. The
default value is true. If you are using the onAllTasksComplete and
onTaskFailure attributes to control Job lifetime, and using the Job Manager
Task only to create the Tasks for the Job (not to monitor progress), then it is
important to set killJobOnCompletion to false.
      userIdentity: {
        username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
        autoUser: {
          scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task
should be specified if stricter isolation between tasks is required. For
example, if the task mutates the registry in a way which could impact other
tasks, or if certificates have been specified on the pool which should not be
accessible by normal tasks but should be accessible by StartTasks.
          elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
      }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
      runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job
Manager is running. If false, other Tasks can run simultaneously with the Job
Manager on a Compute Node. The Job Manager Task counts normally against the
Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute
Node allows multiple concurrent Tasks. The default value is true.
      applicationPackageReferences: [ApplicationPackageReference], # Optional. Application Packages are downloaded and deployed to a shared directory, not the
Task working directory. Therefore, if a referenced Application Package is
already on the Compute Node, and is up to date, then it is not re-downloaded;
the existing copy on the Compute Node is used. If a referenced Application
Package cannot be installed, for example because the package has been deleted
or because download failed, the Task fails.
      authenticationTokenSettings: {
        access: [&quot;job&quot;], # Optional. The authentication token grants access to a limited set of Batch service
operations. Currently the only supported value for the access property is
&apos;job&apos;, which grants access to all operations related to the Job which contains
the Task.
      }, # Optional. If this property is set, the Batch service provides the Task with an
authentication token which can be used to authenticate Batch service operations
without requiring an Account access key. The token is provided via the
AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the
Task can carry out using the token depend on the settings. For example, a Task
can request Job permissions in order to add other Tasks to the Job, or check
the status of the Job or of other Tasks under the Job.
      allowLowPriorityNode: boolean, # Optional. The default value is true.
    }, # Optional. If the Job does not specify a Job Manager Task, the user must explicitly add
Tasks to the Job using the Task API. If the Job does specify a Job Manager
Task, the Batch service creates the Job Manager Task when the Job is created,
and will try to schedule the Job Manager Task before scheduling other Tasks in
the Job.
    jobPreparationTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job
Preparation Task. If you try to submit a Task with the same id, the Batch
service rejects the request with error code TaskIdSameAsJobPreparationTask; if
you are calling the REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory. 
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
      constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
      waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries
the Job Preparation Task up to its maximum retry count (as specified in the
constraints element). If the Task has still not completed successfully after
all retries, then the Batch service will not schedule Tasks of the Job to the
Node. The Node remains active and eligible to run Tasks of other Jobs. If
false, the Batch service will not wait for the Job Preparation Task to
complete. In this case, other Tasks of the Job can start executing on the
Compute Node while the Job Preparation Task is still running; and even if the
Job Preparation Task fails, new Tasks will continue to be scheduled on the
Compute Node. The default value is true.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on
Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux
Compute Nodes.
      rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if
the Job Preparation Task did not complete (e.g. because the reboot occurred
while the Task was running). Therefore, you should always write a Job
Preparation Task to be idempotent and to behave correctly if run multiple
times. The default value is true.
    }, # Optional. If a Job has a Job Preparation Task, the Batch service will run the Job
Preparation Task on a Node before starting any Tasks of that Job on that
Compute Node.
    jobReleaseTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release
Task. If you try to submit a Task with the same id, the Batch service rejects
the request with error code TaskIdSameAsJobReleaseTask; if you are calling the
REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute
Node, measured from the time the Task starts. If the Task does not complete
within the time limit, the Batch service terminates it. The default value is 15
minutes. You may not specify a timeout longer than 15 minutes. If you do, the
Batch service rejects it with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
      retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
    }, # Optional. The primary purpose of the Job Release Task is to undo changes to Nodes made by
the Job Preparation Task. Example activities include deleting local files, or
shutting down services that were started as part of Job preparation. A Job
Release Task cannot be specified without also specifying a Job Preparation Task
for the Job. The Batch service runs the Job Release Task on the Compute Nodes
that have run the Job Preparation Task.
    commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by
specifying the same setting name with a different value.
    poolInfo: {
      poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool
does not exist at the time the Batch service tries to schedule a Job, no Tasks
for the Job will run until you create a Pool with that id. Note that the Batch
service will not reject the Job request; it will simply not run Tasks until the
Pool exists. You must specify either the Pool ID or the auto Pool
specification, but not both.
      autoPoolSpecification: {
        autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To
distinguish between Pools created for different purposes, you can specify this
element to add a prefix to the ID that is assigned. The prefix can be up to 20
characters long.
        poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule
are assigned to Pools.
        keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined
by the poolLifetimeOption setting) expires; that is, when the Job or Job
Schedule completes. If true, the Batch service does not delete the Pool
automatically. It is up to the user to delete auto Pools created with this
option.
        pool: {
          displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up
to a maximum length of 1024.
          vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose
a VM size for Compute Nodes in an Azure Batch Pool
(https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
          cloudServiceConfiguration: {
            osFamily: string, # Required. Possible values are:
2 - OS Family 2, equivalent to Windows Server 2008 R2
SP1.
3 - OS Family 3, equivalent to Windows Server 2012.
4 - OS Family 4,
equivalent to Windows Server 2012 R2.
5 - OS Family 5, equivalent to Windows
Server 2016.
6 - OS Family 6, equivalent to Windows Server 2019. For more
information, see Azure Guest OS Releases
(https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
            osVersion: string, # Optional. The default value is * which specifies the latest operating system version for
the specified OS family.
          }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS
VMs. This property and virtualMachineConfiguration are mutually exclusive and
one of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request). This property cannot be specified if the
Batch Account was created with its poolAllocationMode property set to
&apos;UserSubscription&apos;.
          virtualMachineConfiguration: {
            imageReference: {
              publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
              offer: string, # Optional. For example, UbuntuServer or WindowsServer.
              sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
              version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image.
If omitted, the default is &apos;latest&apos;.
              virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The
Shared Image Gallery Image must have replicas in the same region and must be in
the same subscription as the Azure Batch account. If the image version is not
specified in the imageId, the latest version will be used. For information
about the firewall settings for the Batch Compute Node agent to communicate
with the Batch service see
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
              exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create
the node. This read-only field differs from &apos;version&apos; only if the value
specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
            }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image
Gallery Image. To get the list of all Azure Marketplace Image references
verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
            nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the
Pool, and provides the command-and-control interface between the Compute Node
and the Batch service. There are different implementations of the Compute Node
agent, known as SKUs, for different operating systems. You must specify a
Compute Node agent SKU which matches the selected Image reference. To get the
list of supported Compute Node agent SKUs along with their list of verified
Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
            windowsConfiguration: {
              enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
            }, # Optional. This property must not be specified if the imageReference property specifies a
Linux OS Image.
            dataDisks: [DataDisk], # Optional. This property must be specified if the Compute Nodes in the Pool need to have
empty data disks attached to them. This cannot be updated. Each Compute Node
gets its own disk (the disk is not a file share). Existing disks cannot be
attached, each attached disk is empty. When the Compute Node is removed from
the Pool, the disk and all data associated with it is also deleted. The disk is
not formatted after being attached, it must be formatted before use - for more
information see
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux
and
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
            licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and
should only be used when you hold valid on-premises licenses for the Compute
Nodes which will be deployed. If omitted, no on-premises licensing discount is
applied. Values are:

 Windows_Server - The on-premises license is for Windows
Server.
 Windows_Client - The on-premises license is for Windows Client.

            containerConfiguration: {
              type: &quot;dockerCompatible&quot;, # Required. The container technology to be used.
              containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An
Image will be sourced from the default Docker registry unless the Image is
fully qualified with an alternative registry.
              containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires
credentials, then those credentials must be provided here.
            }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow
Tasks to run in containers. All regular Tasks and Job manager Tasks run on this
Pool must specify the containerSettings property, and all other Tasks may
specify it.
            diskEncryptionConfiguration: {
              targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On
Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot;
and &quot;TemporaryDisk&quot; must be specified.
            }, # Optional. If specified, encryption is performed on each node in the pool during node
provisioning.
            nodePlacementConfiguration: {
              policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not
specified, Batch will use the regional policy.
            }, # Optional. This configuration will specify rules on how nodes in the pool will be
physically allocated.
            extensions: [VMExtension], # Optional. If specified, the extensions mentioned in this configuration will be installed
on each node.
            osDisk: {
              ephemeralOSDiskSettings: {
                placement: &quot;cachedisk&quot;, # Optional. This property can be used by user in the request to choose the location e.g.,
cache disk space for Ephemeral OS disk provisioning. For more information on
Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size
requirements for Windows VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements
and Linux VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
              }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the
compute node (VM).
            }, # Optional. Settings for the operating system disk of the compute node (VM).
          }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS
VMs. This property and cloudServiceConfiguration are mutually exclusive and one
of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request).
          taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number
of cores of the vmSize of the pool or 256.
          taskSchedulingPolicy: {
            nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
          }, # Optional. If not specified, the default is spread.
          resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when
enableAutoScale is set to true. The default value is 15 minutes. The minimum
value is 5 minutes. If you specify a value less than 5 minutes, the Batch
service rejects the request with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
          targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must
be specified. If true, the autoScaleFormula element is required. The Pool
automatically resizes according to the formula. The default value is false.
          autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is
required if enableAutoScale is set to true. The formula is checked for validity
before the Pool is created. If the formula is not valid, the Batch service
rejects the request with detailed error information.
          autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes
and 168 hours respectively. If you specify a value less than 5 minutes or
greater than 168 hours, the Batch service rejects the request with an invalid
property value error; if you are calling the REST API directly, the HTTP status
code is 400 (Bad Request).
          enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to
deployment restrictions on the Compute Nodes of the Pool. This may result in
the Pool not reaching its desired size. The default value is false.
          networkConfiguration: {
            subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have
enough free IP addresses, the Pool will partially allocate Nodes and a resize
error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the
&apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for
the specified VNet. The specified subnet must allow communication from the
Azure Batch service to be able to schedule Tasks on the Nodes. This can be
verified by checking if the specified VNet has any associated Network Security
Groups (NSG). If communication to the Nodes in the specified subnet is denied
by an NSG, then the Batch service will set the state of the Compute Nodes to
unusable. For Pools created with virtualMachineConfiguration only ARM virtual
networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools
created with cloudServiceConfiguration both ARM and classic virtual networks
are supported. If the specified VNet has any associated Network Security Groups
(NSG), then a few reserved system ports must be enabled for inbound
communication. For Pools created with a virtual machine configuration, enable
ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows.
For Pools created with a cloud service configuration, enable ports 10100,
20100, and 30100. Also enable outbound connections to Azure Storage on port
443. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
            dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
            endpointConfiguration: {
              inboundNATPools: [InboundNATPool], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum
number of inbound NAT Pools is exceeded the request fails with HTTP status code
400. This cannot be specified if the IPAddressProvisioningType is
NoPublicIPAddresses.
            }, # Optional. Pool endpoint configuration is only supported on Pools with the
virtualMachineConfiguration property.
            publicIPAddressConfiguration: {
              provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
              ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100
dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public
IP. For example, a pool needing 250 dedicated VMs would need at least 3 public
IPs specified. Each element of this collection is of the form:
/subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
            }, # Optional. Public IP configuration property is only supported on Pools with the
virtualMachineConfiguration property.
          }, # Optional. The network configuration for a Pool.
          startTask: {
            commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
            containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
            resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
            environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
            userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
            maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this
value specifically controls the number of retries. The Batch service will try
the Task once, and may then retry up to this limit. For example, if the maximum
retry count is 3, Batch tries the Task up to 4 times (one initial try and 3
retries). If the maximum retry count is 0, the Batch service does not retry the
Task. If the maximum retry count is -1, the Batch service retries the Task
without limit, however this is not recommended for a start task or any task.
The default value is 0 (no retries)
            waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the
StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has
still not completed successfully after all retries, then the Batch service
marks the Node unusable, and will not schedule Tasks to it. This condition can
be detected via the Compute Node state and failure info details. If false, the
Batch service will not wait for the StartTask to complete. In this case, other
Tasks can start executing on the Compute Node while the StartTask is still
running; and even if the StartTask fails, new Tasks will continue to be
scheduled on the Compute Node. The default is true.
          }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node.
Examples of recovery operations include (but are not limited to) when an
unhealthy Node is rebooted or a Compute Node disappeared due to host failure.
Retries due to recovery operations are independent of and are not counted
against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal
retry due to a recovery operation may occur. Because of this, all Tasks should
be idempotent. This means Tasks need to tolerate being interrupted and
restarted without causing any corruption or duplicate data. The best practice
for long running Tasks is to use some form of checkpointing. In some cases the
StartTask may be re-run even though the Compute Node was not rebooted. Special
care should be taken to avoid StartTasks which create breakaway process or
install/launch services from the StartTask working directory, as this will
block Batch from being able to re-run the StartTask.
          certificateReferences: [CertificateReference], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified
Certificate store and location. For Linux Compute Nodes, the Certificates are
stored in a directory inside the Task working directory and an environment
variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this
location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory
is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and
Certificates are placed in that directory.
          applicationPackageReferences: [ApplicationPackageReference], # Optional. When creating a pool, the package&apos;s application ID must be fully qualified
(/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationName}).
Changes to Package references affect all new Nodes joining the Pool, but do not
affect Compute Nodes that are already in the Pool until they are rebooted or
reimaged. There is a maximum of 10 Package references on any given Pool.
          applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service
application licenses. If a license is requested which is not supported, Pool
creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;,
&apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application
license added to the Pool.
          userAccounts: [UserAccount], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
          metadata: [
            {
              name: string, # Required. The name of the metadata item.
              value: string, # Required. The value of the metadata item.
            }
          ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
          mountConfiguration: [MountConfiguration], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
          targetNodeCommunicationMode: &quot;default&quot; | &quot;classic&quot; | &quot;simplified&quot;, # Optional. If omitted, the default value is Default.
        }, # Optional. Specification for creating a new Pool.
      }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed
state, and the Pool creation error is set in the Job&apos;s scheduling error
property. The Batch service manages the lifetime (both creation and, unless
keepAlive is specified, deletion) of the auto Pool. Any user actions that
affect the lifetime of the auto Pool while the Job is active will result in
unexpected behavior. You must specify either the Pool ID or the auto Pool
specification, but not both.
    }, # Required. Specifies how a Job should be assigned to a Pool.
    metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  }, # Optional. Specifies details of the Jobs to be created on a schedule.
  executionInfo: {
    nextRunTime: string (date &amp; time), # Optional. This property is meaningful only if the schedule is in the active state when
the time comes around. For example, if the schedule is disabled, no Job will be
created at nextRunTime unless the Job is enabled before then.
    recentJob: {
      id: string, # Optional. The ID of the Job.
      url: string, # Optional. The URL of the Job.
    }, # Optional. This property is present only if the at least one Job has run under the
schedule.
    endTime: string (date &amp; time), # Optional. This property is set only if the Job Schedule is in the completed state.
  }, # Optional. Contains information about Jobs that have been and will be run under a Job
Schedule.
  metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  stats: {
    url: string, # Required. The URL of the statistics.
    startTime: string (date &amp; time), # Required. The start time of the time range covered by the statistics.
    lastUpdateTime: string (date &amp; time), # Required. The time at which the statistics were last updated. All statistics are limited
to the range between startTime and lastUpdateTime.
    userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    wallClockTime: string (duration ISO 8601 Format), # Required. The wall clock time is the elapsed time from when the Task started running on a
Compute Node to when it finished (or to the last time the statistics were
updated, if the Task had not finished by then). If a Task was retried, this
includes the wall clock time of all the Task retries.
    readIOps: number, # Required. The total number of disk read operations made by all Tasks in all Jobs created
under the schedule.
    writeIOps: number, # Required. The total number of disk write operations made by all Tasks in all Jobs created
under the schedule.
    readIOGiB: number, # Required. The total gibibytes read from disk by all Tasks in all Jobs created under the
schedule.
    writeIOGiB: number, # Required. The total gibibytes written to disk by all Tasks in all Jobs created under the
schedule.
    numSucceededTasks: number, # Required. The total number of Tasks successfully completed during the given time range in
Jobs created under the schedule. A Task completes successfully if it returns
exit code 0.
    numFailedTasks: number, # Required. The total number of Tasks that failed during the given time range in Jobs
created under the schedule. A Task fails if it exhausts its maximum retry count
without returning exit code 0.
    numTaskRetries: number, # Required. The total number of retries during the given time range on all Tasks in all
Jobs created under the schedule.
    waitTime: string (duration ISO 8601 Format), # Required. This value is only reported in the Account lifetime statistics; it is not
included in the Job statistics.
  }, # Optional. Resource usage statistics for a Job Schedule.
}
</code>

</remarks>
    </member>
    <member name="Add(RequestContent,Int32,String,Boolean,String,RequestContext)">
<example>
This sample shows how to call Add with required parameters.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

var data = new {};

Response response = client.Add(RequestContent.Create(data));
Console.WriteLine(response.Status);
]]></code>
This sample shows how to call Add with all parameters and request content.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

var data = new {
    id = "<id>",
    displayName = "<displayName>",
    schedule = new {
        doNotRunUntil = "2022-05-10T14:57:31.2311892-04:00",
        doNotRunAfter = "2022-05-10T14:57:31.2311892-04:00",
        startWindow = PT1H23M45S,
        recurrenceInterval = PT1H23M45S,
    },
    jobSpecification = new {
        priority = 1234,
        allowTaskPreemption = true,
        maxParallelTasks = 1234,
        displayName = "<displayName>",
        usesTaskDependencies = true,
        onAllTasksComplete = "noaction",
        onTaskFailure = "noaction",
        networkConfiguration = new {
            subnetId = "<subnetId>",
        },
        constraints = new {
            maxWallClockTime = PT1H23M45S,
            maxTaskRetryCount = 1234,
        },
        jobManagerTask = new {
            id = "<id>",
            displayName = "<displayName>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            outputFiles = new[] {
                new {
                    filePattern = "<filePattern>",
                    destination = new {
                        container = new {
                            path = "<path>",
                            containerUrl = "<containerUrl>",
                            identityReference = new {
                                resourceId = "<resourceId>",
                            },
                            uploadHeaders = new[] {
                                new {
                                    name = "<name>",
                                    value = "<value>",
                                }
                            },
                        },
                    },
                    uploadOptions = new {
                        uploadCondition = "tasksuccess",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            constraints = new {
                maxWallClockTime = PT1H23M45S,
                retentionTime = PT1H23M45S,
                maxTaskRetryCount = 1234,
            },
            requiredSlots = 1234,
            killJobOnCompletion = true,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
            runExclusive = true,
            applicationPackageReferences = new[] {
                new {
                    applicationId = "<applicationId>",
                    version = "<version>",
                }
            },
            authenticationTokenSettings = new {
                access = new[] {
                    "job"
                },
            },
            allowLowPriorityNode = true,
        },
        jobPreparationTask = new {
            id = "<id>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            constraints = new {
                maxWallClockTime = PT1H23M45S,
                retentionTime = PT1H23M45S,
                maxTaskRetryCount = 1234,
            },
            waitForSuccess = true,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
            rerunOnNodeRebootAfterSuccess = true,
        },
        jobReleaseTask = new {
            id = "<id>",
            commandLine = "<commandLine>",
            containerSettings = new {
                containerRunOptions = "<containerRunOptions>",
                imageName = "<imageName>",
                registry = new {
                    username = "<username>",
                    password = "<password>",
                    registryServer = "<registryServer>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                },
                workingDirectory = "taskWorkingDirectory",
            },
            resourceFiles = new[] {
                new {
                    autoStorageContainerName = "<autoStorageContainerName>",
                    storageContainerUrl = "<storageContainerUrl>",
                    httpUrl = "<httpUrl>",
                    blobPrefix = "<blobPrefix>",
                    filePath = "<filePath>",
                    fileMode = "<fileMode>",
                    identityReference = new {
                        resourceId = "<resourceId>",
                    },
                }
            },
            environmentSettings = new[] {
                new {
                    name = "<name>",
                    value = "<value>",
                }
            },
            maxWallClockTime = PT1H23M45S,
            retentionTime = PT1H23M45S,
            userIdentity = new {
                username = "<username>",
                autoUser = new {
                    scope = "task",
                    elevationLevel = "nonadmin",
                },
            },
        },
        commonEnvironmentSettings = new[] {
            new {
                name = "<name>",
                value = "<value>",
            }
        },
        poolInfo = new {
            poolId = "<poolId>",
            autoPoolSpecification = new {
                autoPoolIdPrefix = "<autoPoolIdPrefix>",
                poolLifetimeOption = "jobschedule",
                keepAlive = true,
                pool = new {
                    displayName = "<displayName>",
                    vmSize = "<vmSize>",
                    cloudServiceConfiguration = new {
                        osFamily = "<osFamily>",
                        osVersion = "<osVersion>",
                    },
                    virtualMachineConfiguration = new {
                        imageReference = new {
                            publisher = "<publisher>",
                            offer = "<offer>",
                            sku = "<sku>",
                            version = "<version>",
                            virtualMachineImageId = "<virtualMachineImageId>",
                        },
                        nodeAgentSKUId = "<nodeAgentSKUId>",
                        windowsConfiguration = new {
                            enableAutomaticUpdates = true,
                        },
                        dataDisks = new[] {
                            new {
                                lun = 1234,
                                caching = "none",
                                diskSizeGB = 1234,
                                storageAccountType = "standard_lrs",
                            }
                        },
                        licenseType = "<licenseType>",
                        containerConfiguration = new {
                            type = "dockerCompatible",
                            containerImageNames = new[] {
                                "<String>"
                            },
                            containerRegistries = new[] {
                                new {
                                    username = "<username>",
                                    password = "<password>",
                                    registryServer = "<registryServer>",
                                    identityReference = new {
                                        resourceId = "<resourceId>",
                                    },
                                }
                            },
                        },
                        diskEncryptionConfiguration = new {
                            targets = new[] {
                                "osdisk"
                            },
                        },
                        nodePlacementConfiguration = new {
                            policy = "regional",
                        },
                        extensions = new[] {
                            new {
                                name = "<name>",
                                publisher = "<publisher>",
                                type = "<type>",
                                typeHandlerVersion = "<typeHandlerVersion>",
                                autoUpgradeMinorVersion = true,
                                settings = new {},
                                protectedSettings = new {},
                                provisionAfterExtensions = new[] {
                                    "<String>"
                                },
                            }
                        },
                        osDisk = new {
                            ephemeralOSDiskSettings = new {
                                placement = "cachedisk",
                            },
                        },
                    },
                    taskSlotsPerNode = 1234,
                    taskSchedulingPolicy = new {
                        nodeFillType = "spread",
                    },
                    resizeTimeout = PT1H23M45S,
                    targetDedicatedNodes = 1234,
                    targetLowPriorityNodes = 1234,
                    enableAutoScale = true,
                    autoScaleFormula = "<autoScaleFormula>",
                    autoScaleEvaluationInterval = PT1H23M45S,
                    enableInterNodeCommunication = true,
                    networkConfiguration = new {
                        subnetId = "<subnetId>",
                        dynamicVNetAssignmentScope = "none",
                        endpointConfiguration = new {
                            inboundNATPools = new[] {
                                new {
                                    name = "<name>",
                                    protocol = "tcp",
                                    backendPort = 1234,
                                    frontendPortRangeStart = 1234,
                                    frontendPortRangeEnd = 1234,
                                    networkSecurityGroupRules = new[] {
                                        new {
                                            priority = 1234,
                                            access = "allow",
                                            sourceAddressPrefix = "<sourceAddressPrefix>",
                                            sourcePortRanges = new[] {
                                                "<String>"
                                            },
                                        }
                                    },
                                }
                            },
                        },
                        publicIPAddressConfiguration = new {
                            provision = "batchmanaged",
                            ipAddressIds = new[] {
                                "<String>"
                            },
                        },
                    },
                    startTask = new {
                        commandLine = "<commandLine>",
                        containerSettings = new {
                            containerRunOptions = "<containerRunOptions>",
                            imageName = "<imageName>",
                            registry = new {
                                username = "<username>",
                                password = "<password>",
                                registryServer = "<registryServer>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            },
                            workingDirectory = "taskWorkingDirectory",
                        },
                        resourceFiles = new[] {
                            new {
                                autoStorageContainerName = "<autoStorageContainerName>",
                                storageContainerUrl = "<storageContainerUrl>",
                                httpUrl = "<httpUrl>",
                                blobPrefix = "<blobPrefix>",
                                filePath = "<filePath>",
                                fileMode = "<fileMode>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            }
                        },
                        environmentSettings = new[] {
                            new {
                                name = "<name>",
                                value = "<value>",
                            }
                        },
                        userIdentity = new {
                            username = "<username>",
                            autoUser = new {
                                scope = "task",
                                elevationLevel = "nonadmin",
                            },
                        },
                        maxTaskRetryCount = 1234,
                        waitForSuccess = true,
                    },
                    certificateReferences = new[] {
                        new {
                            thumbprint = "<thumbprint>",
                            thumbprintAlgorithm = "<thumbprintAlgorithm>",
                            storeLocation = "currentuser",
                            storeName = "<storeName>",
                            visibility = new[] {
                                "starttask"
                            },
                        }
                    },
                    applicationPackageReferences = new[] {
                        new {
                            applicationId = "<applicationId>",
                            version = "<version>",
                        }
                    },
                    applicationLicenses = new[] {
                        "<String>"
                    },
                    userAccounts = new[] {
                        new {
                            name = "<name>",
                            password = "<password>",
                            elevationLevel = "nonadmin",
                            linuxUserConfiguration = new {
                                uid = 1234,
                                gid = 1234,
                                sshPrivateKey = "<sshPrivateKey>",
                            },
                            windowsUserConfiguration = new {
                                loginMode = "batch",
                            },
                        }
                    },
                    metadata = new[] {
                        new {
                            name = "<name>",
                            value = "<value>",
                        }
                    },
                    mountConfiguration = new[] {
                        new {
                            azureBlobFileSystemConfiguration = new {
                                accountName = "<accountName>",
                                containerName = "<containerName>",
                                accountKey = "<accountKey>",
                                sasKey = "<sasKey>",
                                blobfuseOptions = "<blobfuseOptions>",
                                relativeMountPath = "<relativeMountPath>",
                                identityReference = new {
                                    resourceId = "<resourceId>",
                                },
                            },
                            nfsMountConfiguration = new {
                                source = "<source>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                            },
                            cifsMountConfiguration = new {
                                username = "<username>",
                                source = "<source>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                                password = "<password>",
                            },
                            azureFileShareConfiguration = new {
                                accountName = "<accountName>",
                                azureFileUrl = "<azureFileUrl>",
                                accountKey = "<accountKey>",
                                relativeMountPath = "<relativeMountPath>",
                                mountOptions = "<mountOptions>",
                            },
                        }
                    },
                    targetNodeCommunicationMode = "default",
                },
            },
        },
        metadata = new[] {
            new {
                name = "<name>",
                value = "<value>",
            }
        },
    },
    metadata = new[] {
        new {
            name = "<name>",
            value = "<value>",
        }
    },
};

Response response = client.Add(RequestContent.Create(data), 1234, "<clientRequestId>", true, "<ocpDate>");
Console.WriteLine(response.Status);
]]></code>
</example>
<remarks>
Below is the JSON schema for the request payload.

Request Body:

Schema for <c>BatchJobSchedule</c>:
<code>{
  id: string, # Optional. A string that uniquely identifies the schedule within the Account.
  displayName: string, # Optional. The display name for the schedule.
  url: string, # Optional. The URL of the Job Schedule.
  eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job Schedule has
changed between requests. In particular, you can be pass the ETag with an
Update Job Schedule request to specify that your changes should take effect
only if nobody else has modified the schedule in the meantime.
  lastModified: string (date &amp; time), # Optional. This is the last time at which the schedule level data, such as the Job
specification or recurrence information, changed. It does not factor in
job-level changes such as new Jobs being created or Jobs changing state.
  creationTime: string (date &amp; time), # Optional. The creation time of the Job Schedule.
  state: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. The state of the Job Schedule.
  stateTransitionTime: string (date &amp; time), # Optional. The time at which the Job Schedule entered the current state.
  previousState: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. This property is not present if the Job Schedule is in its initial active state.
  previousStateTransitionTime: string (date &amp; time), # Optional. This property is not present if the Job Schedule is in its initial active state.
  schedule: {
    doNotRunUntil: string (date &amp; time), # Optional. If you do not specify a doNotRunUntil time, the schedule becomes ready to
create Jobs immediately.
    doNotRunAfter: string (date &amp; time), # Optional. If you do not specify a doNotRunAfter time, and you are creating a recurring
Job Schedule, the Job Schedule will remain active until you explicitly
terminate it.
    startWindow: string (duration ISO 8601 Format), # Optional. If a Job is not created within the startWindow interval, then the &apos;opportunity&apos;
is lost; no Job will be created until the next recurrence of the schedule. If
the schedule is recurring, and the startWindow is longer than the recurrence
interval, then this is equivalent to an infinite startWindow, because the Job
that is &apos;due&apos; in one recurrenceInterval is not carried forward into the next
recurrence interval. The default is infinite. The minimum value is 1 minute. If
you specify a lower value, the Batch service rejects the schedule with an
error; if you are calling the REST API directly, the HTTP status code is 400
(Bad Request).
    recurrenceInterval: string (duration ISO 8601 Format), # Optional. Because a Job Schedule can have at most one active Job under it at any given
time, if it is time to create a new Job under a Job Schedule, but the previous
Job is still running, the Batch service will not create the new Job until the
previous Job finishes. If the previous Job does not finish within the
startWindow period of the new recurrenceInterval, then no new Job will be
scheduled for that interval. For recurring Jobs, you should normally specify a
jobManagerTask in the jobSpecification. If you do not use jobManagerTask, you
will need an external process to monitor when Jobs are created, add Tasks to
the Jobs and terminate the Jobs ready for the next recurrence. The default is
that the schedule does not recur: one Job is created, within the startWindow
after the doNotRunUntil time, and the schedule is complete as soon as that Job
finishes. The minimum value is 1 minute. If you specify a lower value, the
Batch service rejects the schedule with an error; if you are calling the REST
API directly, the HTTP status code is 400 (Bad Request).
  }, # Optional. All times are fixed respective to UTC and are not impacted by daylight saving
time.
  jobSpecification: {
    priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest
priority and 1000 being the highest priority. The default value is 0. This
priority is used as the default for all Jobs under the Job Schedule. You can
update a Job&apos;s priority after it has been created using by using the update Job
API.
    allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system
will take precedence and will be able requeue tasks from this job. You can
update a job&apos;s allowTaskPreemption after it has been created using the update
job API.
    maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not
specified, the default value is -1, which means there&apos;s no limit to the number
of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after
it has been created using the update job API.
    displayName: string, # Optional. The name need not be unique and can contain any Unicode characters up to a
maximum length of 1024.
    usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is
false.
    onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. Note that if a Job contains no Tasks, then all Tasks are considered complete.
This option is therefore most commonly used with a Job Manager task; if you
want to use automatic Job termination without a Job Manager, you should
initially set onAllTasksComplete to noaction and update the Job properties to
set onAllTasksComplete to terminatejob once you have finished adding Tasks. The
default is noaction.
    onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. The default is noaction.
    networkConfiguration: {
      subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes which will run Tasks from the Job. This
can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos;
service principal must have the &apos;Classic Virtual Machine Contributor&apos;
Role-Based Access Control (RBAC) role for the specified VNet so that Azure
Batch service can schedule Tasks on the Nodes. This can be verified by checking
if the specified VNet has any associated Network Security Groups (NSG). If
communication to the Nodes in the specified subnet is denied by an NSG, then
the Batch service will set the state of the Compute Nodes to unusable. This is
of the form
/subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}.
If the specified VNet has any associated Network Security Groups (NSG), then a
few reserved system ports must be enabled for inbound communication from the
Azure Batch service. For Pools created with a Virtual Machine configuration,
enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for
Windows. Port 443 is also required to be open for outbound connections for
communications to Azure Storage. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
    }, # Optional. The network configuration for the Job.
    constraints: {
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service
terminates it and any Tasks that are still running. In this case, the
termination reason will be MaxWallClockTimeExpiry. If this property is not
specified, there is no time limit on how long the Job may run.
      maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch
service will try each Task once, and may then retry up to this limit. For
example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one
initial try and 3 retries). If the maximum retry count is 0, the Batch service
does not retry Tasks. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
    }, # Optional. The execution constraints for a Job.
    jobManagerTask: {
      id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters.
      displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum
length of 1024.
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: {
        containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot;
command, in addition to those controlled by the Batch Service.
        imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If
no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a
default.
        registry: {
          username: string, # Optional. The user name to log into the registry server.
          password: string, # Optional. The password to log into the registry server.
          registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
          identityReference: {
            resourceId: string, # Optional. The ARM resource id of the user assigned identity.
          }, # Optional. The reference to a user assigned identity associated with the Batch pool which
a compute node will use.
        }, # Optional. This setting can be omitted if was already provided at Pool creation.
        workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
      }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must
be set as well. If the Pool that will run this Task doesn&apos;t have
containerConfiguration set, this must not be set. When this is specified, all
directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure
Batch directories on the node) are mapped into the container, all Task
environment variables are mapped into the container, and the Task command line
is executed in the container. Files produced in the container outside of
AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that
Batch file APIs will not be able to access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      outputFiles: [OutputFile], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node
on which the primary Task is executed.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Manager Task.
      constraints: {
        maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
        retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
        maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task
executable due to a nonzero exit code. The Batch service will try the Task
once, and may then retry up to this limit. For example, if the maximum retry
count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries).
If the maximum retry count is 0, the Batch service does not retry the Task
after the first attempt. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
      }, # Optional. Execution constraints to apply to a Task.
      requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the
node has enough free scheduling slots available. For multi-instance Tasks, this
property is not supported and must not be specified.
      killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job
as complete. If any Tasks are still running at this time (other than Job
Release), those Tasks are terminated. If false, the completion of the Job
Manager Task does not affect the Job status. In this case, you should either
use the onAllTasksComplete attribute to terminate the Job, or have a client or
user terminate the Job explicitly. An example of this is if the Job Manager
creates a set of Tasks but then takes no further role in their execution. The
default value is true. If you are using the onAllTasksComplete and
onTaskFailure attributes to control Job lifetime, and using the Job Manager
Task only to create the Tasks for the Job (not to monitor progress), then it is
important to set killJobOnCompletion to false.
      userIdentity: {
        username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
        autoUser: {
          scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task
should be specified if stricter isolation between tasks is required. For
example, if the task mutates the registry in a way which could impact other
tasks, or if certificates have been specified on the pool which should not be
accessible by normal tasks but should be accessible by StartTasks.
          elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
        }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
      }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
      runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job
Manager is running. If false, other Tasks can run simultaneously with the Job
Manager on a Compute Node. The Job Manager Task counts normally against the
Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute
Node allows multiple concurrent Tasks. The default value is true.
      applicationPackageReferences: [ApplicationPackageReference], # Optional. Application Packages are downloaded and deployed to a shared directory, not the
Task working directory. Therefore, if a referenced Application Package is
already on the Compute Node, and is up to date, then it is not re-downloaded;
the existing copy on the Compute Node is used. If a referenced Application
Package cannot be installed, for example because the package has been deleted
or because download failed, the Task fails.
      authenticationTokenSettings: {
        access: [&quot;job&quot;], # Optional. The authentication token grants access to a limited set of Batch service
operations. Currently the only supported value for the access property is
&apos;job&apos;, which grants access to all operations related to the Job which contains
the Task.
      }, # Optional. If this property is set, the Batch service provides the Task with an
authentication token which can be used to authenticate Batch service operations
without requiring an Account access key. The token is provided via the
AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the
Task can carry out using the token depend on the settings. For example, a Task
can request Job permissions in order to add other Tasks to the Job, or check
the status of the Job or of other Tasks under the Job.
      allowLowPriorityNode: boolean, # Optional. The default value is true.
    }, # Optional. If the Job does not specify a Job Manager Task, the user must explicitly add
Tasks to the Job using the Task API. If the Job does specify a Job Manager
Task, the Batch service creates the Job Manager Task when the Job is created,
and will try to schedule the Job Manager Task before scheduling other Tasks in
the Job.
    jobPreparationTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job
Preparation Task. If you try to submit a Task with the same id, the Batch
service rejects the request with error code TaskIdSameAsJobPreparationTask; if
you are calling the REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory. 
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
      constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
      waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries
the Job Preparation Task up to its maximum retry count (as specified in the
constraints element). If the Task has still not completed successfully after
all retries, then the Batch service will not schedule Tasks of the Job to the
Node. The Node remains active and eligible to run Tasks of other Jobs. If
false, the Batch service will not wait for the Job Preparation Task to
complete. In this case, other Tasks of the Job can start executing on the
Compute Node while the Job Preparation Task is still running; and even if the
Job Preparation Task fails, new Tasks will continue to be scheduled on the
Compute Node. The default value is true.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on
Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux
Compute Nodes.
      rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if
the Job Preparation Task did not complete (e.g. because the reboot occurred
while the Task was running). Therefore, you should always write a Job
Preparation Task to be idempotent and to behave correctly if run multiple
times. The default value is true.
    }, # Optional. If a Job has a Job Preparation Task, the Batch service will run the Job
Preparation Task on a Node before starting any Tasks of that Job on that
Compute Node.
    jobReleaseTask: {
      id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release
Task. If you try to submit a Task with the same id, the Batch service rejects
the request with error code TaskIdSameAsJobReleaseTask; if you are calling the
REST API directly, the HTTP status code is 409 (Conflict).
      commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
      containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
      resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
      environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
      maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute
Node, measured from the time the Task starts. If the Task does not complete
within the time limit, the Batch service terminates it. The default value is 15
minutes. You may not specify a timeout longer than 15 minutes. If you do, the
Batch service rejects it with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
      retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
      userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
    }, # Optional. The primary purpose of the Job Release Task is to undo changes to Nodes made by
the Job Preparation Task. Example activities include deleting local files, or
shutting down services that were started as part of Job preparation. A Job
Release Task cannot be specified without also specifying a Job Preparation Task
for the Job. The Batch service runs the Job Release Task on the Compute Nodes
that have run the Job Preparation Task.
    commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by
specifying the same setting name with a different value.
    poolInfo: {
      poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool
does not exist at the time the Batch service tries to schedule a Job, no Tasks
for the Job will run until you create a Pool with that id. Note that the Batch
service will not reject the Job request; it will simply not run Tasks until the
Pool exists. You must specify either the Pool ID or the auto Pool
specification, but not both.
      autoPoolSpecification: {
        autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To
distinguish between Pools created for different purposes, you can specify this
element to add a prefix to the ID that is assigned. The prefix can be up to 20
characters long.
        poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule
are assigned to Pools.
        keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined
by the poolLifetimeOption setting) expires; that is, when the Job or Job
Schedule completes. If true, the Batch service does not delete the Pool
automatically. It is up to the user to delete auto Pools created with this
option.
        pool: {
          displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up
to a maximum length of 1024.
          vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose
a VM size for Compute Nodes in an Azure Batch Pool
(https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
          cloudServiceConfiguration: {
            osFamily: string, # Required. Possible values are:
2 - OS Family 2, equivalent to Windows Server 2008 R2
SP1.
3 - OS Family 3, equivalent to Windows Server 2012.
4 - OS Family 4,
equivalent to Windows Server 2012 R2.
5 - OS Family 5, equivalent to Windows
Server 2016.
6 - OS Family 6, equivalent to Windows Server 2019. For more
information, see Azure Guest OS Releases
(https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
            osVersion: string, # Optional. The default value is * which specifies the latest operating system version for
the specified OS family.
          }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS
VMs. This property and virtualMachineConfiguration are mutually exclusive and
one of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request). This property cannot be specified if the
Batch Account was created with its poolAllocationMode property set to
&apos;UserSubscription&apos;.
          virtualMachineConfiguration: {
            imageReference: {
              publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
              offer: string, # Optional. For example, UbuntuServer or WindowsServer.
              sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
              version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image.
If omitted, the default is &apos;latest&apos;.
              virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The
Shared Image Gallery Image must have replicas in the same region and must be in
the same subscription as the Azure Batch account. If the image version is not
specified in the imageId, the latest version will be used. For information
about the firewall settings for the Batch Compute Node agent to communicate
with the Batch service see
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
              exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create
the node. This read-only field differs from &apos;version&apos; only if the value
specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
            }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image
Gallery Image. To get the list of all Azure Marketplace Image references
verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
            nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the
Pool, and provides the command-and-control interface between the Compute Node
and the Batch service. There are different implementations of the Compute Node
agent, known as SKUs, for different operating systems. You must specify a
Compute Node agent SKU which matches the selected Image reference. To get the
list of supported Compute Node agent SKUs along with their list of verified
Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
            windowsConfiguration: {
              enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
            }, # Optional. This property must not be specified if the imageReference property specifies a
Linux OS Image.
            dataDisks: [DataDisk], # Optional. This property must be specified if the Compute Nodes in the Pool need to have
empty data disks attached to them. This cannot be updated. Each Compute Node
gets its own disk (the disk is not a file share). Existing disks cannot be
attached, each attached disk is empty. When the Compute Node is removed from
the Pool, the disk and all data associated with it is also deleted. The disk is
not formatted after being attached, it must be formatted before use - for more
information see
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux
and
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
            licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and
should only be used when you hold valid on-premises licenses for the Compute
Nodes which will be deployed. If omitted, no on-premises licensing discount is
applied. Values are:

 Windows_Server - The on-premises license is for Windows
Server.
 Windows_Client - The on-premises license is for Windows Client.

            containerConfiguration: {
              type: &quot;dockerCompatible&quot;, # Required. The container technology to be used.
              containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An
Image will be sourced from the default Docker registry unless the Image is
fully qualified with an alternative registry.
              containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires
credentials, then those credentials must be provided here.
            }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow
Tasks to run in containers. All regular Tasks and Job manager Tasks run on this
Pool must specify the containerSettings property, and all other Tasks may
specify it.
            diskEncryptionConfiguration: {
              targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On
Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot;
and &quot;TemporaryDisk&quot; must be specified.
            }, # Optional. If specified, encryption is performed on each node in the pool during node
provisioning.
            nodePlacementConfiguration: {
              policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not
specified, Batch will use the regional policy.
            }, # Optional. This configuration will specify rules on how nodes in the pool will be
physically allocated.
            extensions: [VMExtension], # Optional. If specified, the extensions mentioned in this configuration will be installed
on each node.
            osDisk: {
              ephemeralOSDiskSettings: {
                placement: &quot;cachedisk&quot;, # Optional. This property can be used by user in the request to choose the location e.g.,
cache disk space for Ephemeral OS disk provisioning. For more information on
Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size
requirements for Windows VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements
and Linux VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
              }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the
compute node (VM).
            }, # Optional. Settings for the operating system disk of the compute node (VM).
          }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS
VMs. This property and cloudServiceConfiguration are mutually exclusive and one
of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request).
          taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number
of cores of the vmSize of the pool or 256.
          taskSchedulingPolicy: {
            nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
          }, # Optional. If not specified, the default is spread.
          resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when
enableAutoScale is set to true. The default value is 15 minutes. The minimum
value is 5 minutes. If you specify a value less than 5 minutes, the Batch
service rejects the request with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
          targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
          enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must
be specified. If true, the autoScaleFormula element is required. The Pool
automatically resizes according to the formula. The default value is false.
          autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is
required if enableAutoScale is set to true. The formula is checked for validity
before the Pool is created. If the formula is not valid, the Batch service
rejects the request with detailed error information.
          autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes
and 168 hours respectively. If you specify a value less than 5 minutes or
greater than 168 hours, the Batch service rejects the request with an invalid
property value error; if you are calling the REST API directly, the HTTP status
code is 400 (Bad Request).
          enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to
deployment restrictions on the Compute Nodes of the Pool. This may result in
the Pool not reaching its desired size. The default value is false.
          networkConfiguration: {
            subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have
enough free IP addresses, the Pool will partially allocate Nodes and a resize
error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the
&apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for
the specified VNet. The specified subnet must allow communication from the
Azure Batch service to be able to schedule Tasks on the Nodes. This can be
verified by checking if the specified VNet has any associated Network Security
Groups (NSG). If communication to the Nodes in the specified subnet is denied
by an NSG, then the Batch service will set the state of the Compute Nodes to
unusable. For Pools created with virtualMachineConfiguration only ARM virtual
networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools
created with cloudServiceConfiguration both ARM and classic virtual networks
are supported. If the specified VNet has any associated Network Security Groups
(NSG), then a few reserved system ports must be enabled for inbound
communication. For Pools created with a virtual machine configuration, enable
ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows.
For Pools created with a cloud service configuration, enable ports 10100,
20100, and 30100. Also enable outbound connections to Azure Storage on port
443. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
            dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
            endpointConfiguration: {
              inboundNATPools: [InboundNATPool], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum
number of inbound NAT Pools is exceeded the request fails with HTTP status code
400. This cannot be specified if the IPAddressProvisioningType is
NoPublicIPAddresses.
            }, # Optional. Pool endpoint configuration is only supported on Pools with the
virtualMachineConfiguration property.
            publicIPAddressConfiguration: {
              provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
              ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100
dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public
IP. For example, a pool needing 250 dedicated VMs would need at least 3 public
IPs specified. Each element of this collection is of the form:
/subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
            }, # Optional. Public IP configuration property is only supported on Pools with the
virtualMachineConfiguration property.
          }, # Optional. The network configuration for a Pool.
          startTask: {
            commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
            containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
            resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
            environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
            userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
            maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this
value specifically controls the number of retries. The Batch service will try
the Task once, and may then retry up to this limit. For example, if the maximum
retry count is 3, Batch tries the Task up to 4 times (one initial try and 3
retries). If the maximum retry count is 0, the Batch service does not retry the
Task. If the maximum retry count is -1, the Batch service retries the Task
without limit, however this is not recommended for a start task or any task.
The default value is 0 (no retries)
            waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the
StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has
still not completed successfully after all retries, then the Batch service
marks the Node unusable, and will not schedule Tasks to it. This condition can
be detected via the Compute Node state and failure info details. If false, the
Batch service will not wait for the StartTask to complete. In this case, other
Tasks can start executing on the Compute Node while the StartTask is still
running; and even if the StartTask fails, new Tasks will continue to be
scheduled on the Compute Node. The default is true.
          }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node.
Examples of recovery operations include (but are not limited to) when an
unhealthy Node is rebooted or a Compute Node disappeared due to host failure.
Retries due to recovery operations are independent of and are not counted
against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal
retry due to a recovery operation may occur. Because of this, all Tasks should
be idempotent. This means Tasks need to tolerate being interrupted and
restarted without causing any corruption or duplicate data. The best practice
for long running Tasks is to use some form of checkpointing. In some cases the
StartTask may be re-run even though the Compute Node was not rebooted. Special
care should be taken to avoid StartTasks which create breakaway process or
install/launch services from the StartTask working directory, as this will
block Batch from being able to re-run the StartTask.
          certificateReferences: [CertificateReference], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified
Certificate store and location. For Linux Compute Nodes, the Certificates are
stored in a directory inside the Task working directory and an environment
variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this
location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory
is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and
Certificates are placed in that directory.
          applicationPackageReferences: [ApplicationPackageReference], # Optional. When creating a pool, the package&apos;s application ID must be fully qualified
(/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationName}).
Changes to Package references affect all new Nodes joining the Pool, but do not
affect Compute Nodes that are already in the Pool until they are rebooted or
reimaged. There is a maximum of 10 Package references on any given Pool.
          applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service
application licenses. If a license is requested which is not supported, Pool
creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;,
&apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application
license added to the Pool.
          userAccounts: [UserAccount], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
          metadata: [
            {
              name: string, # Required. The name of the metadata item.
              value: string, # Required. The value of the metadata item.
            }
          ], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
          mountConfiguration: [MountConfiguration], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
          targetNodeCommunicationMode: &quot;default&quot; | &quot;classic&quot; | &quot;simplified&quot;, # Optional. If omitted, the default value is Default.
        }, # Optional. Specification for creating a new Pool.
      }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed
state, and the Pool creation error is set in the Job&apos;s scheduling error
property. The Batch service manages the lifetime (both creation and, unless
keepAlive is specified, deletion) of the auto Pool. Any user actions that
affect the lifetime of the auto Pool while the Job is active will result in
unexpected behavior. You must specify either the Pool ID or the auto Pool
specification, but not both.
    }, # Required. Specifies how a Job should be assigned to a Pool.
    metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  }, # Optional. Specifies details of the Jobs to be created on a schedule.
  executionInfo: {
    nextRunTime: string (date &amp; time), # Optional. This property is meaningful only if the schedule is in the active state when
the time comes around. For example, if the schedule is disabled, no Job will be
created at nextRunTime unless the Job is enabled before then.
    recentJob: {
      id: string, # Optional. The ID of the Job.
      url: string, # Optional. The URL of the Job.
    }, # Optional. This property is present only if the at least one Job has run under the
schedule.
    endTime: string (date &amp; time), # Optional. This property is set only if the Job Schedule is in the completed state.
  }, # Optional. Contains information about Jobs that have been and will be run under a Job
Schedule.
  metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
  stats: {
    url: string, # Required. The URL of the statistics.
    startTime: string (date &amp; time), # Required. The start time of the time range covered by the statistics.
    lastUpdateTime: string (date &amp; time), # Required. The time at which the statistics were last updated. All statistics are limited
to the range between startTime and lastUpdateTime.
    userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
    wallClockTime: string (duration ISO 8601 Format), # Required. The wall clock time is the elapsed time from when the Task started running on a
Compute Node to when it finished (or to the last time the statistics were
updated, if the Task had not finished by then). If a Task was retried, this
includes the wall clock time of all the Task retries.
    readIOps: number, # Required. The total number of disk read operations made by all Tasks in all Jobs created
under the schedule.
    writeIOps: number, # Required. The total number of disk write operations made by all Tasks in all Jobs created
under the schedule.
    readIOGiB: number, # Required. The total gibibytes read from disk by all Tasks in all Jobs created under the
schedule.
    writeIOGiB: number, # Required. The total gibibytes written to disk by all Tasks in all Jobs created under the
schedule.
    numSucceededTasks: number, # Required. The total number of Tasks successfully completed during the given time range in
Jobs created under the schedule. A Task completes successfully if it returns
exit code 0.
    numFailedTasks: number, # Required. The total number of Tasks that failed during the given time range in Jobs
created under the schedule. A Task fails if it exhausts its maximum retry count
without returning exit code 0.
    numTaskRetries: number, # Required. The total number of retries during the given time range on all Tasks in all
Jobs created under the schedule.
    waitTime: string (duration ISO 8601 Format), # Required. This value is only reported in the Account lifetime statistics; it is not
included in the Job statistics.
  }, # Optional. Resource usage statistics for a Job Schedule.
}
</code>

</remarks>
    </member>
    <member name="GetJobSchedulesAsync(Int32,String,Int32,String,Boolean,String,String,String,RequestContext)">
<example>
This sample shows how to call GetJobSchedulesAsync and parse the result.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = await client.GetJobSchedulesAsync();

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.ToString());
]]></code>
This sample shows how to call GetJobSchedulesAsync with all parameters, and how to parse the result.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = await client.GetJobSchedulesAsync(1234, "<ocpDate>", 1234, "<clientRequestId>", true, "<filter>", "<select>", "<expand>");

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("value")[0].GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("url").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("eTag").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("lastModified").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("creationTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("state").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stateTransitionTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("previousState").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("previousStateTransitionTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("schedule").GetProperty("doNotRunUntil").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("schedule").GetProperty("doNotRunAfter").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("schedule").GetProperty("startWindow").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("schedule").GetProperty("recurrenceInterval").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("priority").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("allowTaskPreemption").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("maxParallelTasks").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("usesTaskDependencies").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("onAllTasksComplete").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("onTaskFailure").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("filePattern").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("path").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("containerUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("uploadOptions").GetProperty("uploadCondition").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("requiredSlots").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("killJobOnCompletion").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("runExclusive").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("authenticationTokenSettings").GetProperty("access")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("allowLowPriorityNode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("waitForSuccess").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("rerunOnNodeRebootAfterSuccess").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("retentionTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("commonEnvironmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("commonEnvironmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("poolId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("autoPoolIdPrefix").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("poolLifetimeOption").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("keepAlive").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("vmSize").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osFamily").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osVersion").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("publisher").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("offer").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("sku").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("version").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("virtualMachineImageId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("exactVersion").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodeAgentSKUId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("windowsConfiguration").GetProperty("enableAutomaticUpdates").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("lun").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("caching").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("diskSizeGB").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("storageAccountType").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("licenseType").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("type").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerImageNames")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("diskEncryptionConfiguration").GetProperty("targets")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodePlacementConfiguration").GetProperty("policy").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("publisher").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("type").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("typeHandlerVersion").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("autoUpgradeMinorVersion").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("provisionAfterExtensions")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("osDisk").GetProperty("ephemeralOSDiskSettings").GetProperty("placement").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSlotsPerNode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSchedulingPolicy").GetProperty("nodeFillType").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("resizeTimeout").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetDedicatedNodes").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetLowPriorityNodes").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableAutoScale").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleFormula").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleEvaluationInterval").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableInterNodeCommunication").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("dynamicVNetAssignmentScope").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("protocol").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("backendPort").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeStart").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeEnd").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("priority").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("access").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourceAddressPrefix").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourcePortRanges")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("provision").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("ipAddressIds")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("waitForSuccess").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprint").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprintAlgorithm").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeLocation").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("visibility")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationLicenses")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("uid").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("gid").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("sshPrivateKey").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("windowsUserConfiguration").GetProperty("loginMode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("containerName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountKey").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("sasKey").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("blobfuseOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("source").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("mountOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("source").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("mountOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("azureFileUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountKey").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("mountOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetNodeCommunicationMode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("metadata")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("metadata")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("executionInfo").GetProperty("nextRunTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("executionInfo").GetProperty("recentJob").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("executionInfo").GetProperty("recentJob").GetProperty("url").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("executionInfo").GetProperty("endTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("metadata")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("metadata")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("url").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("startTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("lastUpdateTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("userCPUTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("kernelCPUTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("wallClockTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("readIOps").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("writeIOps").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("readIOGiB").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("writeIOGiB").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("numSucceededTasks").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("numFailedTasks").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("numTaskRetries").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("waitTime").ToString());
Console.WriteLine(result.GetProperty("odata.nextLink").ToString());
]]></code>
</example>
<remarks>
Below is the JSON schema for the response payload.

Response Body:

Schema for <c>BatchJobScheduleListResult</c>:
<code>{
  value: [
    {
      id: string, # Optional. A string that uniquely identifies the schedule within the Account.
      displayName: string, # Optional. The display name for the schedule.
      url: string, # Optional. The URL of the Job Schedule.
      eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job Schedule has
changed between requests. In particular, you can be pass the ETag with an
Update Job Schedule request to specify that your changes should take effect
only if nobody else has modified the schedule in the meantime.
      lastModified: string (date &amp; time), # Optional. This is the last time at which the schedule level data, such as the Job
specification or recurrence information, changed. It does not factor in
job-level changes such as new Jobs being created or Jobs changing state.
      creationTime: string (date &amp; time), # Optional. The creation time of the Job Schedule.
      state: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. The state of the Job Schedule.
      stateTransitionTime: string (date &amp; time), # Optional. The time at which the Job Schedule entered the current state.
      previousState: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. This property is not present if the Job Schedule is in its initial active state.
      previousStateTransitionTime: string (date &amp; time), # Optional. This property is not present if the Job Schedule is in its initial active state.
      schedule: {
        doNotRunUntil: string (date &amp; time), # Optional. If you do not specify a doNotRunUntil time, the schedule becomes ready to
create Jobs immediately.
        doNotRunAfter: string (date &amp; time), # Optional. If you do not specify a doNotRunAfter time, and you are creating a recurring
Job Schedule, the Job Schedule will remain active until you explicitly
terminate it.
        startWindow: string (duration ISO 8601 Format), # Optional. If a Job is not created within the startWindow interval, then the &apos;opportunity&apos;
is lost; no Job will be created until the next recurrence of the schedule. If
the schedule is recurring, and the startWindow is longer than the recurrence
interval, then this is equivalent to an infinite startWindow, because the Job
that is &apos;due&apos; in one recurrenceInterval is not carried forward into the next
recurrence interval. The default is infinite. The minimum value is 1 minute. If
you specify a lower value, the Batch service rejects the schedule with an
error; if you are calling the REST API directly, the HTTP status code is 400
(Bad Request).
        recurrenceInterval: string (duration ISO 8601 Format), # Optional. Because a Job Schedule can have at most one active Job under it at any given
time, if it is time to create a new Job under a Job Schedule, but the previous
Job is still running, the Batch service will not create the new Job until the
previous Job finishes. If the previous Job does not finish within the
startWindow period of the new recurrenceInterval, then no new Job will be
scheduled for that interval. For recurring Jobs, you should normally specify a
jobManagerTask in the jobSpecification. If you do not use jobManagerTask, you
will need an external process to monitor when Jobs are created, add Tasks to
the Jobs and terminate the Jobs ready for the next recurrence. The default is
that the schedule does not recur: one Job is created, within the startWindow
after the doNotRunUntil time, and the schedule is complete as soon as that Job
finishes. The minimum value is 1 minute. If you specify a lower value, the
Batch service rejects the schedule with an error; if you are calling the REST
API directly, the HTTP status code is 400 (Bad Request).
      }, # Optional. All times are fixed respective to UTC and are not impacted by daylight saving
time.
      jobSpecification: {
        priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest
priority and 1000 being the highest priority. The default value is 0. This
priority is used as the default for all Jobs under the Job Schedule. You can
update a Job&apos;s priority after it has been created using by using the update Job
API.
        allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system
will take precedence and will be able requeue tasks from this job. You can
update a job&apos;s allowTaskPreemption after it has been created using the update
job API.
        maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not
specified, the default value is -1, which means there&apos;s no limit to the number
of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after
it has been created using the update job API.
        displayName: string, # Optional. The name need not be unique and can contain any Unicode characters up to a
maximum length of 1024.
        usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is
false.
        onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. Note that if a Job contains no Tasks, then all Tasks are considered complete.
This option is therefore most commonly used with a Job Manager task; if you
want to use automatic Job termination without a Job Manager, you should
initially set onAllTasksComplete to noaction and update the Job properties to
set onAllTasksComplete to terminatejob once you have finished adding Tasks. The
default is noaction.
        onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. The default is noaction.
        networkConfiguration: {
          subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes which will run Tasks from the Job. This
can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos;
service principal must have the &apos;Classic Virtual Machine Contributor&apos;
Role-Based Access Control (RBAC) role for the specified VNet so that Azure
Batch service can schedule Tasks on the Nodes. This can be verified by checking
if the specified VNet has any associated Network Security Groups (NSG). If
communication to the Nodes in the specified subnet is denied by an NSG, then
the Batch service will set the state of the Compute Nodes to unusable. This is
of the form
/subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}.
If the specified VNet has any associated Network Security Groups (NSG), then a
few reserved system ports must be enabled for inbound communication from the
Azure Batch service. For Pools created with a Virtual Machine configuration,
enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for
Windows. Port 443 is also required to be open for outbound connections for
communications to Azure Storage. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        }, # Optional. The network configuration for the Job.
        constraints: {
          maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service
terminates it and any Tasks that are still running. In this case, the
termination reason will be MaxWallClockTimeExpiry. If this property is not
specified, there is no time limit on how long the Job may run.
          maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch
service will try each Task once, and may then retry up to this limit. For
example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one
initial try and 3 retries). If the maximum retry count is 0, the Batch service
does not retry Tasks. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
        }, # Optional. The execution constraints for a Job.
        jobManagerTask: {
          id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters.
          displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum
length of 1024.
          commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
          containerSettings: {
            containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot;
command, in addition to those controlled by the Batch Service.
            imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If
no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a
default.
            registry: {
              username: string, # Optional. The user name to log into the registry server.
              password: string, # Optional. The password to log into the registry server.
              registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
              identityReference: {
                resourceId: string, # Optional. The ARM resource id of the user assigned identity.
              }, # Optional. The reference to a user assigned identity associated with the Batch pool which
a compute node will use.
            }, # Optional. This setting can be omitted if was already provided at Pool creation.
            workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
          }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must
be set as well. If the Pool that will run this Task doesn&apos;t have
containerConfiguration set, this must not be set. When this is specified, all
directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure
Batch directories on the node) are mapped into the container, all Task
environment variables are mapped into the container, and the Task command line
is executed in the container. Files produced in the container outside of
AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that
Batch file APIs will not be able to access those files.
          resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
          outputFiles: [OutputFile], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node
on which the primary Task is executed.
          environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Manager Task.
          constraints: {
            maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
            retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
            maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task
executable due to a nonzero exit code. The Batch service will try the Task
once, and may then retry up to this limit. For example, if the maximum retry
count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries).
If the maximum retry count is 0, the Batch service does not retry the Task
after the first attempt. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
          }, # Optional. Execution constraints to apply to a Task.
          requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the
node has enough free scheduling slots available. For multi-instance Tasks, this
property is not supported and must not be specified.
          killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job
as complete. If any Tasks are still running at this time (other than Job
Release), those Tasks are terminated. If false, the completion of the Job
Manager Task does not affect the Job status. In this case, you should either
use the onAllTasksComplete attribute to terminate the Job, or have a client or
user terminate the Job explicitly. An example of this is if the Job Manager
creates a set of Tasks but then takes no further role in their execution. The
default value is true. If you are using the onAllTasksComplete and
onTaskFailure attributes to control Job lifetime, and using the Job Manager
Task only to create the Tasks for the Job (not to monitor progress), then it is
important to set killJobOnCompletion to false.
          userIdentity: {
            username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
            autoUser: {
              scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task
should be specified if stricter isolation between tasks is required. For
example, if the task mutates the registry in a way which could impact other
tasks, or if certificates have been specified on the pool which should not be
accessible by normal tasks but should be accessible by StartTasks.
              elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
            }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
          }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
          runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job
Manager is running. If false, other Tasks can run simultaneously with the Job
Manager on a Compute Node. The Job Manager Task counts normally against the
Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute
Node allows multiple concurrent Tasks. The default value is true.
          applicationPackageReferences: [ApplicationPackageReference], # Optional. Application Packages are downloaded and deployed to a shared directory, not the
Task working directory. Therefore, if a referenced Application Package is
already on the Compute Node, and is up to date, then it is not re-downloaded;
the existing copy on the Compute Node is used. If a referenced Application
Package cannot be installed, for example because the package has been deleted
or because download failed, the Task fails.
          authenticationTokenSettings: {
            access: [&quot;job&quot;], # Optional. The authentication token grants access to a limited set of Batch service
operations. Currently the only supported value for the access property is
&apos;job&apos;, which grants access to all operations related to the Job which contains
the Task.
          }, # Optional. If this property is set, the Batch service provides the Task with an
authentication token which can be used to authenticate Batch service operations
without requiring an Account access key. The token is provided via the
AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the
Task can carry out using the token depend on the settings. For example, a Task
can request Job permissions in order to add other Tasks to the Job, or check
the status of the Job or of other Tasks under the Job.
          allowLowPriorityNode: boolean, # Optional. The default value is true.
        }, # Optional. If the Job does not specify a Job Manager Task, the user must explicitly add
Tasks to the Job using the Task API. If the Job does specify a Job Manager
Task, the Batch service creates the Job Manager Task when the Job is created,
and will try to schedule the Job Manager Task before scheduling other Tasks in
the Job.
        jobPreparationTask: {
          id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job
Preparation Task. If you try to submit a Task with the same id, the Batch
service rejects the request with error code TaskIdSameAsJobPreparationTask; if
you are calling the REST API directly, the HTTP status code is 409 (Conflict).
          commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
          containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
          resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory. 
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
          environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
          constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
          waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries
the Job Preparation Task up to its maximum retry count (as specified in the
constraints element). If the Task has still not completed successfully after
all retries, then the Batch service will not schedule Tasks of the Job to the
Node. The Node remains active and eligible to run Tasks of other Jobs. If
false, the Batch service will not wait for the Job Preparation Task to
complete. In this case, other Tasks of the Job can start executing on the
Compute Node while the Job Preparation Task is still running; and even if the
Job Preparation Task fails, new Tasks will continue to be scheduled on the
Compute Node. The default value is true.
          userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on
Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux
Compute Nodes.
          rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if
the Job Preparation Task did not complete (e.g. because the reboot occurred
while the Task was running). Therefore, you should always write a Job
Preparation Task to be idempotent and to behave correctly if run multiple
times. The default value is true.
        }, # Optional. If a Job has a Job Preparation Task, the Batch service will run the Job
Preparation Task on a Node before starting any Tasks of that Job on that
Compute Node.
        jobReleaseTask: {
          id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release
Task. If you try to submit a Task with the same id, the Batch service rejects
the request with error code TaskIdSameAsJobReleaseTask; if you are calling the
REST API directly, the HTTP status code is 409 (Conflict).
          commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
          containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
          resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
          environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
          maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute
Node, measured from the time the Task starts. If the Task does not complete
within the time limit, the Batch service terminates it. The default value is 15
minutes. You may not specify a timeout longer than 15 minutes. If you do, the
Batch service rejects it with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
          retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
          userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        }, # Optional. The primary purpose of the Job Release Task is to undo changes to Nodes made by
the Job Preparation Task. Example activities include deleting local files, or
shutting down services that were started as part of Job preparation. A Job
Release Task cannot be specified without also specifying a Job Preparation Task
for the Job. The Batch service runs the Job Release Task on the Compute Nodes
that have run the Job Preparation Task.
        commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by
specifying the same setting name with a different value.
        poolInfo: {
          poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool
does not exist at the time the Batch service tries to schedule a Job, no Tasks
for the Job will run until you create a Pool with that id. Note that the Batch
service will not reject the Job request; it will simply not run Tasks until the
Pool exists. You must specify either the Pool ID or the auto Pool
specification, but not both.
          autoPoolSpecification: {
            autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To
distinguish between Pools created for different purposes, you can specify this
element to add a prefix to the ID that is assigned. The prefix can be up to 20
characters long.
            poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule
are assigned to Pools.
            keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined
by the poolLifetimeOption setting) expires; that is, when the Job or Job
Schedule completes. If true, the Batch service does not delete the Pool
automatically. It is up to the user to delete auto Pools created with this
option.
            pool: {
              displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up
to a maximum length of 1024.
              vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose
a VM size for Compute Nodes in an Azure Batch Pool
(https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
              cloudServiceConfiguration: {
                osFamily: string, # Required. Possible values are:
2 - OS Family 2, equivalent to Windows Server 2008 R2
SP1.
3 - OS Family 3, equivalent to Windows Server 2012.
4 - OS Family 4,
equivalent to Windows Server 2012 R2.
5 - OS Family 5, equivalent to Windows
Server 2016.
6 - OS Family 6, equivalent to Windows Server 2019. For more
information, see Azure Guest OS Releases
(https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
                osVersion: string, # Optional. The default value is * which specifies the latest operating system version for
the specified OS family.
              }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS
VMs. This property and virtualMachineConfiguration are mutually exclusive and
one of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request). This property cannot be specified if the
Batch Account was created with its poolAllocationMode property set to
&apos;UserSubscription&apos;.
              virtualMachineConfiguration: {
                imageReference: {
                  publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
                  offer: string, # Optional. For example, UbuntuServer or WindowsServer.
                  sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
                  version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image.
If omitted, the default is &apos;latest&apos;.
                  virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The
Shared Image Gallery Image must have replicas in the same region and must be in
the same subscription as the Azure Batch account. If the image version is not
specified in the imageId, the latest version will be used. For information
about the firewall settings for the Batch Compute Node agent to communicate
with the Batch service see
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
                  exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create
the node. This read-only field differs from &apos;version&apos; only if the value
specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
                }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image
Gallery Image. To get the list of all Azure Marketplace Image references
verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
                nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the
Pool, and provides the command-and-control interface between the Compute Node
and the Batch service. There are different implementations of the Compute Node
agent, known as SKUs, for different operating systems. You must specify a
Compute Node agent SKU which matches the selected Image reference. To get the
list of supported Compute Node agent SKUs along with their list of verified
Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
                windowsConfiguration: {
                  enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
                }, # Optional. This property must not be specified if the imageReference property specifies a
Linux OS Image.
                dataDisks: [DataDisk], # Optional. This property must be specified if the Compute Nodes in the Pool need to have
empty data disks attached to them. This cannot be updated. Each Compute Node
gets its own disk (the disk is not a file share). Existing disks cannot be
attached, each attached disk is empty. When the Compute Node is removed from
the Pool, the disk and all data associated with it is also deleted. The disk is
not formatted after being attached, it must be formatted before use - for more
information see
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux
and
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
                licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and
should only be used when you hold valid on-premises licenses for the Compute
Nodes which will be deployed. If omitted, no on-premises licensing discount is
applied. Values are:

 Windows_Server - The on-premises license is for Windows
Server.
 Windows_Client - The on-premises license is for Windows Client.

                containerConfiguration: {
                  type: &quot;dockerCompatible&quot;, # Required. The container technology to be used.
                  containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An
Image will be sourced from the default Docker registry unless the Image is
fully qualified with an alternative registry.
                  containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires
credentials, then those credentials must be provided here.
                }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow
Tasks to run in containers. All regular Tasks and Job manager Tasks run on this
Pool must specify the containerSettings property, and all other Tasks may
specify it.
                diskEncryptionConfiguration: {
                  targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On
Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot;
and &quot;TemporaryDisk&quot; must be specified.
                }, # Optional. If specified, encryption is performed on each node in the pool during node
provisioning.
                nodePlacementConfiguration: {
                  policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not
specified, Batch will use the regional policy.
                }, # Optional. This configuration will specify rules on how nodes in the pool will be
physically allocated.
                extensions: [VMExtension], # Optional. If specified, the extensions mentioned in this configuration will be installed
on each node.
                osDisk: {
                  ephemeralOSDiskSettings: {
                    placement: &quot;cachedisk&quot;, # Optional. This property can be used by user in the request to choose the location e.g.,
cache disk space for Ephemeral OS disk provisioning. For more information on
Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size
requirements for Windows VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements
and Linux VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
                  }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the
compute node (VM).
                }, # Optional. Settings for the operating system disk of the compute node (VM).
              }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS
VMs. This property and cloudServiceConfiguration are mutually exclusive and one
of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request).
              taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number
of cores of the vmSize of the pool or 256.
              taskSchedulingPolicy: {
                nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
              }, # Optional. If not specified, the default is spread.
              resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when
enableAutoScale is set to true. The default value is 15 minutes. The minimum
value is 5 minutes. If you specify a value less than 5 minutes, the Batch
service rejects the request with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
              targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
              targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
              enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must
be specified. If true, the autoScaleFormula element is required. The Pool
automatically resizes according to the formula. The default value is false.
              autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is
required if enableAutoScale is set to true. The formula is checked for validity
before the Pool is created. If the formula is not valid, the Batch service
rejects the request with detailed error information.
              autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes
and 168 hours respectively. If you specify a value less than 5 minutes or
greater than 168 hours, the Batch service rejects the request with an invalid
property value error; if you are calling the REST API directly, the HTTP status
code is 400 (Bad Request).
              enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to
deployment restrictions on the Compute Nodes of the Pool. This may result in
the Pool not reaching its desired size. The default value is false.
              networkConfiguration: {
                subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have
enough free IP addresses, the Pool will partially allocate Nodes and a resize
error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the
&apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for
the specified VNet. The specified subnet must allow communication from the
Azure Batch service to be able to schedule Tasks on the Nodes. This can be
verified by checking if the specified VNet has any associated Network Security
Groups (NSG). If communication to the Nodes in the specified subnet is denied
by an NSG, then the Batch service will set the state of the Compute Nodes to
unusable. For Pools created with virtualMachineConfiguration only ARM virtual
networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools
created with cloudServiceConfiguration both ARM and classic virtual networks
are supported. If the specified VNet has any associated Network Security Groups
(NSG), then a few reserved system ports must be enabled for inbound
communication. For Pools created with a virtual machine configuration, enable
ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows.
For Pools created with a cloud service configuration, enable ports 10100,
20100, and 30100. Also enable outbound connections to Azure Storage on port
443. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
                dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
                endpointConfiguration: {
                  inboundNATPools: [InboundNATPool], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum
number of inbound NAT Pools is exceeded the request fails with HTTP status code
400. This cannot be specified if the IPAddressProvisioningType is
NoPublicIPAddresses.
                }, # Optional. Pool endpoint configuration is only supported on Pools with the
virtualMachineConfiguration property.
                publicIPAddressConfiguration: {
                  provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
                  ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100
dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public
IP. For example, a pool needing 250 dedicated VMs would need at least 3 public
IPs specified. Each element of this collection is of the form:
/subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
                }, # Optional. Public IP configuration property is only supported on Pools with the
virtualMachineConfiguration property.
              }, # Optional. The network configuration for a Pool.
              startTask: {
                commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
                containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
                resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
                environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
                userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
                maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this
value specifically controls the number of retries. The Batch service will try
the Task once, and may then retry up to this limit. For example, if the maximum
retry count is 3, Batch tries the Task up to 4 times (one initial try and 3
retries). If the maximum retry count is 0, the Batch service does not retry the
Task. If the maximum retry count is -1, the Batch service retries the Task
without limit, however this is not recommended for a start task or any task.
The default value is 0 (no retries)
                waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the
StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has
still not completed successfully after all retries, then the Batch service
marks the Node unusable, and will not schedule Tasks to it. This condition can
be detected via the Compute Node state and failure info details. If false, the
Batch service will not wait for the StartTask to complete. In this case, other
Tasks can start executing on the Compute Node while the StartTask is still
running; and even if the StartTask fails, new Tasks will continue to be
scheduled on the Compute Node. The default is true.
              }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node.
Examples of recovery operations include (but are not limited to) when an
unhealthy Node is rebooted or a Compute Node disappeared due to host failure.
Retries due to recovery operations are independent of and are not counted
against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal
retry due to a recovery operation may occur. Because of this, all Tasks should
be idempotent. This means Tasks need to tolerate being interrupted and
restarted without causing any corruption or duplicate data. The best practice
for long running Tasks is to use some form of checkpointing. In some cases the
StartTask may be re-run even though the Compute Node was not rebooted. Special
care should be taken to avoid StartTasks which create breakaway process or
install/launch services from the StartTask working directory, as this will
block Batch from being able to re-run the StartTask.
              certificateReferences: [CertificateReference], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified
Certificate store and location. For Linux Compute Nodes, the Certificates are
stored in a directory inside the Task working directory and an environment
variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this
location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory
is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and
Certificates are placed in that directory.
              applicationPackageReferences: [ApplicationPackageReference], # Optional. When creating a pool, the package&apos;s application ID must be fully qualified
(/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationName}).
Changes to Package references affect all new Nodes joining the Pool, but do not
affect Compute Nodes that are already in the Pool until they are rebooted or
reimaged. There is a maximum of 10 Package references on any given Pool.
              applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service
application licenses. If a license is requested which is not supported, Pool
creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;,
&apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application
license added to the Pool.
              userAccounts: [UserAccount], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
              metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
              mountConfiguration: [MountConfiguration], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
              targetNodeCommunicationMode: &quot;default&quot; | &quot;classic&quot; | &quot;simplified&quot;, # Optional. If omitted, the default value is Default.
            }, # Optional. Specification for creating a new Pool.
          }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed
state, and the Pool creation error is set in the Job&apos;s scheduling error
property. The Batch service manages the lifetime (both creation and, unless
keepAlive is specified, deletion) of the auto Pool. Any user actions that
affect the lifetime of the auto Pool while the Job is active will result in
unexpected behavior. You must specify either the Pool ID or the auto Pool
specification, but not both.
        }, # Required. Specifies how a Job should be assigned to a Pool.
        metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
      }, # Optional. Specifies details of the Jobs to be created on a schedule.
      executionInfo: {
        nextRunTime: string (date &amp; time), # Optional. This property is meaningful only if the schedule is in the active state when
the time comes around. For example, if the schedule is disabled, no Job will be
created at nextRunTime unless the Job is enabled before then.
        recentJob: {
          id: string, # Optional. The ID of the Job.
          url: string, # Optional. The URL of the Job.
        }, # Optional. This property is present only if the at least one Job has run under the
schedule.
        endTime: string (date &amp; time), # Optional. This property is set only if the Job Schedule is in the completed state.
      }, # Optional. Contains information about Jobs that have been and will be run under a Job
Schedule.
      metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
      stats: {
        url: string, # Required. The URL of the statistics.
        startTime: string (date &amp; time), # Required. The start time of the time range covered by the statistics.
        lastUpdateTime: string (date &amp; time), # Required. The time at which the statistics were last updated. All statistics are limited
to the range between startTime and lastUpdateTime.
        userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
        kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
        wallClockTime: string (duration ISO 8601 Format), # Required. The wall clock time is the elapsed time from when the Task started running on a
Compute Node to when it finished (or to the last time the statistics were
updated, if the Task had not finished by then). If a Task was retried, this
includes the wall clock time of all the Task retries.
        readIOps: number, # Required. The total number of disk read operations made by all Tasks in all Jobs created
under the schedule.
        writeIOps: number, # Required. The total number of disk write operations made by all Tasks in all Jobs created
under the schedule.
        readIOGiB: number, # Required. The total gibibytes read from disk by all Tasks in all Jobs created under the
schedule.
        writeIOGiB: number, # Required. The total gibibytes written to disk by all Tasks in all Jobs created under the
schedule.
        numSucceededTasks: number, # Required. The total number of Tasks successfully completed during the given time range in
Jobs created under the schedule. A Task completes successfully if it returns
exit code 0.
        numFailedTasks: number, # Required. The total number of Tasks that failed during the given time range in Jobs
created under the schedule. A Task fails if it exhausts its maximum retry count
without returning exit code 0.
        numTaskRetries: number, # Required. The total number of retries during the given time range on all Tasks in all
Jobs created under the schedule.
        waitTime: string (duration ISO 8601 Format), # Required. This value is only reported in the Account lifetime statistics; it is not
included in the Job statistics.
      }, # Optional. Resource usage statistics for a Job Schedule.
    }
  ], # Optional. The list of Job Schedules.
  odata.nextLink: string, # Optional. The URL to get the next set of results.
}
</code>

</remarks>
    </member>
    <member name="GetJobSchedules(Int32,String,Int32,String,Boolean,String,String,String,RequestContext)">
<example>
This sample shows how to call GetJobSchedules and parse the result.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = client.GetJobSchedules();

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.ToString());
]]></code>
This sample shows how to call GetJobSchedules with all parameters, and how to parse the result.
<code><![CDATA[
var credential = new DefaultAzureCredential();
var client = new BatchServiceClient(credential).GetJobScheduleClient(<2022-10-01.16.0>);

Response response = client.GetJobSchedules(1234, "<ocpDate>", 1234, "<clientRequestId>", true, "<filter>", "<select>", "<expand>");

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("value")[0].GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("url").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("eTag").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("lastModified").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("creationTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("state").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stateTransitionTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("previousState").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("previousStateTransitionTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("schedule").GetProperty("doNotRunUntil").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("schedule").GetProperty("doNotRunAfter").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("schedule").GetProperty("startWindow").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("schedule").GetProperty("recurrenceInterval").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("priority").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("allowTaskPreemption").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("maxParallelTasks").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("usesTaskDependencies").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("onAllTasksComplete").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("onTaskFailure").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("filePattern").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("path").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("containerUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("destination").GetProperty("container").GetProperty("uploadHeaders")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("outputFiles")[0].GetProperty("uploadOptions").GetProperty("uploadCondition").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("requiredSlots").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("killJobOnCompletion").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("runExclusive").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("authenticationTokenSettings").GetProperty("access")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobManagerTask").GetProperty("allowLowPriorityNode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("retentionTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("constraints").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("waitForSuccess").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobPreparationTask").GetProperty("rerunOnNodeRebootAfterSuccess").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("maxWallClockTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("retentionTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("jobReleaseTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("commonEnvironmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("commonEnvironmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("poolId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("autoPoolIdPrefix").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("poolLifetimeOption").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("keepAlive").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("displayName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("vmSize").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osFamily").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("cloudServiceConfiguration").GetProperty("osVersion").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("publisher").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("offer").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("sku").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("version").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("virtualMachineImageId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("imageReference").GetProperty("exactVersion").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodeAgentSKUId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("windowsConfiguration").GetProperty("enableAutomaticUpdates").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("lun").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("caching").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("diskSizeGB").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("dataDisks")[0].GetProperty("storageAccountType").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("licenseType").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("type").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerImageNames")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("containerConfiguration").GetProperty("containerRegistries")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("diskEncryptionConfiguration").GetProperty("targets")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("nodePlacementConfiguration").GetProperty("policy").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("publisher").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("type").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("typeHandlerVersion").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("autoUpgradeMinorVersion").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("extensions")[0].GetProperty("provisionAfterExtensions")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("virtualMachineConfiguration").GetProperty("osDisk").GetProperty("ephemeralOSDiskSettings").GetProperty("placement").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSlotsPerNode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("taskSchedulingPolicy").GetProperty("nodeFillType").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("resizeTimeout").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetDedicatedNodes").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetLowPriorityNodes").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableAutoScale").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleFormula").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("autoScaleEvaluationInterval").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("enableInterNodeCommunication").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("subnetId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("dynamicVNetAssignmentScope").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("protocol").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("backendPort").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeStart").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("frontendPortRangeEnd").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("priority").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("access").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourceAddressPrefix").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("endpointConfiguration").GetProperty("inboundNATPools")[0].GetProperty("networkSecurityGroupRules")[0].GetProperty("sourcePortRanges")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("provision").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("networkConfiguration").GetProperty("publicIPAddressConfiguration").GetProperty("ipAddressIds")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("commandLine").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("containerRunOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("imageName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("registryServer").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("registry").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("containerSettings").GetProperty("workingDirectory").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("autoStorageContainerName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("storageContainerUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("httpUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("blobPrefix").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("filePath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("fileMode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("resourceFiles")[0].GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("environmentSettings")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("scope").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("userIdentity").GetProperty("autoUser").GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("maxTaskRetryCount").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("startTask").GetProperty("waitForSuccess").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprint").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("thumbprintAlgorithm").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeLocation").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("storeName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("certificateReferences")[0].GetProperty("visibility")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("applicationId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationPackageReferences")[0].GetProperty("version").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("applicationLicenses")[0].ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("elevationLevel").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("uid").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("gid").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("linuxUserConfiguration").GetProperty("sshPrivateKey").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("userAccounts")[0].GetProperty("windowsUserConfiguration").GetProperty("loginMode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("metadata")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("containerName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("accountKey").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("sasKey").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("blobfuseOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureBlobFileSystemConfiguration").GetProperty("identityReference").GetProperty("resourceId").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("source").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("nfsMountConfiguration").GetProperty("mountOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("username").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("source").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("mountOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("cifsMountConfiguration").GetProperty("password").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountName").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("azureFileUrl").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("accountKey").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("relativeMountPath").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("mountConfiguration")[0].GetProperty("azureFileShareConfiguration").GetProperty("mountOptions").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("poolInfo").GetProperty("autoPoolSpecification").GetProperty("pool").GetProperty("targetNodeCommunicationMode").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("metadata")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("jobSpecification").GetProperty("metadata")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("executionInfo").GetProperty("nextRunTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("executionInfo").GetProperty("recentJob").GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("executionInfo").GetProperty("recentJob").GetProperty("url").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("executionInfo").GetProperty("endTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("metadata")[0].GetProperty("name").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("metadata")[0].GetProperty("value").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("url").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("startTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("lastUpdateTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("userCPUTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("kernelCPUTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("wallClockTime").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("readIOps").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("writeIOps").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("readIOGiB").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("writeIOGiB").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("numSucceededTasks").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("numFailedTasks").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("numTaskRetries").ToString());
Console.WriteLine(result.GetProperty("value")[0].GetProperty("stats").GetProperty("waitTime").ToString());
Console.WriteLine(result.GetProperty("odata.nextLink").ToString());
]]></code>
</example>
<remarks>
Below is the JSON schema for the response payload.

Response Body:

Schema for <c>BatchJobScheduleListResult</c>:
<code>{
  value: [
    {
      id: string, # Optional. A string that uniquely identifies the schedule within the Account.
      displayName: string, # Optional. The display name for the schedule.
      url: string, # Optional. The URL of the Job Schedule.
      eTag: string, # Optional. This is an opaque string. You can use it to detect whether the Job Schedule has
changed between requests. In particular, you can be pass the ETag with an
Update Job Schedule request to specify that your changes should take effect
only if nobody else has modified the schedule in the meantime.
      lastModified: string (date &amp; time), # Optional. This is the last time at which the schedule level data, such as the Job
specification or recurrence information, changed. It does not factor in
job-level changes such as new Jobs being created or Jobs changing state.
      creationTime: string (date &amp; time), # Optional. The creation time of the Job Schedule.
      state: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. The state of the Job Schedule.
      stateTransitionTime: string (date &amp; time), # Optional. The time at which the Job Schedule entered the current state.
      previousState: &quot;active&quot; | &quot;completed&quot; | &quot;disabled&quot; | &quot;terminating&quot; | &quot;deleting&quot;, # Optional. This property is not present if the Job Schedule is in its initial active state.
      previousStateTransitionTime: string (date &amp; time), # Optional. This property is not present if the Job Schedule is in its initial active state.
      schedule: {
        doNotRunUntil: string (date &amp; time), # Optional. If you do not specify a doNotRunUntil time, the schedule becomes ready to
create Jobs immediately.
        doNotRunAfter: string (date &amp; time), # Optional. If you do not specify a doNotRunAfter time, and you are creating a recurring
Job Schedule, the Job Schedule will remain active until you explicitly
terminate it.
        startWindow: string (duration ISO 8601 Format), # Optional. If a Job is not created within the startWindow interval, then the &apos;opportunity&apos;
is lost; no Job will be created until the next recurrence of the schedule. If
the schedule is recurring, and the startWindow is longer than the recurrence
interval, then this is equivalent to an infinite startWindow, because the Job
that is &apos;due&apos; in one recurrenceInterval is not carried forward into the next
recurrence interval. The default is infinite. The minimum value is 1 minute. If
you specify a lower value, the Batch service rejects the schedule with an
error; if you are calling the REST API directly, the HTTP status code is 400
(Bad Request).
        recurrenceInterval: string (duration ISO 8601 Format), # Optional. Because a Job Schedule can have at most one active Job under it at any given
time, if it is time to create a new Job under a Job Schedule, but the previous
Job is still running, the Batch service will not create the new Job until the
previous Job finishes. If the previous Job does not finish within the
startWindow period of the new recurrenceInterval, then no new Job will be
scheduled for that interval. For recurring Jobs, you should normally specify a
jobManagerTask in the jobSpecification. If you do not use jobManagerTask, you
will need an external process to monitor when Jobs are created, add Tasks to
the Jobs and terminate the Jobs ready for the next recurrence. The default is
that the schedule does not recur: one Job is created, within the startWindow
after the doNotRunUntil time, and the schedule is complete as soon as that Job
finishes. The minimum value is 1 minute. If you specify a lower value, the
Batch service rejects the schedule with an error; if you are calling the REST
API directly, the HTTP status code is 400 (Bad Request).
      }, # Optional. All times are fixed respective to UTC and are not impacted by daylight saving
time.
      jobSpecification: {
        priority: number, # Optional. Priority values can range from -1000 to 1000, with -1000 being the lowest
priority and 1000 being the highest priority. The default value is 0. This
priority is used as the default for all Jobs under the Job Schedule. You can
update a Job&apos;s priority after it has been created using by using the update Job
API.
        allowTaskPreemption: boolean, # Optional. If the value is set to True, other high priority jobs submitted to the system
will take precedence and will be able requeue tasks from this job. You can
update a job&apos;s allowTaskPreemption after it has been created using the update
job API.
        maxParallelTasks: number, # Optional. The value of maxParallelTasks must be -1 or greater than 0 if specified. If not
specified, the default value is -1, which means there&apos;s no limit to the number
of tasks that can be run at once. You can update a job&apos;s maxParallelTasks after
it has been created using the update job API.
        displayName: string, # Optional. The name need not be unique and can contain any Unicode characters up to a
maximum length of 1024.
        usesTaskDependencies: boolean, # Optional. Whether Tasks in the Job can define dependencies on each other. The default is
false.
        onAllTasksComplete: &quot;noaction&quot; | &quot;terminatejob&quot;, # Optional. Note that if a Job contains no Tasks, then all Tasks are considered complete.
This option is therefore most commonly used with a Job Manager task; if you
want to use automatic Job termination without a Job Manager, you should
initially set onAllTasksComplete to noaction and update the Job properties to
set onAllTasksComplete to terminatejob once you have finished adding Tasks. The
default is noaction.
        onTaskFailure: &quot;noaction&quot; | &quot;performexitoptionsjobaction&quot;, # Optional. The default is noaction.
        networkConfiguration: {
          subnetId: string, # Required. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes which will run Tasks from the Job. This
can be up to the number of Compute Nodes in the Pool. The &apos;MicrosoftAzureBatch&apos;
service principal must have the &apos;Classic Virtual Machine Contributor&apos;
Role-Based Access Control (RBAC) role for the specified VNet so that Azure
Batch service can schedule Tasks on the Nodes. This can be verified by checking
if the specified VNet has any associated Network Security Groups (NSG). If
communication to the Nodes in the specified subnet is denied by an NSG, then
the Batch service will set the state of the Compute Nodes to unusable. This is
of the form
/subscriptions/{subscription}/resourceGroups/{group}/providers/{provider}/virtualNetworks/{network}/subnets/{subnet}.
If the specified VNet has any associated Network Security Groups (NSG), then a
few reserved system ports must be enabled for inbound communication from the
Azure Batch service. For Pools created with a Virtual Machine configuration,
enable ports 29876 and 29877, as well as port 22 for Linux and port 3389 for
Windows. Port 443 is also required to be open for outbound connections for
communications to Azure Storage. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
        }, # Optional. The network configuration for the Job.
        constraints: {
          maxWallClockTime: string (duration ISO 8601 Format), # Optional. If the Job does not complete within the time limit, the Batch service
terminates it and any Tasks that are still running. In this case, the
termination reason will be MaxWallClockTimeExpiry. If this property is not
specified, there is no time limit on how long the Job may run.
          maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries. The Batch
service will try each Task once, and may then retry up to this limit. For
example, if the maximum retry count is 3, Batch tries a Task up to 4 times (one
initial try and 3 retries). If the maximum retry count is 0, the Batch service
does not retry Tasks. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
        }, # Optional. The execution constraints for a Job.
        jobManagerTask: {
          id: string, # Required. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters.
          displayName: string, # Optional. It need not be unique and can contain any Unicode characters up to a maximum
length of 1024.
          commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
          containerSettings: {
            containerRunOptions: string, # Optional. These additional options are supplied as arguments to the &quot;docker create&quot;
command, in addition to those controlled by the Batch Service.
            imageName: string, # Required. This is the full Image reference, as would be specified to &quot;docker pull&quot;. If
no tag is provided as part of the Image name, the tag &quot;:latest&quot; is used as a
default.
            registry: {
              username: string, # Optional. The user name to log into the registry server.
              password: string, # Optional. The password to log into the registry server.
              registryServer: string, # Optional. If omitted, the default is &quot;docker.io&quot;.
              identityReference: {
                resourceId: string, # Optional. The ARM resource id of the user assigned identity.
              }, # Optional. The reference to a user assigned identity associated with the Batch pool which
a compute node will use.
            }, # Optional. This setting can be omitted if was already provided at Pool creation.
            workingDirectory: &quot;taskWorkingDirectory&quot; | &quot;containerImageDefault&quot;, # Optional. The default is &apos;taskWorkingDirectory&apos;.
          }, # Optional. If the Pool that will run this Task has containerConfiguration set, this must
be set as well. If the Pool that will run this Task doesn&apos;t have
containerConfiguration set, this must not be set. When this is specified, all
directories recursively below the AZ_BATCH_NODE_ROOT_DIR (the root of Azure
Batch directories on the node) are mapped into the container, all Task
environment variables are mapped into the container, and the Task command line
is executed in the container. Files produced in the container outside of
AZ_BATCH_NODE_ROOT_DIR might not be reflected to the host disk, meaning that
Batch file APIs will not be able to access those files.
          resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
          outputFiles: [OutputFile], # Optional. For multi-instance Tasks, the files will only be uploaded from the Compute Node
on which the primary Task is executed.
          environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Manager Task.
          constraints: {
            maxWallClockTime: string (duration ISO 8601 Format), # Optional. If this is not specified, there is no time limit on how long the Task may run.
            retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
            maxTaskRetryCount: number, # Optional. Note that this value specifically controls the number of retries for the Task
executable due to a nonzero exit code. The Batch service will try the Task
once, and may then retry up to this limit. For example, if the maximum retry
count is 3, Batch tries the Task up to 4 times (one initial try and 3 retries).
If the maximum retry count is 0, the Batch service does not retry the Task
after the first attempt. If the maximum retry count is -1, the Batch service
retries the Task without limit, however this is not recommended for a start
task or any task. The default value is 0 (no retries)
          }, # Optional. Execution constraints to apply to a Task.
          requiredSlots: number, # Optional. The default is 1. A Task can only be scheduled to run on a compute node if the
node has enough free scheduling slots available. For multi-instance Tasks, this
property is not supported and must not be specified.
          killJobOnCompletion: boolean, # Optional. If true, when the Job Manager Task completes, the Batch service marks the Job
as complete. If any Tasks are still running at this time (other than Job
Release), those Tasks are terminated. If false, the completion of the Job
Manager Task does not affect the Job status. In this case, you should either
use the onAllTasksComplete attribute to terminate the Job, or have a client or
user terminate the Job explicitly. An example of this is if the Job Manager
creates a set of Tasks but then takes no further role in their execution. The
default value is true. If you are using the onAllTasksComplete and
onTaskFailure attributes to control Job lifetime, and using the Job Manager
Task only to create the Tasks for the Job (not to monitor progress), then it is
important to set killJobOnCompletion to false.
          userIdentity: {
            username: string, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
            autoUser: {
              scope: &quot;task&quot; | &quot;pool&quot;, # Optional. The default value is pool. If the pool is running Windows a value of Task
should be specified if stricter isolation between tasks is required. For
example, if the task mutates the registry in a way which could impact other
tasks, or if certificates have been specified on the pool which should not be
accessible by normal tasks but should be accessible by StartTasks.
              elevationLevel: &quot;nonadmin&quot; | &quot;admin&quot;, # Optional. The default value is nonAdmin.
            }, # Optional. The userName and autoUser properties are mutually exclusive; you must specify
one but not both.
          }, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
          runExclusive: boolean, # Optional. If true, no other Tasks will run on the same Node for as long as the Job
Manager is running. If false, other Tasks can run simultaneously with the Job
Manager on a Compute Node. The Job Manager Task counts normally against the
Compute Node&apos;s concurrent Task limit, so this is only relevant if the Compute
Node allows multiple concurrent Tasks. The default value is true.
          applicationPackageReferences: [ApplicationPackageReference], # Optional. Application Packages are downloaded and deployed to a shared directory, not the
Task working directory. Therefore, if a referenced Application Package is
already on the Compute Node, and is up to date, then it is not re-downloaded;
the existing copy on the Compute Node is used. If a referenced Application
Package cannot be installed, for example because the package has been deleted
or because download failed, the Task fails.
          authenticationTokenSettings: {
            access: [&quot;job&quot;], # Optional. The authentication token grants access to a limited set of Batch service
operations. Currently the only supported value for the access property is
&apos;job&apos;, which grants access to all operations related to the Job which contains
the Task.
          }, # Optional. If this property is set, the Batch service provides the Task with an
authentication token which can be used to authenticate Batch service operations
without requiring an Account access key. The token is provided via the
AZ_BATCH_AUTHENTICATION_TOKEN environment variable. The operations that the
Task can carry out using the token depend on the settings. For example, a Task
can request Job permissions in order to add other Tasks to the Job, or check
the status of the Job or of other Tasks under the Job.
          allowLowPriorityNode: boolean, # Optional. The default value is true.
        }, # Optional. If the Job does not specify a Job Manager Task, the user must explicitly add
Tasks to the Job using the Task API. If the Job does specify a Job Manager
Task, the Batch service creates the Job Manager Task when the Job is created,
and will try to schedule the Job Manager Task before scheduling other Tasks in
the Job.
        jobPreparationTask: {
          id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobpreparation&apos;. No other Task in the Job can have the same ID as the Job
Preparation Task. If you try to submit a Task with the same id, the Batch
service rejects the request with error code TaskIdSameAsJobPreparationTask; if
you are calling the REST API directly, the HTTP status code is 409 (Conflict).
          commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
          containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
          resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory. 
There is a maximum size for the list of resource files.  When the max size is
exceeded, the request will fail and the response error code will be
RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be
reduced in size. This can be achieved using .zip files, Application Packages,
or Docker Containers.
          environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Preparation Task.
          constraints: TaskConstraints, # Optional. Execution constraints to apply to a Task.
          waitForSuccess: boolean, # Optional. If true and the Job Preparation Task fails on a Node, the Batch service retries
the Job Preparation Task up to its maximum retry count (as specified in the
constraints element). If the Task has still not completed successfully after
all retries, then the Batch service will not schedule Tasks of the Job to the
Node. The Node remains active and eligible to run Tasks of other Jobs. If
false, the Batch service will not wait for the Job Preparation Task to
complete. In this case, other Tasks of the Job can start executing on the
Compute Node while the Job Preparation Task is still running; and even if the
Job Preparation Task fails, new Tasks will continue to be scheduled on the
Compute Node. The default value is true.
          userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task on
Windows Compute Nodes, or a non-administrative user unique to the Pool on Linux
Compute Nodes.
          rerunOnNodeRebootAfterSuccess: boolean, # Optional. The Job Preparation Task is always rerun if a Compute Node is reimaged, or if
the Job Preparation Task did not complete (e.g. because the reboot occurred
while the Task was running). Therefore, you should always write a Job
Preparation Task to be idempotent and to behave correctly if run multiple
times. The default value is true.
        }, # Optional. If a Job has a Job Preparation Task, the Batch service will run the Job
Preparation Task on a Node before starting any Tasks of that Job on that
Compute Node.
        jobReleaseTask: {
          id: string, # Optional. The ID can contain any combination of alphanumeric characters including hyphens
and underscores and cannot contain more than 64 characters. If you do not
specify this property, the Batch service assigns a default value of
&apos;jobrelease&apos;. No other Task in the Job can have the same ID as the Job Release
Task. If you try to submit a Task with the same id, the Batch service rejects
the request with error code TaskIdSameAsJobReleaseTask; if you are calling the
REST API directly, the HTTP status code is 409 (Conflict).
          commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
          containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
          resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
          environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the Job Release Task.
          maxWallClockTime: string (duration ISO 8601 Format), # Optional. The maximum elapsed time that the Job Release Task may run on a given Compute
Node, measured from the time the Task starts. If the Task does not complete
within the time limit, the Batch service terminates it. The default value is 15
minutes. You may not specify a timeout longer than 15 minutes. If you do, the
Batch service rejects it with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
          retentionTime: string (duration ISO 8601 Format), # Optional. The default is 7 days, i.e. the Task directory will be retained for 7 days
unless the Compute Node is removed or the Job is deleted.
          userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
        }, # Optional. The primary purpose of the Job Release Task is to undo changes to Nodes made by
the Job Preparation Task. Example activities include deleting local files, or
shutting down services that were started as part of Job preparation. A Job
Release Task cannot be specified without also specifying a Job Preparation Task
for the Job. The Batch service runs the Job Release Task on the Compute Nodes
that have run the Job Preparation Task.
        commonEnvironmentSettings: [EnvironmentSetting], # Optional. Individual Tasks can override an environment setting specified here by
specifying the same setting name with a different value.
        poolInfo: {
          poolId: string, # Optional. You must ensure that the Pool referenced by this property exists. If the Pool
does not exist at the time the Batch service tries to schedule a Job, no Tasks
for the Job will run until you create a Pool with that id. Note that the Batch
service will not reject the Job request; it will simply not run Tasks until the
Pool exists. You must specify either the Pool ID or the auto Pool
specification, but not both.
          autoPoolSpecification: {
            autoPoolIdPrefix: string, # Optional. The Batch service assigns each auto Pool a unique identifier on creation. To
distinguish between Pools created for different purposes, you can specify this
element to add a prefix to the ID that is assigned. The prefix can be up to 20
characters long.
            poolLifetimeOption: &quot;jobschedule&quot; | &quot;job&quot;, # Required. The minimum lifetime of created auto Pools, and how multiple Jobs on a schedule
are assigned to Pools.
            keepAlive: boolean, # Optional. If false, the Batch service deletes the Pool once its lifetime (as determined
by the poolLifetimeOption setting) expires; that is, when the Job or Job
Schedule completes. If true, the Batch service does not delete the Pool
automatically. It is up to the user to delete auto Pools created with this
option.
            pool: {
              displayName: string, # Optional. The display name need not be unique and can contain any Unicode characters up
to a maximum length of 1024.
              vmSize: string, # Required. For information about available sizes of virtual machines in Pools, see Choose
a VM size for Compute Nodes in an Azure Batch Pool
(https://docs.microsoft.com/azure/batch/batch-pool-vm-sizes).
              cloudServiceConfiguration: {
                osFamily: string, # Required. Possible values are:
2 - OS Family 2, equivalent to Windows Server 2008 R2
SP1.
3 - OS Family 3, equivalent to Windows Server 2012.
4 - OS Family 4,
equivalent to Windows Server 2012 R2.
5 - OS Family 5, equivalent to Windows
Server 2016.
6 - OS Family 6, equivalent to Windows Server 2019. For more
information, see Azure Guest OS Releases
(https://azure.microsoft.com/documentation/articles/cloud-services-guestos-update-matrix/#releases).
                osVersion: string, # Optional. The default value is * which specifies the latest operating system version for
the specified OS family.
              }, # Optional. This property must be specified if the Pool needs to be created with Azure PaaS
VMs. This property and virtualMachineConfiguration are mutually exclusive and
one of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request). This property cannot be specified if the
Batch Account was created with its poolAllocationMode property set to
&apos;UserSubscription&apos;.
              virtualMachineConfiguration: {
                imageReference: {
                  publisher: string, # Optional. For example, Canonical or MicrosoftWindowsServer.
                  offer: string, # Optional. For example, UbuntuServer or WindowsServer.
                  sku: string, # Optional. For example, 18.04-LTS or 2019-Datacenter.
                  version: string, # Optional. A value of &apos;latest&apos; can be specified to select the latest version of an Image.
If omitted, the default is &apos;latest&apos;.
                  virtualMachineImageId: string, # Optional. This property is mutually exclusive with other ImageReference properties. The
Shared Image Gallery Image must have replicas in the same region and must be in
the same subscription as the Azure Batch account. If the image version is not
specified in the imageId, the latest version will be used. For information
about the firewall settings for the Batch Compute Node agent to communicate
with the Batch service see
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
                  exactVersion: string, # Optional. The specific version of the platform image or marketplace image used to create
the node. This read-only field differs from &apos;version&apos; only if the value
specified for &apos;version&apos; when the pool was created was &apos;latest&apos;.
                }, # Required. A reference to an Azure Virtual Machines Marketplace Image or a Shared Image
Gallery Image. To get the list of all Azure Marketplace Image references
verified by Azure Batch, see the &apos;List Supported Images&apos; operation.
                nodeAgentSKUId: string, # Required. The Batch Compute Node agent is a program that runs on each Compute Node in the
Pool, and provides the command-and-control interface between the Compute Node
and the Batch service. There are different implementations of the Compute Node
agent, known as SKUs, for different operating systems. You must specify a
Compute Node agent SKU which matches the selected Image reference. To get the
list of supported Compute Node agent SKUs along with their list of verified
Image references, see the &apos;List supported Compute Node agent SKUs&apos; operation.
                windowsConfiguration: {
                  enableAutomaticUpdates: boolean, # Optional. If omitted, the default value is true.
                }, # Optional. This property must not be specified if the imageReference property specifies a
Linux OS Image.
                dataDisks: [DataDisk], # Optional. This property must be specified if the Compute Nodes in the Pool need to have
empty data disks attached to them. This cannot be updated. Each Compute Node
gets its own disk (the disk is not a file share). Existing disks cannot be
attached, each attached disk is empty. When the Compute Node is removed from
the Pool, the disk and all data associated with it is also deleted. The disk is
not formatted after being attached, it must be formatted before use - for more
information see
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/classic/attach-disk#initialize-a-new-data-disk-in-linux
and
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/attach-disk-ps#add-an-empty-data-disk-to-a-virtual-machine.
                licenseType: string, # Optional. This only applies to Images that contain the Windows operating system, and
should only be used when you hold valid on-premises licenses for the Compute
Nodes which will be deployed. If omitted, no on-premises licensing discount is
applied. Values are:

 Windows_Server - The on-premises license is for Windows
Server.
 Windows_Client - The on-premises license is for Windows Client.

                containerConfiguration: {
                  type: &quot;dockerCompatible&quot;, # Required. The container technology to be used.
                  containerImageNames: [string], # Optional. This is the full Image reference, as would be specified to &quot;docker pull&quot;. An
Image will be sourced from the default Docker registry unless the Image is
fully qualified with an alternative registry.
                  containerRegistries: [ContainerRegistry], # Optional. If any Images must be downloaded from a private registry which requires
credentials, then those credentials must be provided here.
                }, # Optional. If specified, setup is performed on each Compute Node in the Pool to allow
Tasks to run in containers. All regular Tasks and Job manager Tasks run on this
Pool must specify the containerSettings property, and all other Tasks may
specify it.
                diskEncryptionConfiguration: {
                  targets: [&quot;osdisk&quot; | &quot;temporarydisk&quot;], # Optional. If omitted, no disks on the compute nodes in the pool will be encrypted. On
Linux pool, only &quot;TemporaryDisk&quot; is supported; on Windows pool, &quot;OsDisk&quot;
and &quot;TemporaryDisk&quot; must be specified.
                }, # Optional. If specified, encryption is performed on each node in the pool during node
provisioning.
                nodePlacementConfiguration: {
                  policy: &quot;regional&quot; | &quot;zonal&quot;, # Optional. Allocation policy used by Batch Service to provision the nodes. If not
specified, Batch will use the regional policy.
                }, # Optional. This configuration will specify rules on how nodes in the pool will be
physically allocated.
                extensions: [VMExtension], # Optional. If specified, the extensions mentioned in this configuration will be installed
on each node.
                osDisk: {
                  ephemeralOSDiskSettings: {
                    placement: &quot;cachedisk&quot;, # Optional. This property can be used by user in the request to choose the location e.g.,
cache disk space for Ephemeral OS disk provisioning. For more information on
Ephemeral OS disk size requirements, please refer to Ephemeral OS disk size
requirements for Windows VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/windows/ephemeral-os-disks#size-requirements
and Linux VMs at
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/ephemeral-os-disks#size-requirements.
                  }, # Optional. Specifies the ephemeral Disk Settings for the operating system disk used by the
compute node (VM).
                }, # Optional. Settings for the operating system disk of the compute node (VM).
              }, # Optional. This property must be specified if the Pool needs to be created with Azure IaaS
VMs. This property and cloudServiceConfiguration are mutually exclusive and one
of the properties must be specified. If neither is specified then the Batch
service returns an error; if you are calling the REST API directly, the HTTP
status code is 400 (Bad Request).
              taskSlotsPerNode: number, # Optional. The default value is 1. The maximum value is the smaller of 4 times the number
of cores of the vmSize of the pool or 256.
              taskSchedulingPolicy: {
                nodeFillType: &quot;spread&quot; | &quot;pack&quot;, # Required. If not specified, the default is spread.
              }, # Optional. If not specified, the default is spread.
              resizeTimeout: string (duration ISO 8601 Format), # Optional. This timeout applies only to manual scaling; it has no effect when
enableAutoScale is set to true. The default value is 15 minutes. The minimum
value is 5 minutes. If you specify a value less than 5 minutes, the Batch
service rejects the request with an error; if you are calling the REST API
directly, the HTTP status code is 400 (Bad Request).
              targetDedicatedNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
              targetLowPriorityNodes: number, # Optional. This property must not be specified if enableAutoScale is set to true. If
enableAutoScale is set to false, then you must set either targetDedicatedNodes,
targetLowPriorityNodes, or both.
              enableAutoScale: boolean, # Optional. If false, at least one of targetDedicatedNodes and targetLowPriorityNodes must
be specified. If true, the autoScaleFormula element is required. The Pool
automatically resizes according to the formula. The default value is false.
              autoScaleFormula: string, # Optional. This property must not be specified if enableAutoScale is set to false. It is
required if enableAutoScale is set to true. The formula is checked for validity
before the Pool is created. If the formula is not valid, the Batch service
rejects the request with detailed error information.
              autoScaleEvaluationInterval: string (duration ISO 8601 Format), # Optional. The default value is 15 minutes. The minimum and maximum value are 5 minutes
and 168 hours respectively. If you specify a value less than 5 minutes or
greater than 168 hours, the Batch service rejects the request with an invalid
property value error; if you are calling the REST API directly, the HTTP status
code is 400 (Bad Request).
              enableInterNodeCommunication: boolean, # Optional. Enabling inter-node communication limits the maximum size of the Pool due to
deployment restrictions on the Compute Nodes of the Pool. This may result in
the Pool not reaching its desired size. The default value is false.
              networkConfiguration: {
                subnetId: string, # Optional. The virtual network must be in the same region and subscription as the Azure
Batch Account. The specified subnet should have enough free IP addresses to
accommodate the number of Compute Nodes in the Pool. If the subnet doesn&apos;t have
enough free IP addresses, the Pool will partially allocate Nodes and a resize
error will occur. The &apos;MicrosoftAzureBatch&apos; service principal must have the
&apos;Classic Virtual Machine Contributor&apos; Role-Based Access Control (RBAC) role for
the specified VNet. The specified subnet must allow communication from the
Azure Batch service to be able to schedule Tasks on the Nodes. This can be
verified by checking if the specified VNet has any associated Network Security
Groups (NSG). If communication to the Nodes in the specified subnet is denied
by an NSG, then the Batch service will set the state of the Compute Nodes to
unusable. For Pools created with virtualMachineConfiguration only ARM virtual
networks (&apos;Microsoft.Network/virtualNetworks&apos;) are supported, but for Pools
created with cloudServiceConfiguration both ARM and classic virtual networks
are supported. If the specified VNet has any associated Network Security Groups
(NSG), then a few reserved system ports must be enabled for inbound
communication. For Pools created with a virtual machine configuration, enable
ports 29876 and 29877, as well as port 22 for Linux and port 3389 for Windows.
For Pools created with a cloud service configuration, enable ports 10100,
20100, and 30100. Also enable outbound connections to Azure Storage on port
443. For more details see:
https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration
                dynamicVNetAssignmentScope: &quot;none&quot; | &quot;job&quot;, # Optional. The scope of dynamic vnet assignment.
                endpointConfiguration: {
                  inboundNATPools: [InboundNATPool], # Required. The maximum number of inbound NAT Pools per Batch Pool is 5. If the maximum
number of inbound NAT Pools is exceeded the request fails with HTTP status code
400. This cannot be specified if the IPAddressProvisioningType is
NoPublicIPAddresses.
                }, # Optional. Pool endpoint configuration is only supported on Pools with the
virtualMachineConfiguration property.
                publicIPAddressConfiguration: {
                  provision: &quot;batchmanaged&quot; | &quot;usermanaged&quot; | &quot;nopublicipaddresses&quot;, # Optional. The default value is BatchManaged.
                  ipAddressIds: [string], # Optional. The number of IPs specified here limits the maximum size of the Pool - 100
dedicated nodes or 100 Spot/Low-priority nodes can be allocated for each public
IP. For example, a pool needing 250 dedicated VMs would need at least 3 public
IPs specified. Each element of this collection is of the form:
/subscriptions/{subscription}/resourceGroups/{group}/providers/Microsoft.Network/publicIPAddresses/{ip}.
                }, # Optional. Public IP configuration property is only supported on Pools with the
virtualMachineConfiguration property.
              }, # Optional. The network configuration for a Pool.
              startTask: {
                commandLine: string, # Required. The command line does not run under a shell, and therefore cannot take
advantage of shell features such as environment variable expansion. If you want
to take advantage of such features, you should invoke the shell in the command
line, for example using &quot;cmd /c MyCommand&quot; in Windows or &quot;/bin/sh -c
MyCommand&quot; in Linux. If the command line refers to file paths, it should use a
relative path (relative to the Task working directory), or use the Batch
provided environment variable
(https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).
                containerSettings: TaskContainerSettings, # Optional. When this is specified, all directories recursively below the
AZ_BATCH_NODE_ROOT_DIR (the root of Azure Batch directories on the node) are
mapped into the container, all Task environment variables are mapped into the
container, and the Task command line is executed in the container. Files
produced in the container outside of AZ_BATCH_NODE_ROOT_DIR might not be
reflected to the host disk, meaning that Batch file APIs will not be able to
access those files.
                resourceFiles: [ResourceFile], # Optional. Files listed under this element are located in the Task&apos;s working directory.
                environmentSettings: [EnvironmentSetting], # Optional. A list of environment variable settings for the StartTask.
                userIdentity: UserIdentity, # Optional. If omitted, the Task runs as a non-administrative user unique to the Task.
                maxTaskRetryCount: number, # Optional. The Batch service retries a Task if its exit code is nonzero. Note that this
value specifically controls the number of retries. The Batch service will try
the Task once, and may then retry up to this limit. For example, if the maximum
retry count is 3, Batch tries the Task up to 4 times (one initial try and 3
retries). If the maximum retry count is 0, the Batch service does not retry the
Task. If the maximum retry count is -1, the Batch service retries the Task
without limit, however this is not recommended for a start task or any task.
The default value is 0 (no retries)
                waitForSuccess: boolean, # Optional. If true and the StartTask fails on a Node, the Batch service retries the
StartTask up to its maximum retry count (maxTaskRetryCount). If the Task has
still not completed successfully after all retries, then the Batch service
marks the Node unusable, and will not schedule Tasks to it. This condition can
be detected via the Compute Node state and failure info details. If false, the
Batch service will not wait for the StartTask to complete. In this case, other
Tasks can start executing on the Compute Node while the StartTask is still
running; and even if the StartTask fails, new Tasks will continue to be
scheduled on the Compute Node. The default is true.
              }, # Optional. Batch will retry Tasks when a recovery operation is triggered on a Node.
Examples of recovery operations include (but are not limited to) when an
unhealthy Node is rebooted or a Compute Node disappeared due to host failure.
Retries due to recovery operations are independent of and are not counted
against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal
retry due to a recovery operation may occur. Because of this, all Tasks should
be idempotent. This means Tasks need to tolerate being interrupted and
restarted without causing any corruption or duplicate data. The best practice
for long running Tasks is to use some form of checkpointing. In some cases the
StartTask may be re-run even though the Compute Node was not rebooted. Special
care should be taken to avoid StartTasks which create breakaway process or
install/launch services from the StartTask working directory, as this will
block Batch from being able to re-run the StartTask.
              certificateReferences: [CertificateReference], # Optional. For Windows Nodes, the Batch service installs the Certificates to the specified
Certificate store and location. For Linux Compute Nodes, the Certificates are
stored in a directory inside the Task working directory and an environment
variable AZ_BATCH_CERTIFICATES_DIR is supplied to the Task to query for this
location. For Certificates with visibility of &apos;remoteUser&apos;, a &apos;certs&apos; directory
is created in the user&apos;s home directory (e.g., /home/{user-name}/certs) and
Certificates are placed in that directory.
              applicationPackageReferences: [ApplicationPackageReference], # Optional. When creating a pool, the package&apos;s application ID must be fully qualified
(/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationName}).
Changes to Package references affect all new Nodes joining the Pool, but do not
affect Compute Nodes that are already in the Pool until they are rebooted or
reimaged. There is a maximum of 10 Package references on any given Pool.
              applicationLicenses: [string], # Optional. The list of application licenses must be a subset of available Batch service
application licenses. If a license is requested which is not supported, Pool
creation will fail. The permitted licenses available on the Pool are &apos;maya&apos;,
&apos;vray&apos;, &apos;3dsmax&apos;, &apos;arnold&apos;. An additional charge applies for each application
license added to the Pool.
              userAccounts: [UserAccount], # Optional. The list of user Accounts to be created on each Compute Node in the Pool.
              metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
              mountConfiguration: [MountConfiguration], # Optional. This supports Azure Files, NFS, CIFS/SMB, and Blobfuse.
              targetNodeCommunicationMode: &quot;default&quot; | &quot;classic&quot; | &quot;simplified&quot;, # Optional. If omitted, the default value is Default.
            }, # Optional. Specification for creating a new Pool.
          }, # Optional. If auto Pool creation fails, the Batch service moves the Job to a completed
state, and the Pool creation error is set in the Job&apos;s scheduling error
property. The Batch service manages the lifetime (both creation and, unless
keepAlive is specified, deletion) of the auto Pool. Any user actions that
affect the lifetime of the auto Pool while the Job is active will result in
unexpected behavior. You must specify either the Pool ID or the auto Pool
specification, but not both.
        }, # Required. Specifies how a Job should be assigned to a Pool.
        metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
      }, # Optional. Specifies details of the Jobs to be created on a schedule.
      executionInfo: {
        nextRunTime: string (date &amp; time), # Optional. This property is meaningful only if the schedule is in the active state when
the time comes around. For example, if the schedule is disabled, no Job will be
created at nextRunTime unless the Job is enabled before then.
        recentJob: {
          id: string, # Optional. The ID of the Job.
          url: string, # Optional. The URL of the Job.
        }, # Optional. This property is present only if the at least one Job has run under the
schedule.
        endTime: string (date &amp; time), # Optional. This property is set only if the Job Schedule is in the completed state.
      }, # Optional. Contains information about Jobs that have been and will be run under a Job
Schedule.
      metadata: [MetadataItem], # Optional. The Batch service does not assign any meaning to metadata; it is solely for the
use of user code.
      stats: {
        url: string, # Required. The URL of the statistics.
        startTime: string (date &amp; time), # Required. The start time of the time range covered by the statistics.
        lastUpdateTime: string (date &amp; time), # Required. The time at which the statistics were last updated. All statistics are limited
to the range between startTime and lastUpdateTime.
        userCPUTime: string (duration ISO 8601 Format), # Required. The total user mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
        kernelCPUTime: string (duration ISO 8601 Format), # Required. The total kernel mode CPU time (summed across all cores and all Compute Nodes)
consumed by all Tasks in all Jobs created under the schedule.
        wallClockTime: string (duration ISO 8601 Format), # Required. The wall clock time is the elapsed time from when the Task started running on a
Compute Node to when it finished (or to the last time the statistics were
updated, if the Task had not finished by then). If a Task was retried, this
includes the wall clock time of all the Task retries.
        readIOps: number, # Required. The total number of disk read operations made by all Tasks in all Jobs created
under the schedule.
        writeIOps: number, # Required. The total number of disk write operations made by all Tasks in all Jobs created
under the schedule.
        readIOGiB: number, # Required. The total gibibytes read from disk by all Tasks in all Jobs created under the
schedule.
        writeIOGiB: number, # Required. The total gibibytes written to disk by all Tasks in all Jobs created under the
schedule.
        numSucceededTasks: number, # Required. The total number of Tasks successfully completed during the given time range in
Jobs created under the schedule. A Task completes successfully if it returns
exit code 0.
        numFailedTasks: number, # Required. The total number of Tasks that failed during the given time range in Jobs
created under the schedule. A Task fails if it exhausts its maximum retry count
without returning exit code 0.
        numTaskRetries: number, # Required. The total number of retries during the given time range on all Tasks in all
Jobs created under the schedule.
        waitTime: string (duration ISO 8601 Format), # Required. This value is only reported in the Account lifetime statistics; it is not
included in the Job statistics.
      }, # Optional. Resource usage statistics for a Job Schedule.
    }
  ], # Optional. The list of Job Schedules.
  odata.nextLink: string, # Optional. The URL to get the next set of results.
}
</code>

</remarks>
    </member>
  </members>
</doc>