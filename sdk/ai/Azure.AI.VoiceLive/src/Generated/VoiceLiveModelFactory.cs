// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

// <auto-generated/>

#nullable disable

using System;
using System.Collections.Generic;
using System.ComponentModel;
using System.Linq;

namespace Azure.AI.VoiceLive
{
    /// <summary> A factory class for creating instances of the models for mocking. </summary>
    public static partial class VoiceLiveModelFactory
    {
        /// <summary> Error object returned in case of API failure. </summary>
        /// <param name="code"> Error code, or null if unspecified. </param>
        /// <param name="message"> Human-readable error message. </param>
        /// <param name="param"> Parameter name related to the error, if applicable. </param>
        /// <param name="type"> Type or category of the error. </param>
        /// <param name="eventId"> Event id of the error. </param>
        /// <returns> A new <see cref="VoiceLive.VoiceLiveErrorDetails"/> instance for mocking. </returns>
        public static VoiceLiveErrorDetails VoiceLiveErrorDetails(string code = default, string message = default, string @param = default, string @type = default, string eventId = default)
        {
            return new VoiceLiveErrorDetails(
                code,
                message,
                @param,
                @type,
                eventId,
                additionalBinaryDataProperties: null);
        }

        /// <summary> A single log probability entry for a token. </summary>
        /// <param name="token"> The token that was used to generate the log probability. </param>
        /// <param name="logprob"> The log probability of the token. </param>
        /// <param name="bytes"> The bytes that were used to generate the log probability. </param>
        /// <returns> A new <see cref="VoiceLive.LogProbProperties"/> instance for mocking. </returns>
        public static LogProbProperties LogProbProperties(string token = default, float logprob = default, BinaryData bytes = default)
        {
            return new LogProbProperties(token, logprob, bytes, additionalBinaryDataProperties: null);
        }

        /// <summary> Base for session configuration shared between request and response. </summary>
        /// <param name="model"> The model for the session. </param>
        /// <param name="modalities"> The modalities to be used in the session. </param>
        /// <param name="animation"> The animation configuration for the session. </param>
        /// <param name="voice"> The voice configuration for the session. </param>
        /// <param name="instructions"> Optional instructions to guide the model's behavior throughout the session. </param>
        /// <param name="inputAudioSamplingRate">
        /// Input audio sampling rate in Hz. Available values:
        /// 
        /// - For pcm16: 8000, 16000, 24000
        /// 
        /// - For g711_alaw/g711_ulaw: 8000
        /// </param>
        /// <param name="inputAudioFormat"> Input audio format. Default is 'pcm16'. </param>
        /// <param name="outputAudioFormat"> Output audio format. Default is 'pcm16'. </param>
        /// <param name="inputAudioNoiseReduction"> Configuration for input audio noise reduction. </param>
        /// <param name="inputAudioEchoCancellation"> Configuration for echo cancellation during server-side audio processing. </param>
        /// <param name="avatar"> Configuration for avatar streaming and behavior during the session. </param>
        /// <param name="inputAudioTranscription"> Configuration for input audio transcription. </param>
        /// <param name="outputAudioTimestampTypes"> Types of timestamps to include in audio response content. </param>
        /// <param name="tools"> Configuration for tools to be used during the session, if applicable. </param>
        /// <param name="toolChoice"> Specifies which tools the model is allowed to call during the session. </param>
        /// <param name="temperature"> Controls the randomness of the model's output. Range: 0.0 to 1.0. Default is 0.7. </param>
        /// <param name="maxResponseOutputTokens"> Maximum number of tokens to generate in the response. Default is unlimited. </param>
        /// <param name="turnDetection"> Type of turn detection to use. </param>
        /// <returns> A new <see cref="VoiceLive.VoiceLiveSessionOptions"/> instance for mocking. </returns>
        public static VoiceLiveSessionOptions VoiceLiveSessionOptions(string model = default, IEnumerable<InteractionModality> modalities = default, AnimationOptions animation = default, VoiceProvider voice = default, string instructions = default, int? inputAudioSamplingRate = default, InputAudioFormat? inputAudioFormat = default, OutputAudioFormat? outputAudioFormat = default, AudioNoiseReduction inputAudioNoiseReduction = default, AudioEchoCancellation inputAudioEchoCancellation = default, AvatarConfiguration avatar = default, AudioInputTranscriptionOptions inputAudioTranscription = default, IEnumerable<AudioTimestampType> outputAudioTimestampTypes = default, IEnumerable<VoiceLiveToolDefinition> tools = default, ToolChoiceOption toolChoice = default, float? temperature = default, MaxResponseOutputTokensOption maxResponseOutputTokens = default, BinaryData turnDetection = default)
        {
            modalities ??= new ChangeTrackingList<InteractionModality>();
            outputAudioTimestampTypes ??= new ChangeTrackingList<AudioTimestampType>();
            tools ??= new ChangeTrackingList<VoiceLiveToolDefinition>();

            return new VoiceLiveSessionOptions(
                model,
                modalities.ToList(),
                animation,
                voice,
                instructions,
                inputAudioSamplingRate,
                inputAudioFormat,
                outputAudioFormat,
                inputAudioNoiseReduction,
                inputAudioEchoCancellation,
                avatar,
                inputAudioTranscription,
                outputAudioTimestampTypes.ToList(),
                tools.ToList(),
                toolChoice,
                temperature,
                maxResponseOutputTokens,
                turnDetection,
                additionalBinaryDataProperties: null);
        }

        /// <summary> Configuration for animation outputs including blendshapes and visemes metadata. </summary>
        /// <param name="modelName"> The name of the animation model to use. </param>
        /// <param name="outputs"> Set of output data types requested from the animation system. </param>
        /// <returns> A new <see cref="VoiceLive.AnimationOptions"/> instance for mocking. </returns>
        public static AnimationOptions AnimationOptions(string modelName = default, IEnumerable<AnimationOutputType> outputs = default)
        {
            outputs ??= new ChangeTrackingList<AnimationOutputType>();

            return new AnimationOptions(modelName, outputs.ToList(), additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// OpenAI voice configuration with explicit type field.
        /// 
        /// This provides a unified interface for OpenAI voices, complementing the
        /// existing string-based OAIVoice for backward compatibility.
        /// </summary>
        /// <param name="name"> The name of the OpenAI voice. </param>
        /// <returns> A new <see cref="VoiceLive.OpenAIVoice"/> instance for mocking. </returns>
        public static OpenAIVoice OpenAIVoice(OAIVoice name = default)
        {
            return new OpenAIVoice("openai", name, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Base for Azure voice configurations.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="VoiceLive.AzureCustomVoice"/>, <see cref="VoiceLive.AzureStandardVoice"/>, and <see cref="VoiceLive.AzurePersonalVoice"/>.
        /// </summary>
        /// <param name="type"> The type of the Azure voice. </param>
        /// <returns> A new <see cref="VoiceLive.AzureVoice"/> instance for mocking. </returns>
        public static AzureVoice AzureVoice(string @type = default)
        {
            return new UnknownAzureVoice(new AzureVoiceType(@type), additionalBinaryDataProperties: null);
        }

        /// <summary> Azure custom voice configuration. </summary>
        /// <param name="name"> Voice name cannot be empty. </param>
        /// <param name="endpointId"> Endpoint ID cannot be empty. </param>
        /// <param name="temperature"> Temperature must be between 0.0 and 1.0. </param>
        /// <param name="customLexiconUri"></param>
        /// <param name="preferLocales"></param>
        /// <param name="locale"></param>
        /// <param name="style"></param>
        /// <param name="pitch"></param>
        /// <param name="rate"></param>
        /// <param name="volume"></param>
        /// <returns> A new <see cref="VoiceLive.AzureCustomVoice"/> instance for mocking. </returns>
        public static AzureCustomVoice AzureCustomVoice(string name = default, string endpointId = default, float? temperature = default, string customLexiconUri = default, IEnumerable<string> preferLocales = default, string locale = default, string style = default, string pitch = default, string rate = default, string volume = default)
        {
            preferLocales ??= new ChangeTrackingList<string>();

            return new AzureCustomVoice(
                AzureVoiceType.AzureCustom,
                additionalBinaryDataProperties: null,
                name,
                endpointId,
                temperature,
                customLexiconUri,
                preferLocales.ToList(),
                locale,
                style,
                pitch,
                rate,
                volume);
        }

        /// <summary> Azure standard voice configuration. </summary>
        /// <param name="name"> Voice name cannot be empty. </param>
        /// <param name="temperature"> Temperature must be between 0.0 and 1.0. </param>
        /// <param name="customLexiconUrl"></param>
        /// <param name="preferLocales"></param>
        /// <param name="locale"></param>
        /// <param name="style"></param>
        /// <param name="pitch"></param>
        /// <param name="rate"></param>
        /// <param name="volume"></param>
        /// <returns> A new <see cref="VoiceLive.AzureStandardVoice"/> instance for mocking. </returns>
        public static AzureStandardVoice AzureStandardVoice(string name = default, float? temperature = default, string customLexiconUrl = default, IEnumerable<string> preferLocales = default, string locale = default, string style = default, string pitch = default, string rate = default, string volume = default)
        {
            preferLocales ??= new ChangeTrackingList<string>();

            return new AzureStandardVoice(
                AzureVoiceType.AzureStandard,
                additionalBinaryDataProperties: null,
                name,
                temperature,
                customLexiconUrl,
                preferLocales.ToList(),
                locale,
                style,
                pitch,
                rate,
                volume);
        }

        /// <summary> Azure personal voice configuration. </summary>
        /// <param name="name"> Voice name cannot be empty. </param>
        /// <param name="temperature"> Temperature must be between 0.0 and 1.0. </param>
        /// <param name="model"> Underlying neural model to use for personal voice. </param>
        /// <returns> A new <see cref="VoiceLive.AzurePersonalVoice"/> instance for mocking. </returns>
        public static AzurePersonalVoice AzurePersonalVoice(string name = default, float? temperature = default, PersonalVoiceModels model = default)
        {
            return new AzurePersonalVoice(AzureVoiceType.AzurePersonal, additionalBinaryDataProperties: null, name, temperature, model);
        }

        /// <summary> Configuration for input audio noise reduction. </summary>
        /// <param name="type"> The type of noise reduction model. </param>
        /// <returns> A new <see cref="VoiceLive.AudioNoiseReduction"/> instance for mocking. </returns>
        public static AudioNoiseReduction AudioNoiseReduction(AudioNoiseReductionType @type = default)
        {
            return new AudioNoiseReduction(@type, additionalBinaryDataProperties: null);
        }

        /// <summary> Echo cancellation configuration for server-side audio processing. </summary>
        /// <returns> A new <see cref="VoiceLive.AudioEchoCancellation"/> instance for mocking. </returns>
        public static AudioEchoCancellation AudioEchoCancellation()
        {
            return new AudioEchoCancellation("server_echo_cancellation", additionalBinaryDataProperties: null);
        }

        /// <summary> Configuration for avatar streaming and behavior during the session. </summary>
        /// <param name="iceServers"> Optional list of ICE servers to use for WebRTC connection establishment. </param>
        /// <param name="character"> The character name or ID used for the avatar. </param>
        /// <param name="style"> Optional avatar style, such as emotional tone or speaking style. </param>
        /// <param name="customized"> Indicates whether the avatar is customized or not. </param>
        /// <param name="video"> Optional video configuration including resolution, bitrate, and codec. </param>
        /// <returns> A new <see cref="VoiceLive.AvatarConfiguration"/> instance for mocking. </returns>
        public static AvatarConfiguration AvatarConfiguration(IEnumerable<IceServer> iceServers = default, string character = default, string style = default, bool customized = default, VideoParams video = default)
        {
            iceServers ??= new ChangeTrackingList<IceServer>();

            return new AvatarConfiguration(
                iceServers.ToList(),
                character,
                style,
                customized,
                video,
                additionalBinaryDataProperties: null);
        }

        /// <summary> ICE server configuration for WebRTC connection negotiation. </summary>
        /// <param name="uris"> List of ICE server URLs (e.g., TURN or STUN endpoints). </param>
        /// <param name="username"> Optional username used for authentication with the ICE server. </param>
        /// <param name="credential"> Optional credential (e.g., password or token) used for authentication. </param>
        /// <returns> A new <see cref="VoiceLive.IceServer"/> instance for mocking. </returns>
        public static IceServer IceServer(IEnumerable<Uri> uris = default, string username = default, string credential = default)
        {
            uris ??= new ChangeTrackingList<Uri>();

            return new IceServer(uris.ToList(), username, credential, additionalBinaryDataProperties: null);
        }

        /// <summary> Video streaming parameters for avatar. </summary>
        /// <param name="bitrate"> Bitrate in bits per second (e.g., 2000000 for 2 Mbps). </param>
        /// <param name="codec"> Codec to use for encoding. Currently only 'h264' is supported. </param>
        /// <param name="crop"> Optional cropping settings for the video stream. </param>
        /// <param name="resolution"> Optional resolution settings for the video stream. </param>
        /// <param name="background"> Optional background settings for the video. Allows specifying either a solid color or an image URL. </param>
        /// <param name="gopSize"> Group of Pictures (GOP) size for video encoding. Controls the interval between keyframes, affecting compression efficiency and seeking performance. </param>
        /// <returns> A new <see cref="VoiceLive.VideoParams"/> instance for mocking. </returns>
        public static VideoParams VideoParams(int? bitrate = default, string codec = default, VideoCrop crop = default, VideoResolution resolution = default, VideoBackground background = default, int? gopSize = default)
        {
            return new VideoParams(
                bitrate,
                codec,
                crop,
                resolution,
                background,
                gopSize,
                additionalBinaryDataProperties: null);
        }

        /// <summary> Defines a video crop rectangle using top-left and bottom-right coordinates. </summary>
        /// <param name="topLeftInternal"> Top-left corner of the crop region. Array of [x, y], must be non-negative integers. </param>
        /// <param name="bottomRightInternal"> Bottom-right corner of the crop region. Array of [x, y], must be non-negative integers. </param>
        /// <returns> A new <see cref="VoiceLive.VideoCrop"/> instance for mocking. </returns>
        public static VideoCrop VideoCrop(IEnumerable<int> topLeftInternal = default, IEnumerable<int> bottomRightInternal = default)
        {
            topLeftInternal ??= new ChangeTrackingList<int>();
            bottomRightInternal ??= new ChangeTrackingList<int>();

            return new VideoCrop(topLeftInternal.ToList(), bottomRightInternal.ToList(), additionalBinaryDataProperties: null);
        }

        /// <summary> Resolution of the video feed in pixels. </summary>
        /// <param name="width"> Width of the video in pixels. Must be greater than 0. </param>
        /// <param name="height"> Height of the video in pixels. Must be greater than 0. </param>
        /// <returns> A new <see cref="VoiceLive.VideoResolution"/> instance for mocking. </returns>
        public static VideoResolution VideoResolution(int width = default, int height = default)
        {
            return new VideoResolution(width, height, additionalBinaryDataProperties: null);
        }

        /// <summary> Defines a video background, either a solid color or an image URL (mutually exclusive). </summary>
        /// <param name="color"> Background color in hex format (e.g., `#00FF00FF`). Cannot be set if `image_url` is provided. </param>
        /// <param name="imageUrl"> Background image URL. Cannot be set if `color` is provided. </param>
        /// <returns> A new <see cref="VoiceLive.VideoBackground"/> instance for mocking. </returns>
        public static VideoBackground VideoBackground(string color = default, string imageUrl = default)
        {
            return new VideoBackground(color, imageUrl, additionalBinaryDataProperties: null);
        }

        /// <summary> Configuration for input audio transcription. </summary>
        /// <param name="model">
        /// The transcription model to use. Supported values:
        /// 'whisper-1', 'gpt-4o-transcribe', 'gpt-4o-mini-transcribe',
        /// 'azure-speech'.
        /// </param>
        /// <param name="language"> Optional language code in BCP-47 (e.g., 'en-US'), or ISO-639-1 (e.g., 'en'), or multi languages with auto detection, (e.g., 'en,zh'). </param>
        /// <param name="customSpeech"> Optional configuration for custom speech models. </param>
        /// <param name="phraseList"> Optional list of phrase hints to bias recognition. </param>
        /// <returns> A new <see cref="VoiceLive.AudioInputTranscriptionOptions"/> instance for mocking. </returns>
        public static AudioInputTranscriptionOptions AudioInputTranscriptionOptions(AudioInputTranscriptionOptionsModel model = default, string language = default, IDictionary<string, string> customSpeech = default, IEnumerable<string> phraseList = default)
        {
            customSpeech ??= new ChangeTrackingDictionary<string, string>();
            phraseList ??= new ChangeTrackingList<string>();

            return new AudioInputTranscriptionOptions(model, language, customSpeech, phraseList.ToList(), additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// The base representation of a voicelive tool definition.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="VoiceLive.VoiceLiveFunctionDefinition"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <returns> A new <see cref="VoiceLive.VoiceLiveToolDefinition"/> instance for mocking. </returns>
        public static VoiceLiveToolDefinition VoiceLiveToolDefinition(string @type = default)
        {
            return new UnknownVoiceLiveToolDefinition(new ToolType(@type), additionalBinaryDataProperties: null);
        }

        /// <summary> The definition of a function tool as used by the voicelive endpoint. </summary>
        /// <param name="name"></param>
        /// <param name="description"></param>
        /// <param name="parameters"></param>
        /// <returns> A new <see cref="VoiceLive.VoiceLiveFunctionDefinition"/> instance for mocking. </returns>
        public static VoiceLiveFunctionDefinition VoiceLiveFunctionDefinition(string name = default, string description = default, BinaryData parameters = default)
        {
            return new VoiceLiveFunctionDefinition(ToolType.Function, additionalBinaryDataProperties: null, name, description, parameters);
        }

        /// <summary>
        /// Base for any response item; discriminated by `type`.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="VoiceLive.MessageItem"/>, <see cref="VoiceLive.FunctionCallItem"/>, and <see cref="VoiceLive.FunctionCallOutputItem"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <param name="id"></param>
        /// <returns> A new <see cref="VoiceLive.ConversationRequestItem"/> instance for mocking. </returns>
        public static ConversationRequestItem ConversationRequestItem(string @type = default, string id = default)
        {
            return new UnknownConversationRequestItem(new ItemType(@type), id, additionalBinaryDataProperties: null);
        }

        /// <summary> A message item within a conversation. </summary>
        /// <param name="id"></param>
        /// <param name="content"> The content parts of the message. </param>
        /// <param name="status"> Processing status of the message item. </param>
        /// <returns> A new <see cref="VoiceLive.MessageItem"/> instance for mocking. </returns>
        public static MessageItem MessageItem(string id = default, IEnumerable<MessageContentPart> content = default, ItemParamStatus? status = default)
        {
            content ??= new ChangeTrackingList<MessageContentPart>();

            return new MessageItem(
                ItemType.Message,
                id,
                additionalBinaryDataProperties: null,
                default,
                content.ToList(),
                status);
        }

        /// <summary>
        /// Base for any message content part; discriminated by `type`.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="VoiceLive.InputTextContentPart"/>, <see cref="VoiceLive.InputAudioContentPart"/>, and <see cref="VoiceLive.OutputTextContentPart"/>.
        /// </summary>
        /// <param name="type"> The type of the content part. </param>
        /// <returns> A new <see cref="VoiceLive.MessageContentPart"/> instance for mocking. </returns>
        public static MessageContentPart MessageContentPart(string @type = default)
        {
            return new UnknownMessageContentPart(new ContentPartType(@type), additionalBinaryDataProperties: null);
        }

        /// <summary> Input text content part. </summary>
        /// <param name="text"></param>
        /// <returns> A new <see cref="VoiceLive.InputTextContentPart"/> instance for mocking. </returns>
        public static InputTextContentPart InputTextContentPart(string text = default)
        {
            return new InputTextContentPart(ContentPartType.InputText, additionalBinaryDataProperties: null, text);
        }

        /// <summary> Input audio content part. </summary>
        /// <param name="audio"></param>
        /// <param name="transcript"></param>
        /// <returns> A new <see cref="VoiceLive.InputAudioContentPart"/> instance for mocking. </returns>
        public static InputAudioContentPart InputAudioContentPart(string audio = default, string transcript = default)
        {
            return new InputAudioContentPart(ContentPartType.InputAudio, additionalBinaryDataProperties: null, audio, transcript);
        }

        /// <summary> Output text content part. </summary>
        /// <param name="text"> The text content. </param>
        /// <returns> A new <see cref="VoiceLive.OutputTextContentPart"/> instance for mocking. </returns>
        public static OutputTextContentPart OutputTextContentPart(string text = default)
        {
            return new OutputTextContentPart(ContentPartType.Text, additionalBinaryDataProperties: null, text);
        }

        /// <summary> A system message item within a conversation. </summary>
        /// <param name="id"></param>
        /// <param name="content"> The content parts of the message. </param>
        /// <param name="status"> Processing status of the message item. </param>
        /// <returns> A new <see cref="VoiceLive.SystemMessageItem"/> instance for mocking. </returns>
        public static SystemMessageItem SystemMessageItem(string id = default, IEnumerable<MessageContentPart> content = default, ItemParamStatus? status = default)
        {
            content ??= new ChangeTrackingList<MessageContentPart>();

            return new SystemMessageItem(
                ItemType.Message,
                id,
                additionalBinaryDataProperties: null,
                ResponseMessageRole.System,
                content.ToList(),
                status);
        }

        /// <summary> A user message item within a conversation. </summary>
        /// <param name="id"></param>
        /// <param name="content"> The content parts of the message. </param>
        /// <param name="status"> Processing status of the message item. </param>
        /// <returns> A new <see cref="VoiceLive.UserMessageItem"/> instance for mocking. </returns>
        public static UserMessageItem UserMessageItem(string id = default, IEnumerable<MessageContentPart> content = default, ItemParamStatus? status = default)
        {
            content ??= new ChangeTrackingList<MessageContentPart>();

            return new UserMessageItem(
                ItemType.Message,
                id,
                additionalBinaryDataProperties: null,
                ResponseMessageRole.User,
                content.ToList(),
                status);
        }

        /// <summary> An assistant message item within a conversation. </summary>
        /// <param name="id"></param>
        /// <param name="content"> The content parts of the message. </param>
        /// <param name="status"> Processing status of the message item. </param>
        /// <returns> A new <see cref="VoiceLive.AssistantMessageItem"/> instance for mocking. </returns>
        public static AssistantMessageItem AssistantMessageItem(string id = default, IEnumerable<MessageContentPart> content = default, ItemParamStatus? status = default)
        {
            content ??= new ChangeTrackingList<MessageContentPart>();

            return new AssistantMessageItem(
                ItemType.Message,
                id,
                additionalBinaryDataProperties: null,
                ResponseMessageRole.Assistant,
                content.ToList(),
                status);
        }

        /// <summary> A function call item within a conversation. </summary>
        /// <param name="id"></param>
        /// <param name="name"></param>
        /// <param name="callId"></param>
        /// <param name="arguments"></param>
        /// <param name="status"></param>
        /// <returns> A new <see cref="VoiceLive.FunctionCallItem"/> instance for mocking. </returns>
        public static FunctionCallItem FunctionCallItem(string id = default, string name = default, string callId = default, string arguments = default, ItemParamStatus? status = default)
        {
            return new FunctionCallItem(
                ItemType.FunctionCall,
                id,
                additionalBinaryDataProperties: null,
                name,
                callId,
                arguments,
                status);
        }

        /// <summary> A function call output item within a conversation. </summary>
        /// <param name="id"></param>
        /// <param name="callId"></param>
        /// <param name="output"></param>
        /// <param name="status"></param>
        /// <returns> A new <see cref="VoiceLive.FunctionCallOutputItem"/> instance for mocking. </returns>
        public static FunctionCallOutputItem FunctionCallOutputItem(string id = default, string callId = default, string output = default, ItemParamStatus? status = default)
        {
            return new FunctionCallOutputItem(
                ItemType.FunctionCallOutput,
                id,
                additionalBinaryDataProperties: null,
                callId,
                output,
                status);
        }

        /// <summary>
        /// Top-level union for turn detection configuration.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="VoiceLive.ServerVadTurnDetection"/>, <see cref="VoiceLive.AzureSemanticVadTurnDetection"/>, <see cref="VoiceLive.AzureSemanticVadTurnDetectionEn"/>, and <see cref="VoiceLive.AzureSemanticVadTurnDetectionMultilingual"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <returns> A new <see cref="VoiceLive.TurnDetection"/> instance for mocking. </returns>
        public static TurnDetection TurnDetection(string @type = default)
        {
            return new UnknownTurnDetection(new TurnDetectionType(@type), additionalBinaryDataProperties: null);
        }

        /// <summary> Base model for VAD-based turn detection. </summary>
        /// <param name="threshold"></param>
        /// <param name="prefixPaddingMs"></param>
        /// <param name="silenceDurationMs"></param>
        /// <param name="endOfUtteranceDetection"></param>
        /// <param name="autoTruncate"></param>
        /// <param name="createResponse"></param>
        /// <param name="interruptResponse"></param>
        /// <returns> A new <see cref="VoiceLive.ServerVadTurnDetection"/> instance for mocking. </returns>
        public static ServerVadTurnDetection ServerVadTurnDetection(float? threshold = default, int? prefixPaddingMs = default, int? silenceDurationMs = default, EouDetection endOfUtteranceDetection = default, bool? autoTruncate = default, bool? createResponse = default, bool? interruptResponse = default)
        {
            return new ServerVadTurnDetection(
                TurnDetectionType.ServerVad,
                additionalBinaryDataProperties: null,
                threshold,
                prefixPaddingMs,
                silenceDurationMs,
                endOfUtteranceDetection,
                autoTruncate,
                createResponse,
                interruptResponse);
        }

        /// <summary>
        /// Top-level union for end-of-utterance (EOU) semantic detection configuration.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="VoiceLive.AzureSemanticEouDetection"/>, <see cref="VoiceLive.AzureSemanticEouDetectionEn"/>, and <see cref="VoiceLive.AzureSemanticEouDetectionMultilingual"/>.
        /// </summary>
        /// <param name="model"></param>
        /// <returns> A new <see cref="VoiceLive.EouDetection"/> instance for mocking. </returns>
        public static EouDetection EouDetection(string model = default)
        {
            return new UnknownEouDetection(new EouDetectionModel(model), additionalBinaryDataProperties: null);
        }

        /// <summary> Azure semantic end-of-utterance detection (default). </summary>
        /// <param name="thresholdLevel"> Threshold level setting. Recommended instead of `threshold`. One of `low`, `medium`, `high`, or `default`. </param>
        /// <param name="timeoutMs"> Timeout in milliseconds. Recommended instead of `timeout`. </param>
        /// <returns> A new <see cref="VoiceLive.AzureSemanticEouDetection"/> instance for mocking. </returns>
        public static AzureSemanticEouDetection AzureSemanticEouDetection(EouThresholdLevel? thresholdLevel = default, float? timeoutMs = default)
        {
            return new AzureSemanticEouDetection(EouDetectionModel.SemanticDetectionV1, additionalBinaryDataProperties: null, thresholdLevel, timeoutMs);
        }

        /// <summary> Azure semantic end-of-utterance detection (English-optimized). </summary>
        /// <param name="thresholdLevel"> Threshold level setting. Recommended instead of `threshold`. One of `low`, `medium`, `high`, or `default`. </param>
        /// <param name="timeoutMs"> Timeout in milliseconds. Recommended instead of `timeout`. </param>
        /// <returns> A new <see cref="VoiceLive.AzureSemanticEouDetectionEn"/> instance for mocking. </returns>
        public static AzureSemanticEouDetectionEn AzureSemanticEouDetectionEn(EouThresholdLevel? thresholdLevel = default, float? timeoutMs = default)
        {
            return new AzureSemanticEouDetectionEn(EouDetectionModel.SemanticDetectionV1En, additionalBinaryDataProperties: null, thresholdLevel, timeoutMs);
        }

        /// <summary> Azure semantic end-of-utterance detection (multilingual). </summary>
        /// <param name="thresholdLevel"> Threshold level setting. Recommended instead of `threshold`. One of `low`, `medium`, `high`, or `default`. </param>
        /// <param name="timeoutMs"> Timeout in milliseconds. Recommended instead of `timeout`. </param>
        /// <returns> A new <see cref="VoiceLive.AzureSemanticEouDetectionMultilingual"/> instance for mocking. </returns>
        public static AzureSemanticEouDetectionMultilingual AzureSemanticEouDetectionMultilingual(EouThresholdLevel? thresholdLevel = default, float? timeoutMs = default)
        {
            return new AzureSemanticEouDetectionMultilingual(EouDetectionModel.SemanticDetectionV1Multilingual, additionalBinaryDataProperties: null, thresholdLevel, timeoutMs);
        }

        /// <summary> Server Speech Detection (Azure semantic VAD, default variant). </summary>
        /// <param name="threshold"></param>
        /// <param name="prefixPaddingMs"></param>
        /// <param name="silenceDurationMs"></param>
        /// <param name="endOfUtteranceDetection"></param>
        /// <param name="speechDurationMs"></param>
        /// <param name="removeFillerWords"></param>
        /// <param name="languages"></param>
        /// <param name="autoTruncate"></param>
        /// <param name="createResponse"></param>
        /// <param name="interruptResponse"></param>
        /// <returns> A new <see cref="VoiceLive.AzureSemanticVadTurnDetection"/> instance for mocking. </returns>
        public static AzureSemanticVadTurnDetection AzureSemanticVadTurnDetection(float? threshold = default, int? prefixPaddingMs = default, int? silenceDurationMs = default, EouDetection endOfUtteranceDetection = default, int? speechDurationMs = default, bool? removeFillerWords = default, IEnumerable<string> languages = default, bool? autoTruncate = default, bool? createResponse = default, bool? interruptResponse = default)
        {
            languages ??= new ChangeTrackingList<string>();

            return new AzureSemanticVadTurnDetection(
                TurnDetectionType.AzureSemanticVad,
                additionalBinaryDataProperties: null,
                threshold,
                prefixPaddingMs,
                silenceDurationMs,
                endOfUtteranceDetection,
                speechDurationMs,
                removeFillerWords,
                languages.ToList(),
                autoTruncate,
                createResponse,
                interruptResponse);
        }

        /// <summary> Server Speech Detection (Azure semantic VAD, English-only). </summary>
        /// <param name="threshold"></param>
        /// <param name="prefixPaddingMs"></param>
        /// <param name="silenceDurationMs"></param>
        /// <param name="endOfUtteranceDetection"></param>
        /// <param name="speechDurationMs"></param>
        /// <param name="removeFillerWords"></param>
        /// <param name="autoTruncate"></param>
        /// <param name="createResponse"></param>
        /// <param name="interruptResponse"></param>
        /// <returns> A new <see cref="VoiceLive.AzureSemanticVadTurnDetectionEn"/> instance for mocking. </returns>
        public static AzureSemanticVadTurnDetectionEn AzureSemanticVadTurnDetectionEn(float? threshold = default, int? prefixPaddingMs = default, int? silenceDurationMs = default, EouDetection endOfUtteranceDetection = default, int? speechDurationMs = default, bool? removeFillerWords = default, bool? autoTruncate = default, bool? createResponse = default, bool? interruptResponse = default)
        {
            return new AzureSemanticVadTurnDetectionEn(
                TurnDetectionType.AzureSemanticVadEn,
                additionalBinaryDataProperties: null,
                threshold,
                prefixPaddingMs,
                silenceDurationMs,
                endOfUtteranceDetection,
                speechDurationMs,
                removeFillerWords,
                autoTruncate,
                createResponse,
                interruptResponse);
        }

        /// <summary> Server Speech Detection (Azure semantic VAD). </summary>
        /// <param name="threshold"></param>
        /// <param name="prefixPaddingMs"></param>
        /// <param name="silenceDurationMs"></param>
        /// <param name="endOfUtteranceDetection"></param>
        /// <param name="speechDurationMs"></param>
        /// <param name="removeFillerWords"></param>
        /// <param name="languages"></param>
        /// <param name="autoTruncate"></param>
        /// <param name="createResponse"></param>
        /// <param name="interruptResponse"></param>
        /// <returns> A new <see cref="VoiceLive.AzureSemanticVadTurnDetectionMultilingual"/> instance for mocking. </returns>
        public static AzureSemanticVadTurnDetectionMultilingual AzureSemanticVadTurnDetectionMultilingual(float? threshold = default, int? prefixPaddingMs = default, int? silenceDurationMs = default, EouDetection endOfUtteranceDetection = default, int? speechDurationMs = default, bool? removeFillerWords = default, IEnumerable<string> languages = default, bool? autoTruncate = default, bool? createResponse = default, bool? interruptResponse = default)
        {
            languages ??= new ChangeTrackingList<string>();

            return new AzureSemanticVadTurnDetectionMultilingual(
                TurnDetectionType.AzureSemanticVadMultilingual,
                additionalBinaryDataProperties: null,
                threshold,
                prefixPaddingMs,
                silenceDurationMs,
                endOfUtteranceDetection,
                speechDurationMs,
                removeFillerWords,
                languages.ToList(),
                autoTruncate,
                createResponse,
                interruptResponse);
        }

        /// <summary> The response resource. </summary>
        /// <param name="id"> The unique ID of the response. </param>
        /// <param name="object"> The object type, must be `realtime.response`. </param>
        /// <param name="status">
        /// The final status of the response.
        /// 
        /// One of: `completed`, `cancelled`, `failed`, `incomplete`, or `in_progress`.
        /// </param>
        /// <param name="statusDetails"> Additional details about the status. </param>
        /// <param name="output"> The list of output items generated by the response. </param>
        /// <param name="usage">
        /// Usage statistics for the Response, this will correspond to billing. A
        /// VoiceLive API session will maintain a conversation context and append new
        /// Items to the Conversation, thus output from previous turns (text and
        /// audio tokens) will become the input for later turns.
        /// </param>
        /// <param name="conversationId">
        /// Which conversation the response is added to, determined by the `conversation`
        /// field in the `response.create` event. If `auto`, the response will be added to
        /// the default conversation and the value of `conversation_id` will be an id like
        /// `conv_1234`. If `none`, the response will not be added to any conversation and
        /// the value of `conversation_id` will be `null`. If responses are being triggered
        /// by server VAD, the response will be added to the default conversation, thus
        /// the `conversation_id` will be an id like `conv_1234`.
        /// </param>
        /// <param name="voice"> supported voice identifiers and configurations. </param>
        /// <param name="modalities">
        /// The set of modalities the model used to respond. If there are multiple modalities,
        /// the model will pick one, for example if `modalities` is `["text", "audio"]`, the model
        /// could be responding in either text or audio.
        /// </param>
        /// <param name="outputAudioFormat"> The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. </param>
        /// <param name="temperature"> Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8. </param>
        /// <param name="maxOutputTokens">
        /// Maximum number of output tokens for a single assistant response,
        /// inclusive of tool calls, that was used in this response.
        /// </param>
        /// <returns> A new <see cref="VoiceLive.SessionResponse"/> instance for mocking. </returns>
        public static SessionResponse SessionResponse(string id = default, string @object = default, SessionResponseStatus? status = default, ResponseStatusDetails statusDetails = default, IEnumerable<SessionResponseItem> output = default, ResponseTokenStatistics usage = default, string conversationId = default, VoiceProvider voice = default, IEnumerable<InteractionModality> modalities = default, OutputAudioFormat? outputAudioFormat = default, float? temperature = default, MaxResponseOutputTokensOption maxOutputTokens = default)
        {
            output ??= new ChangeTrackingList<SessionResponseItem>();
            modalities ??= new ChangeTrackingList<InteractionModality>();

            return new SessionResponse(
                id,
                @object,
                status,
                statusDetails,
                output.ToList(),
                usage,
                conversationId,
                voice,
                modalities.ToList(),
                outputAudioFormat,
                temperature,
                maxOutputTokens,
                additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Base for all non-success response details.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="VoiceLive.ResponseCancelledDetails"/>, <see cref="VoiceLive.ResponseIncompleteDetails"/>, and <see cref="VoiceLive.ResponseFailedDetails"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseStatusDetails"/> instance for mocking. </returns>
        public static ResponseStatusDetails ResponseStatusDetails(string @type = default)
        {
            return new UnknownResponseStatusDetails(new SessionResponseStatus(@type), additionalBinaryDataProperties: null);
        }

        /// <summary> Details for a cancelled response. </summary>
        /// <param name="reason"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseCancelledDetails"/> instance for mocking. </returns>
        public static ResponseCancelledDetails ResponseCancelledDetails(ResponseCancelledDetailsReason reason = default)
        {
            return new ResponseCancelledDetails(SessionResponseStatus.Cancelled, additionalBinaryDataProperties: null, reason);
        }

        /// <summary> Details for an incomplete response. </summary>
        /// <param name="reason"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseIncompleteDetails"/> instance for mocking. </returns>
        public static ResponseIncompleteDetails ResponseIncompleteDetails(ResponseIncompleteDetailsReason reason = default)
        {
            return new ResponseIncompleteDetails(SessionResponseStatus.Incomplete, additionalBinaryDataProperties: null, reason);
        }

        /// <summary> Details for a failed response. </summary>
        /// <param name="error"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseFailedDetails"/> instance for mocking. </returns>
        public static ResponseFailedDetails ResponseFailedDetails(BinaryData error = default)
        {
            return new ResponseFailedDetails(SessionResponseStatus.Failed, additionalBinaryDataProperties: null, error);
        }

        /// <summary>
        /// Base for any response item; discriminated by `type`.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="VoiceLive.SessionResponseMessageItem"/>, <see cref="VoiceLive.ResponseFunctionCallItem"/>, and <see cref="VoiceLive.ResponseFunctionCallOutputItem"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <param name="id"></param>
        /// <param name="object"></param>
        /// <returns> A new <see cref="VoiceLive.SessionResponseItem"/> instance for mocking. </returns>
        public static SessionResponseItem SessionResponseItem(string @type = default, string id = default, string @object = default)
        {
            return new UnknownSessionResponseItem(new ItemType(@type), id, @object, additionalBinaryDataProperties: null);
        }

        /// <summary> Base type for message item within a conversation. </summary>
        /// <param name="id"></param>
        /// <param name="object"></param>
        /// <param name="role"></param>
        /// <param name="content"></param>
        /// <param name="status"></param>
        /// <returns> A new <see cref="VoiceLive.SessionResponseMessageItem"/> instance for mocking. </returns>
        public static SessionResponseMessageItem SessionResponseMessageItem(string id = default, string @object = default, ResponseMessageRole role = default, IEnumerable<VoiceLiveContentPart> content = default, SessionResponseItemStatus status = default)
        {
            content ??= new ChangeTrackingList<VoiceLiveContentPart>();

            return new SessionResponseMessageItem(
                ItemType.Message,
                id,
                @object,
                additionalBinaryDataProperties: null,
                role,
                content.ToList(),
                status);
        }

        /// <summary>
        /// Base for any content part; discriminated by `type`.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="VoiceLive.RequestTextContentPart"/>, <see cref="VoiceLive.RequestAudioContentPart"/>, <see cref="VoiceLive.ResponseTextContentPart"/>, and <see cref="VoiceLive.ResponseAudioContentPart"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <returns> A new <see cref="VoiceLive.VoiceLiveContentPart"/> instance for mocking. </returns>
        public static VoiceLiveContentPart VoiceLiveContentPart(string @type = default)
        {
            return new UnknownVoiceLiveContentPart(new ContentPartType(@type), additionalBinaryDataProperties: null);
        }

        /// <summary> A text content part for a request. </summary>
        /// <param name="text"></param>
        /// <returns> A new <see cref="VoiceLive.RequestTextContentPart"/> instance for mocking. </returns>
        public static RequestTextContentPart RequestTextContentPart(string text = default)
        {
            return new RequestTextContentPart(ContentPartType.InputText, additionalBinaryDataProperties: null, text);
        }

        /// <summary> An audio content part for a request. </summary>
        /// <param name="transcript"></param>
        /// <returns> A new <see cref="VoiceLive.RequestAudioContentPart"/> instance for mocking. </returns>
        public static RequestAudioContentPart RequestAudioContentPart(string transcript = default)
        {
            return new RequestAudioContentPart(ContentPartType.InputAudio, additionalBinaryDataProperties: null, transcript);
        }

        /// <summary> A text content part for a response. </summary>
        /// <param name="text"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseTextContentPart"/> instance for mocking. </returns>
        public static ResponseTextContentPart ResponseTextContentPart(string text = default)
        {
            return new ResponseTextContentPart(ContentPartType.Text, additionalBinaryDataProperties: null, text);
        }

        /// <summary> An audio content part for a response. </summary>
        /// <param name="transcript"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseAudioContentPart"/> instance for mocking. </returns>
        public static ResponseAudioContentPart ResponseAudioContentPart(string transcript = default)
        {
            return new ResponseAudioContentPart(ContentPartType.Audio, additionalBinaryDataProperties: null, transcript);
        }

        /// <summary> A function call item within a conversation. </summary>
        /// <param name="id"></param>
        /// <param name="object"></param>
        /// <param name="name"></param>
        /// <param name="callId"></param>
        /// <param name="arguments"></param>
        /// <param name="status"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseFunctionCallItem"/> instance for mocking. </returns>
        public static ResponseFunctionCallItem ResponseFunctionCallItem(string id = default, string @object = default, string name = default, string callId = default, string arguments = default, SessionResponseItemStatus status = default)
        {
            return new ResponseFunctionCallItem(
                ItemType.FunctionCall,
                id,
                @object,
                additionalBinaryDataProperties: null,
                name,
                callId,
                arguments,
                status);
        }

        /// <summary> A function call output item within a conversation. </summary>
        /// <param name="id"></param>
        /// <param name="object"></param>
        /// <param name="callId"></param>
        /// <param name="output"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseFunctionCallOutputItem"/> instance for mocking. </returns>
        public static ResponseFunctionCallOutputItem ResponseFunctionCallOutputItem(string id = default, string @object = default, string callId = default, string output = default)
        {
            return new ResponseFunctionCallOutputItem(
                ItemType.FunctionCallOutput,
                id,
                @object,
                additionalBinaryDataProperties: null,
                callId,
                output);
        }

        /// <summary> Overall usage statistics for a response. </summary>
        /// <param name="totalTokens"> Total number of tokens (input + output). </param>
        /// <param name="inputTokens"> Number of input tokens. </param>
        /// <param name="outputTokens"> Number of output tokens. </param>
        /// <param name="inputTokenDetails"> Detailed breakdown of input tokens. </param>
        /// <param name="outputTokenDetails"> Detailed breakdown of output tokens. </param>
        /// <returns> A new <see cref="VoiceLive.ResponseTokenStatistics"/> instance for mocking. </returns>
        public static ResponseTokenStatistics ResponseTokenStatistics(int totalTokens = default, int inputTokens = default, int outputTokens = default, InputTokenDetails inputTokenDetails = default, OutputTokenDetails outputTokenDetails = default)
        {
            return new ResponseTokenStatistics(
                totalTokens,
                inputTokens,
                outputTokens,
                inputTokenDetails,
                outputTokenDetails,
                additionalBinaryDataProperties: null);
        }

        /// <summary> Details of input token usage. </summary>
        /// <param name="cachedTokens"> Number of cached tokens used in the input. </param>
        /// <param name="textTokens"> Number of text tokens used in the input. </param>
        /// <param name="audioTokens"> Number of audio tokens used in the input. </param>
        /// <param name="cachedTokensDetails"> Details of cached token usage. </param>
        /// <returns> A new <see cref="VoiceLive.InputTokenDetails"/> instance for mocking. </returns>
        public static InputTokenDetails InputTokenDetails(int cachedTokens = default, int textTokens = default, int audioTokens = default, CachedTokenDetails cachedTokensDetails = default)
        {
            return new InputTokenDetails(cachedTokens, textTokens, audioTokens, cachedTokensDetails, additionalBinaryDataProperties: null);
        }

        /// <summary> Details of output token usage. </summary>
        /// <param name="textTokens"> Number of cached text tokens. </param>
        /// <param name="audioTokens"> Number of cached audio tokens. </param>
        /// <returns> A new <see cref="VoiceLive.CachedTokenDetails"/> instance for mocking. </returns>
        public static CachedTokenDetails CachedTokenDetails(int textTokens = default, int audioTokens = default)
        {
            return new CachedTokenDetails(textTokens, audioTokens, additionalBinaryDataProperties: null);
        }

        /// <summary> Details of output token usage. </summary>
        /// <param name="textTokens"> Number of text tokens generated in the output. </param>
        /// <param name="audioTokens"> Number of audio tokens generated in the output. </param>
        /// <returns> A new <see cref="VoiceLive.OutputTokenDetails"/> instance for mocking. </returns>
        public static OutputTokenDetails OutputTokenDetails(int textTokens = default, int audioTokens = default)
        {
            return new OutputTokenDetails(textTokens, audioTokens, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// A voicelive server event.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="VoiceLive.SessionUpdateError"/>, <see cref="VoiceLive.SessionUpdateSessionCreated"/>, <see cref="VoiceLive.SessionUpdateSessionUpdated"/>, <see cref="VoiceLive.SessionUpdateAvatarConnecting"/>, <see cref="VoiceLive.SessionUpdateInputAudioBufferCommitted"/>, <see cref="VoiceLive.SessionUpdateInputAudioBufferCleared"/>, <see cref="VoiceLive.SessionUpdateInputAudioBufferSpeechStarted"/>, <see cref="VoiceLive.SessionUpdateInputAudioBufferSpeechStopped"/>, <see cref="VoiceLive.SessionUpdateConversationItemCreated"/>, <see cref="VoiceLive.SessionUpdateConversationItemInputAudioTranscriptionCompleted"/>, <see cref="VoiceLive.SessionUpdateConversationItemInputAudioTranscriptionFailed"/>, <see cref="VoiceLive.SessionUpdateConversationItemTruncated"/>, <see cref="VoiceLive.SessionUpdateConversationItemDeleted"/>, <see cref="VoiceLive.SessionUpdateResponseCreated"/>, <see cref="VoiceLive.SessionUpdateResponseDone"/>, <see cref="VoiceLive.SessionUpdateResponseOutputItemAdded"/>, <see cref="VoiceLive.SessionUpdateResponseOutputItemDone"/>, <see cref="VoiceLive.SessionUpdateResponseContentPartAdded"/>, <see cref="VoiceLive.SessionUpdateResponseContentPartDone"/>, <see cref="VoiceLive.SessionUpdateResponseTextDelta"/>, <see cref="VoiceLive.SessionUpdateResponseTextDone"/>, <see cref="VoiceLive.SessionUpdateResponseAudioTranscriptDelta"/>, <see cref="VoiceLive.SessionUpdateResponseAudioTranscriptDone"/>, <see cref="VoiceLive.SessionUpdateResponseAudioDelta"/>, <see cref="VoiceLive.SessionUpdateResponseAudioDone"/>, <see cref="VoiceLive.SessionUpdateResponseAnimationBlendshapeDelta"/>, <see cref="VoiceLive.SessionUpdateResponseAnimationBlendshapeDone"/>, <see cref="VoiceLive.SessionUpdateResponseAudioTimestampDelta"/>, <see cref="VoiceLive.SessionUpdateResponseAudioTimestampDone"/>, <see cref="VoiceLive.SessionUpdateResponseAnimationVisemeDelta"/>, <see cref="VoiceLive.SessionUpdateResponseAnimationVisemeDone"/>, <see cref="VoiceLive.SessionUpdateConversationItemInputAudioTranscriptionDelta"/>, <see cref="VoiceLive.SessionUpdateConversationItemRetrieved"/>, <see cref="VoiceLive.SessionUpdateResponseFunctionCallArgumentsDelta"/>, and <see cref="VoiceLive.SessionUpdateResponseFunctionCallArgumentsDone"/>.
        /// </summary>
        /// <param name="type"> The type of event. </param>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdate"/> instance for mocking. </returns>
        public static SessionUpdate SessionUpdate(string @type = default, string eventId = default)
        {
            return new UnknownSessionUpdate(new ServerEventType(@type), eventId, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Returned when an error occurs, which could be a client problem or a server
        /// problem. Most errors are recoverable and the session will stay open, we
        /// recommend to implementors to monitor and log error messages by default.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="error"> Details of the error. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateError"/> instance for mocking. </returns>
        public static SessionUpdateError SessionUpdateError(string eventId = default, SessionUpdateErrorDetails error = default)
        {
            return new SessionUpdateError(ServerEventType.Error, eventId, additionalBinaryDataProperties: null, error);
        }

        /// <summary> Details of the error. </summary>
        /// <param name="type"> The type of error (e.g., "invalid_request_error", "server_error"). </param>
        /// <param name="code"> Error code, if any. </param>
        /// <param name="message"> A human-readable error message. </param>
        /// <param name="param"> Parameter related to the error, if any. </param>
        /// <param name="eventId"> The event_id of the client event that caused the error, if applicable. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateErrorDetails"/> instance for mocking. </returns>
        public static SessionUpdateErrorDetails SessionUpdateErrorDetails(string @type = default, string code = default, string message = default, string @param = default, string eventId = default)
        {
            return new SessionUpdateErrorDetails(
                @type,
                code,
                message,
                @param,
                eventId,
                additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Returned when a Session is created. Emitted automatically when a new
        /// connection is established as the first server event. This event will contain
        /// the default Session configuration.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="session"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateSessionCreated"/> instance for mocking. </returns>
        public static SessionUpdateSessionCreated SessionUpdateSessionCreated(string eventId = default, VoiceLiveSessionResponse session = default)
        {
            return new SessionUpdateSessionCreated(ServerEventType.SessionCreated, eventId, additionalBinaryDataProperties: null, session);
        }

        /// <summary>
        /// Returned when a session is updated with a `session.update` event, unless
        /// there is an error.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="session"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateSessionUpdated"/> instance for mocking. </returns>
        public static SessionUpdateSessionUpdated SessionUpdateSessionUpdated(string eventId = default, VoiceLiveSessionResponse session = default)
        {
            return new SessionUpdateSessionUpdated(ServerEventType.SessionUpdated, eventId, additionalBinaryDataProperties: null, session);
        }

        /// <summary> Sent when the server is in the process of establishing an avatar media connection and provides its SDP answer. </summary>
        /// <param name="eventId"></param>
        /// <param name="serverSdp"> The server's SDP answer for the avatar connection. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateAvatarConnecting"/> instance for mocking. </returns>
        public static SessionUpdateAvatarConnecting SessionUpdateAvatarConnecting(string eventId = default, string serverSdp = default)
        {
            return new SessionUpdateAvatarConnecting(ServerEventType.SessionAvatarConnecting, eventId, additionalBinaryDataProperties: null, serverSdp);
        }

        /// <summary>
        /// Returned when an input audio buffer is committed, either by the client or
        /// automatically in server VAD mode. The `item_id` property is the ID of the user
        /// message item that will be created, thus a `conversation.item.created` event
        /// will also be sent to the client.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="previousItemId"> The ID of the preceding item after which the new item will be inserted. </param>
        /// <param name="itemId"> The ID of the user message item that will be created. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateInputAudioBufferCommitted"/> instance for mocking. </returns>
        public static SessionUpdateInputAudioBufferCommitted SessionUpdateInputAudioBufferCommitted(string eventId = default, string previousItemId = default, string itemId = default)
        {
            return new SessionUpdateInputAudioBufferCommitted(ServerEventType.InputAudioBufferCommitted, eventId, additionalBinaryDataProperties: null, previousItemId, itemId);
        }

        /// <summary>
        /// Returned when the input audio buffer is cleared by the client with a
        /// `input_audio_buffer.clear` event.
        /// </summary>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateInputAudioBufferCleared"/> instance for mocking. </returns>
        public static SessionUpdateInputAudioBufferCleared SessionUpdateInputAudioBufferCleared(string eventId = default)
        {
            return new SessionUpdateInputAudioBufferCleared(ServerEventType.InputAudioBufferCleared, eventId, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Sent by the server when in `server_vad` mode to indicate that speech has been
        /// detected in the audio buffer. This can happen any time audio is added to the
        /// buffer (unless speech is already detected). The client may want to use this
        /// event to interrupt audio playback or provide visual feedback to the user.
        /// The client should expect to receive a `input_audio_buffer.speech_stopped` event
        /// when speech stops. The `item_id` property is the ID of the user message item
        /// that will be created when speech stops and will also be included in the
        /// `input_audio_buffer.speech_stopped` event (unless the client manually commits
        /// the audio buffer during VAD activation).
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="audioStartMs">
        /// Milliseconds from the start of all audio written to the buffer during the
        /// session when speech was first detected. This will correspond to the
        /// beginning of audio sent to the model, and thus includes the
        /// `prefix_padding_ms` configured in the Session.
        /// </param>
        /// <param name="itemId"> The ID of the user message item that will be created when speech stops. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateInputAudioBufferSpeechStarted"/> instance for mocking. </returns>
        public static SessionUpdateInputAudioBufferSpeechStarted SessionUpdateInputAudioBufferSpeechStarted(string eventId = default, int audioStartMs = default, string itemId = default)
        {
            return new SessionUpdateInputAudioBufferSpeechStarted(ServerEventType.InputAudioBufferSpeechStarted, eventId, additionalBinaryDataProperties: null, audioStartMs, itemId);
        }

        /// <summary>
        /// Returned in `server_vad` mode when the server detects the end of speech in
        /// the audio buffer. The server will also send an `conversation.item.created`
        /// event with the user message item that is created from the audio buffer.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="audioEndMs">
        /// Milliseconds since the session started when speech stopped. This will
        /// correspond to the end of audio sent to the model, and thus includes the
        /// `min_silence_duration_ms` configured in the Session.
        /// </param>
        /// <param name="itemId"> The ID of the user message item that will be created. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateInputAudioBufferSpeechStopped"/> instance for mocking. </returns>
        public static SessionUpdateInputAudioBufferSpeechStopped SessionUpdateInputAudioBufferSpeechStopped(string eventId = default, int audioEndMs = default, string itemId = default)
        {
            return new SessionUpdateInputAudioBufferSpeechStopped(ServerEventType.InputAudioBufferSpeechStopped, eventId, additionalBinaryDataProperties: null, audioEndMs, itemId);
        }

        /// <summary>
        /// Returned when a conversation item is created. There are several scenarios that produce this event:
        /// - The server is generating a Response, which if successful will produce
        /// either one or two Items, which will be of type `message`
        /// (role `assistant`) or type `function_call`.
        /// - The input audio buffer has been committed, either by the client or the
        /// server (in `server_vad` mode). The server will take the content of the
        /// input audio buffer and add it to a new user message Item.
        /// - The client has sent a `conversation.item.create` event to add a new Item
        /// to the Conversation.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="previousItemId">
        /// The ID of the preceding item in the Conversation context, allows the
        /// client to understand the order of the conversation.
        /// </param>
        /// <param name="item"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateConversationItemCreated"/> instance for mocking. </returns>
        public static SessionUpdateConversationItemCreated SessionUpdateConversationItemCreated(string eventId = default, string previousItemId = default, SessionResponseItem item = default)
        {
            return new SessionUpdateConversationItemCreated(ServerEventType.ConversationItemCreated, eventId, additionalBinaryDataProperties: null, previousItemId, item);
        }

        /// <summary>
        /// This event is the output of audio transcription for user audio written to the
        /// user audio buffer. Transcription begins when the input audio buffer is
        /// committed by the client or server (in `server_vad` mode). Transcription runs
        /// asynchronously with Response creation, so this event may come before or after
        /// the Response events.
        /// VoiceLive API models accept audio natively, and thus input transcription is a
        /// separate process run on a separate ASR (Automatic Speech Recognition) model.
        /// The transcript may diverge somewhat from the model's interpretation, and
        /// should be treated as a rough guide.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="itemId"> The ID of the user message item containing the audio. </param>
        /// <param name="contentIndex"> The index of the content part containing the audio. </param>
        /// <param name="transcript"> The transcribed text. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateConversationItemInputAudioTranscriptionCompleted"/> instance for mocking. </returns>
        public static SessionUpdateConversationItemInputAudioTranscriptionCompleted SessionUpdateConversationItemInputAudioTranscriptionCompleted(string eventId = default, string itemId = default, int contentIndex = default, string transcript = default)
        {
            return new SessionUpdateConversationItemInputAudioTranscriptionCompleted(
                ServerEventType.ConversationItemInputAudioTranscriptionCompleted,
                eventId,
                additionalBinaryDataProperties: null,
                itemId,
                contentIndex,
                transcript);
        }

        /// <summary>
        /// Returned when input audio transcription is configured, and a transcription
        /// request for a user message failed. These events are separate from other
        /// `error` events so that the client can identify the related Item.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="itemId"> The ID of the user message item. </param>
        /// <param name="contentIndex"> The index of the content part containing the audio. </param>
        /// <param name="error"> Details of the transcription error. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateConversationItemInputAudioTranscriptionFailed"/> instance for mocking. </returns>
        public static SessionUpdateConversationItemInputAudioTranscriptionFailed SessionUpdateConversationItemInputAudioTranscriptionFailed(string eventId = default, string itemId = default, int contentIndex = default, VoiceLiveErrorDetails error = default)
        {
            return new SessionUpdateConversationItemInputAudioTranscriptionFailed(
                ServerEventType.ConversationItemInputAudioTranscriptionFailed,
                eventId,
                additionalBinaryDataProperties: null,
                itemId,
                contentIndex,
                error);
        }

        /// <summary>
        /// Returned when an earlier assistant audio message item is truncated by the
        /// client with a `conversation.item.truncate` event. This event is used to
        /// synchronize the server's understanding of the audio with the client's playback.
        /// This action will truncate the audio and remove the server-side text transcript
        /// to ensure there is no text in the context that hasn't been heard by the user.
        /// </summary>
        /// <param name="itemId"> The ID of the assistant message item that was truncated. </param>
        /// <param name="contentIndex"> The index of the content part that was truncated. </param>
        /// <param name="audioEndMs"> The duration up to which the audio was truncated, in milliseconds. </param>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateConversationItemTruncated"/> instance for mocking. </returns>
        public static SessionUpdateConversationItemTruncated SessionUpdateConversationItemTruncated(string itemId = default, int contentIndex = default, int audioEndMs = default, string eventId = default)
        {
            return new SessionUpdateConversationItemTruncated(
                ServerEventType.ConversationItemTruncated,
                additionalBinaryDataProperties: null,
                itemId,
                contentIndex,
                audioEndMs,
                eventId);
        }

        /// <summary>
        /// Returned when an item in the conversation is deleted by the client with a
        /// `conversation.item.delete` event. This event is used to synchronize the
        /// server's understanding of the conversation history with the client's view.
        /// </summary>
        /// <param name="itemId"> The ID of the item that was deleted. </param>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateConversationItemDeleted"/> instance for mocking. </returns>
        public static SessionUpdateConversationItemDeleted SessionUpdateConversationItemDeleted(string itemId = default, string eventId = default)
        {
            return new SessionUpdateConversationItemDeleted(ServerEventType.ConversationItemDeleted, additionalBinaryDataProperties: null, itemId, eventId);
        }

        /// <summary>
        /// Returned when a new Response is created. The first event of response creation,
        /// where the response is in an initial state of `in_progress`.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="response"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseCreated"/> instance for mocking. </returns>
        public static SessionUpdateResponseCreated SessionUpdateResponseCreated(string eventId = default, SessionResponse response = default)
        {
            return new SessionUpdateResponseCreated(ServerEventType.ResponseCreated, eventId, additionalBinaryDataProperties: null, response);
        }

        /// <summary>
        /// Returned when a Response is done streaming. Always emitted, no matter the
        /// final state. The Response object included in the `response.done` event will
        /// include all output Items in the Response but will omit the raw audio data.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="response"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseDone"/> instance for mocking. </returns>
        public static SessionUpdateResponseDone SessionUpdateResponseDone(string eventId = default, SessionResponse response = default)
        {
            return new SessionUpdateResponseDone(ServerEventType.ResponseDone, eventId, additionalBinaryDataProperties: null, response);
        }

        /// <summary> Returned when a new Item is created during Response generation. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the Response to which the item belongs. </param>
        /// <param name="outputIndex"> The index of the output item in the Response. </param>
        /// <param name="item"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseOutputItemAdded"/> instance for mocking. </returns>
        public static SessionUpdateResponseOutputItemAdded SessionUpdateResponseOutputItemAdded(string eventId = default, string responseId = default, int outputIndex = default, SessionResponseItem item = default)
        {
            return new SessionUpdateResponseOutputItemAdded(
                ServerEventType.ResponseOutputItemAdded,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                outputIndex,
                item);
        }

        /// <summary>
        /// Returned when an Item is done streaming. Also emitted when a Response is
        /// interrupted, incomplete, or cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the Response to which the item belongs. </param>
        /// <param name="outputIndex"> The index of the output item in the Response. </param>
        /// <param name="item"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseOutputItemDone"/> instance for mocking. </returns>
        public static SessionUpdateResponseOutputItemDone SessionUpdateResponseOutputItemDone(string eventId = default, string responseId = default, int outputIndex = default, SessionResponseItem item = default)
        {
            return new SessionUpdateResponseOutputItemDone(
                ServerEventType.ResponseOutputItemDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                outputIndex,
                item);
        }

        /// <summary>
        /// Returned when a new content part is added to an assistant message item during
        /// response generation.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item to which the content part was added. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="part"> The content part that was added. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseContentPartAdded"/> instance for mocking. </returns>
        public static SessionUpdateResponseContentPartAdded SessionUpdateResponseContentPartAdded(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, VoiceLiveContentPart part = default)
        {
            return new SessionUpdateResponseContentPartAdded(
                ServerEventType.ResponseContentPartAdded,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                part);
        }

        /// <summary>
        /// Returned when a content part is done streaming in an assistant message item.
        /// Also emitted when a Response is interrupted, incomplete, or cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="part"> The content part that is done. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseContentPartDone"/> instance for mocking. </returns>
        public static SessionUpdateResponseContentPartDone SessionUpdateResponseContentPartDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, VoiceLiveContentPart part = default)
        {
            return new SessionUpdateResponseContentPartDone(
                ServerEventType.ResponseContentPartDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                part);
        }

        /// <summary> Returned when the text value of a "text" content part is updated. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="delta"> The text delta. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseTextDelta"/> instance for mocking. </returns>
        public static SessionUpdateResponseTextDelta SessionUpdateResponseTextDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, string delta = default)
        {
            return new SessionUpdateResponseTextDelta(
                ServerEventType.ResponseTextDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                delta);
        }

        /// <summary>
        /// Returned when the text value of a "text" content part is done streaming. Also
        /// emitted when a Response is interrupted, incomplete, or cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="text"> The final text content. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseTextDone"/> instance for mocking. </returns>
        public static SessionUpdateResponseTextDone SessionUpdateResponseTextDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, string text = default)
        {
            return new SessionUpdateResponseTextDone(
                ServerEventType.ResponseTextDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                text);
        }

        /// <summary> Returned when the model-generated transcription of audio output is updated. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="delta"> The transcript delta. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseAudioTranscriptDelta"/> instance for mocking. </returns>
        public static SessionUpdateResponseAudioTranscriptDelta SessionUpdateResponseAudioTranscriptDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, string delta = default)
        {
            return new SessionUpdateResponseAudioTranscriptDelta(
                ServerEventType.ResponseAudioTranscriptDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                delta);
        }

        /// <summary>
        /// Returned when the model-generated transcription of audio output is done
        /// streaming. Also emitted when a Response is interrupted, incomplete, or
        /// cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="transcript"> The final transcript of the audio. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseAudioTranscriptDone"/> instance for mocking. </returns>
        public static SessionUpdateResponseAudioTranscriptDone SessionUpdateResponseAudioTranscriptDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, string transcript = default)
        {
            return new SessionUpdateResponseAudioTranscriptDone(
                ServerEventType.ResponseAudioTranscriptDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                transcript);
        }

        /// <summary> Returned when the model-generated audio is updated. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="delta"> Base64-encoded audio data delta. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseAudioDelta"/> instance for mocking. </returns>
        public static SessionUpdateResponseAudioDelta SessionUpdateResponseAudioDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, BinaryData delta = default)
        {
            return new SessionUpdateResponseAudioDelta(
                ServerEventType.ResponseAudioDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                delta);
        }

        /// <summary>
        /// Returned when the model-generated audio is done. Also emitted when a Response
        /// is interrupted, incomplete, or cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseAudioDone"/> instance for mocking. </returns>
        public static SessionUpdateResponseAudioDone SessionUpdateResponseAudioDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default)
        {
            return new SessionUpdateResponseAudioDone(
                ServerEventType.ResponseAudioDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex);
        }

        /// <summary> Represents a delta update of blendshape animation frames for a specific output of a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <param name="frames"></param>
        /// <param name="frameIndex"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseAnimationBlendshapeDelta"/> instance for mocking. </returns>
        public static SessionUpdateResponseAnimationBlendshapeDelta SessionUpdateResponseAnimationBlendshapeDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, BinaryData frames = default, int frameIndex = default)
        {
            return new SessionUpdateResponseAnimationBlendshapeDelta(
                ServerEventType.ResponseAnimationBlendshapesDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                frames,
                frameIndex);
        }

        /// <summary> Indicates the completion of blendshape animation processing for a specific output of a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseAnimationBlendshapeDone"/> instance for mocking. </returns>
        public static SessionUpdateResponseAnimationBlendshapeDone SessionUpdateResponseAnimationBlendshapeDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default)
        {
            return new SessionUpdateResponseAnimationBlendshapeDone(
                ServerEventType.ResponseAnimationBlendshapesDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex);
        }

        /// <summary> Represents a word-level audio timestamp delta for a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <param name="audioOffsetMs"></param>
        /// <param name="audioDurationMs"></param>
        /// <param name="text"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseAudioTimestampDelta"/> instance for mocking. </returns>
        public static SessionUpdateResponseAudioTimestampDelta SessionUpdateResponseAudioTimestampDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, int audioOffsetMs = default, int audioDurationMs = default, string text = default)
        {
            return new SessionUpdateResponseAudioTimestampDelta(
                ServerEventType.ResponseAudioTimestampDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                audioOffsetMs,
                audioDurationMs,
                text,
                "word");
        }

        /// <summary> Indicates completion of audio timestamp delivery for a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseAudioTimestampDone"/> instance for mocking. </returns>
        public static SessionUpdateResponseAudioTimestampDone SessionUpdateResponseAudioTimestampDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default)
        {
            return new SessionUpdateResponseAudioTimestampDone(
                ServerEventType.ResponseAudioTimestampDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex);
        }

        /// <summary> Represents a viseme ID delta update for animation based on audio. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <param name="audioOffsetMs"></param>
        /// <param name="visemeId"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseAnimationVisemeDelta"/> instance for mocking. </returns>
        public static SessionUpdateResponseAnimationVisemeDelta SessionUpdateResponseAnimationVisemeDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, int audioOffsetMs = default, int visemeId = default)
        {
            return new SessionUpdateResponseAnimationVisemeDelta(
                ServerEventType.ResponseAnimationVisemeDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                audioOffsetMs,
                visemeId);
        }

        /// <summary> Indicates completion of viseme animation delivery for a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseAnimationVisemeDone"/> instance for mocking. </returns>
        public static SessionUpdateResponseAnimationVisemeDone SessionUpdateResponseAnimationVisemeDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default)
        {
            return new SessionUpdateResponseAnimationVisemeDone(
                ServerEventType.ResponseAnimationVisemeDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex);
        }

        /// <summary> Returned when the text value of an input audio transcription content part is updated. </summary>
        /// <param name="eventId"></param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="delta"> The text delta. </param>
        /// <param name="logprobs"> The log probabilities of the transcription. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateConversationItemInputAudioTranscriptionDelta"/> instance for mocking. </returns>
        public static SessionUpdateConversationItemInputAudioTranscriptionDelta SessionUpdateConversationItemInputAudioTranscriptionDelta(string eventId = default, string itemId = default, int? contentIndex = default, string delta = default, IEnumerable<LogProbProperties> logprobs = default)
        {
            logprobs ??= new ChangeTrackingList<LogProbProperties>();

            return new SessionUpdateConversationItemInputAudioTranscriptionDelta(
                ServerEventType.ConversationItemInputAudioTranscriptionDelta,
                eventId,
                additionalBinaryDataProperties: null,
                itemId,
                contentIndex,
                delta,
                logprobs.ToList());
        }

        /// <summary> Returned when a conversation item is retrieved with `conversation.item.retrieve`. </summary>
        /// <param name="item"></param>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateConversationItemRetrieved"/> instance for mocking. </returns>
        public static SessionUpdateConversationItemRetrieved SessionUpdateConversationItemRetrieved(SessionResponseItem item = default, string eventId = default)
        {
            return new SessionUpdateConversationItemRetrieved(ServerEventType.ConversationItemRetrieved, additionalBinaryDataProperties: null, item, eventId);
        }

        /// <summary> Returned when the model-generated function call arguments are updated. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the function call item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="callId"> The ID of the function call. </param>
        /// <param name="delta"> The arguments delta as a JSON string. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseFunctionCallArgumentsDelta"/> instance for mocking. </returns>
        public static SessionUpdateResponseFunctionCallArgumentsDelta SessionUpdateResponseFunctionCallArgumentsDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, string callId = default, string delta = default)
        {
            return new SessionUpdateResponseFunctionCallArgumentsDelta(
                ServerEventType.ResponseFunctionCallArgumentsDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                callId,
                delta);
        }

        /// <summary>
        /// Returned when the model-generated function call arguments are done streaming.
        /// Also emitted when a Response is interrupted, incomplete, or cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the function call item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="callId"> The ID of the function call. </param>
        /// <param name="arguments"> The final arguments as a JSON string. </param>
        /// <param name="name"> The name of the function call. </param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseFunctionCallArgumentsDone"/> instance for mocking. </returns>
        public static SessionUpdateResponseFunctionCallArgumentsDone SessionUpdateResponseFunctionCallArgumentsDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, string callId = default, string arguments = default, string name = default)
        {
            return new SessionUpdateResponseFunctionCallArgumentsDone(
                ServerEventType.ResponseFunctionCallArgumentsDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                callId,
                arguments,
                name);
        }

        /// <summary>
        /// OpenAI voice configuration with explicit type field.
        ///             
        ///             This provides a unified interface for OpenAI voices, complementing the
        ///             existing string-based OAIVoice for backward compatibility.
        /// </summary>
        /// <param name="type"> The type of the voice. </param>
        /// <param name="name"> The name of the OpenAI voice. </param>
        /// <returns> A new <see cref="VoiceLive.OpenAIVoice"/> instance for mocking. </returns>
        [EditorBrowsable(EditorBrowsableState.Never)]
        public static OpenAIVoice OpenAIVoice(string @type, OAIVoice name)
        {
            return new OpenAIVoice(@type, name, additionalBinaryDataProperties: null);
        }

        /// <summary> Echo cancellation configuration for server-side audio processing. </summary>
        /// <param name="type"> The type of echo cancellation model to use. </param>
        /// <returns> A new <see cref="VoiceLive.AudioEchoCancellation"/> instance for mocking. </returns>
        [EditorBrowsable(EditorBrowsableState.Never)]
        public static AudioEchoCancellation AudioEchoCancellation(string @type)
        {
            return new AudioEchoCancellation(@type, additionalBinaryDataProperties: null);
        }

        /// <summary> Represents a word-level audio timestamp delta for a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <param name="audioOffsetMs"> Gets the AudioOffsetMs. </param>
        /// <param name="audioDurationMs"> Gets the AudioDurationMs. </param>
        /// <param name="text"></param>
        /// <param name="timestampType"></param>
        /// <returns> A new <see cref="VoiceLive.SessionUpdateResponseAudioTimestampDelta"/> instance for mocking. </returns>
        [EditorBrowsable(EditorBrowsableState.Never)]
        public static SessionUpdateResponseAudioTimestampDelta SessionUpdateResponseAudioTimestampDelta(string eventId, string responseId, string itemId, int outputIndex, int contentIndex, int audioOffsetMs, int audioDurationMs, string text, string timestampType)
        {
            return new SessionUpdateResponseAudioTimestampDelta(
                ServerEventType.ResponseAudioTimestampDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                audioOffsetMs,
                audioDurationMs,
                text,
                timestampType);
        }
    }
}
