// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

// <auto-generated/>

#nullable disable

using System;
using System.Collections.Generic;
using System.Linq;

namespace Azure.AI.VoiceLive
{
    /// <summary> A factory class for creating instances of the models for mocking. </summary>
    public static partial class VoiceLiveModelFactory
    {
        /// <summary>
        /// Send this event to update the session’s default configuration.
        /// The client may send this event at any time to update any field,
        /// except for `voice`. However, note that once a session has been
        /// initialized with a particular `model`, it can’t be changed to
        /// another model using `session.update`.
        /// 
        /// When the server receives a `session.update`, it will respond
        /// with a `session.updated` event showing the full, effective configuration.
        /// Only the fields that are present are updated. To clear a field like
        /// `instructions`, pass an empty string.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="session"></param>
        /// <returns> A new <see cref="VoiceLive.ClientEventSessionUpdate"/> instance for mocking. </returns>
        public static ClientEventSessionUpdate ClientEventSessionUpdate(string eventId = default, RequestSession session = default)
        {
            return new ClientEventSessionUpdate("session.update", eventId, additionalBinaryDataProperties: null, session);
        }

        /// <summary> The RequestSession. </summary>
        /// <param name="model"></param>
        /// <param name="modalities"></param>
        /// <param name="animation"></param>
        /// <param name="voice"></param>
        /// <param name="instructions"></param>
        /// <param name="inputAudio"></param>
        /// <param name="inputAudioSamplingRate"></param>
        /// <param name="inputAudioFormat"></param>
        /// <param name="outputAudioFormat"></param>
        /// <param name="turnDetection"></param>
        /// <param name="inputAudioNoiseReduction"></param>
        /// <param name="inputAudioEchoCancellation"></param>
        /// <param name="avatar"></param>
        /// <param name="inputAudioTranscription"></param>
        /// <param name="outputAudioTimestampTypes"></param>
        /// <param name="tools"></param>
        /// <param name="toolChoice"></param>
        /// <param name="temperature"></param>
        /// <param name="maxResponseOutputTokens"></param>
        /// <returns> A new <see cref="VoiceLive.RequestSession"/> instance for mocking. </returns>
        public static RequestSession RequestSession(string model = default, IEnumerable<InputModality> modalities = default, AnimationOptions animation = default, BinaryData voice = default, string instructions = default, InputAudio inputAudio = default, int? inputAudioSamplingRate = default, AudioFormat? inputAudioFormat = default, AudioFormat? outputAudioFormat = default, TurnDetection turnDetection = default, AudioNoiseReduction inputAudioNoiseReduction = default, AudioEchoCancellation inputAudioEchoCancellation = default, AvatarConfig avatar = default, AudioInputTranscriptionSettings inputAudioTranscription = default, IEnumerable<AudioTimestampType> outputAudioTimestampTypes = default, IEnumerable<ToolCall> tools = default, BinaryData toolChoice = default, float? temperature = default, BinaryData maxResponseOutputTokens = default)
        {
            modalities ??= new ChangeTrackingList<InputModality>();
            outputAudioTimestampTypes ??= new ChangeTrackingList<AudioTimestampType>();
            tools ??= new ChangeTrackingList<ToolCall>();

            return new RequestSession(
                model,
                modalities.ToList(),
                animation,
                voice,
                instructions,
                inputAudio,
                inputAudioSamplingRate,
                inputAudioFormat,
                outputAudioFormat,
                turnDetection,
                inputAudioNoiseReduction,
                inputAudioEchoCancellation,
                avatar,
                inputAudioTranscription,
                outputAudioTimestampTypes.ToList(),
                tools.ToList(),
                toolChoice,
                temperature,
                maxResponseOutputTokens,
                additionalBinaryDataProperties: null);
        }

        /// <summary> Configuration for animation outputs including blendshapes, visemes, and emotion metadata. </summary>
        /// <param name="modelName"> The name of the animation model to use. </param>
        /// <param name="outputs"> Set of output data types requested from the animation system. </param>
        /// <param name="emotionDetectionIntervalMs"> Interval for emotion detection in milliseconds. If not set, emotion detection is disabled. </param>
        /// <returns> A new <see cref="VoiceLive.AnimationOptions"/> instance for mocking. </returns>
        public static AnimationOptions AnimationOptions(string modelName = default, IEnumerable<AnimationOutputType> outputs = default, int? emotionDetectionIntervalMs = default)
        {
            outputs ??= new ChangeTrackingList<AnimationOutputType>();

            return new AnimationOptions(modelName, outputs.ToList(), emotionDetectionIntervalMs, additionalBinaryDataProperties: null);
        }

        /// <summary> Voice configuration for Azure standard or platform voices. </summary>
        /// <param name="name"> Name of the voice. </param>
        /// <param name="type"> Voice type identifier. </param>
        /// <param name="temperature"> Optional temperature for generation. </param>
        /// <returns> A new <see cref="VoiceLive.AzureStandardVoice"/> instance for mocking. </returns>
        public static AzureStandardVoice AzureStandardVoice(string name = default, AzureStandardVoiceType @type = default, float? temperature = default)
        {
            return new AzureStandardVoice(name, @type, temperature, additionalBinaryDataProperties: null);
        }

        /// <summary> Voice configuration for Azure custom voice. </summary>
        /// <param name="name"> Name of the voice. </param>
        /// <param name="endpointId"> Custom endpoint ID. </param>
        /// <param name="type"> Voice type identifier. </param>
        /// <param name="temperature"> Optional temperature for generation. </param>
        /// <param name="customLexiconUri"> Optional custom lexicon URL. </param>
        /// <param name="preferLocales"> Preferred locale list for voice rendering. </param>
        /// <returns> A new <see cref="VoiceLive.AzureCustomVoice"/> instance for mocking. </returns>
        public static AzureCustomVoice AzureCustomVoice(string name = default, string endpointId = default, AzureCustomVoiceType @type = default, float? temperature = default, Uri customLexiconUri = default, IEnumerable<string> preferLocales = default)
        {
            preferLocales ??= new ChangeTrackingList<string>();

            return new AzureCustomVoice(
                name,
                endpointId,
                @type,
                temperature,
                customLexiconUri,
                preferLocales.ToList(),
                additionalBinaryDataProperties: null);
        }

        /// <summary> Voice configuration for Azure personal voice. </summary>
        /// <param name="name"> Name of the voice. </param>
        /// <param name="type"> Voice type identifier. </param>
        /// <param name="model"> Personal voice model identifier. </param>
        /// <returns> A new <see cref="VoiceLive.AzurePersonalVoice"/> instance for mocking. </returns>
        public static AzurePersonalVoice AzurePersonalVoice(string name = default, AzurePersonalVoiceType @type = default, AzurePersonalVoiceModel model = default)
        {
            return new AzurePersonalVoice(name, @type, model, additionalBinaryDataProperties: null);
        }

        /// <summary> Configuration for client audio input. Used to specify the audio model and optional phrase list. </summary>
        /// <param name="model"> The name of the model to use for input audio (currently only 'azure-standard' is supported). </param>
        /// <param name="phraseList"> Optional list of phrases to bias the speech recognition engine. </param>
        /// <returns> A new <see cref="VoiceLive.InputAudio"/> instance for mocking. </returns>
        public static InputAudio InputAudio(string model = default, IEnumerable<string> phraseList = default)
        {
            phraseList ??= new ChangeTrackingList<string>();

            return new InputAudio(model, phraseList.ToList(), additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Top-level union for turn detection configuration.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="NoTurnDetection"/>, <see cref="ServerVad"/>, and <see cref="AzureSemanticVad"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <returns> A new <see cref="VoiceLive.TurnDetection"/> instance for mocking. </returns>
        public static TurnDetection TurnDetection(string @type = default)
        {
            return new UnknownTurnDetection(@type.ToTurnDetectionType(), additionalBinaryDataProperties: null);
        }

        /// <summary> Disables turn detection. </summary>
        /// <returns> A new <see cref="VoiceLive.NoTurnDetection"/> instance for mocking. </returns>
        public static NoTurnDetection NoTurnDetection()
        {
            return new NoTurnDetection(TurnDetectionType.None, additionalBinaryDataProperties: null);
        }

        /// <summary> Base model for VAD-based turn detection. </summary>
        /// <param name="threshold"></param>
        /// <param name="prefixPaddingMs"></param>
        /// <param name="silenceDurationMs"></param>
        /// <param name="endOfUtteranceDetection"></param>
        /// <returns> A new <see cref="VoiceLive.ServerVad"/> instance for mocking. </returns>
        public static ServerVad ServerVad(float? threshold = default, int? prefixPaddingMs = default, int? silenceDurationMs = default, BinaryData endOfUtteranceDetection = default)
        {
            return new ServerVad(
                TurnDetectionType.ServerVad,
                additionalBinaryDataProperties: null,
                threshold,
                prefixPaddingMs,
                silenceDurationMs,
                endOfUtteranceDetection);
        }

        /// <summary> Semantic VAD settings based on Azure SDK features. </summary>
        /// <param name="negThreshold"></param>
        /// <param name="windowSize"></param>
        /// <param name="distinctCiPhones"></param>
        /// <param name="requireVowel"></param>
        /// <param name="removeFillerWords"></param>
        /// <returns> A new <see cref="VoiceLive.AzureSemanticVad"/> instance for mocking. </returns>
        public static AzureSemanticVad AzureSemanticVad(float? negThreshold = default, int? windowSize = default, int? distinctCiPhones = default, bool? requireVowel = default, bool? removeFillerWords = default)
        {
            return new AzureSemanticVad(
                TurnDetectionType.AzureSemanticVad,
                additionalBinaryDataProperties: null,
                negThreshold,
                windowSize,
                distinctCiPhones,
                requireVowel,
                removeFillerWords);
        }

        /// <summary> Configuration for input audio noise reduction. </summary>
        /// <param name="type"> The type of noise reduction model. </param>
        /// <returns> A new <see cref="VoiceLive.AudioNoiseReduction"/> instance for mocking. </returns>
        public static AudioNoiseReduction AudioNoiseReduction(string @type = default)
        {
            return new AudioNoiseReduction(@type, additionalBinaryDataProperties: null);
        }

        /// <summary> Echo cancellation configuration for server-side audio processing. </summary>
        /// <param name="type"> The type of echo cancellation model to use. </param>
        /// <returns> A new <see cref="VoiceLive.AudioEchoCancellation"/> instance for mocking. </returns>
        public static AudioEchoCancellation AudioEchoCancellation(string @type = default)
        {
            return new AudioEchoCancellation(@type, additionalBinaryDataProperties: null);
        }

        /// <summary> Configuration for avatar streaming and behavior during the session. </summary>
        /// <param name="iceServers"> Optional list of ICE servers to use for WebRTC connection establishment. </param>
        /// <param name="character"> The character name or ID used for the avatar. </param>
        /// <param name="style"> Optional avatar style, such as emotional tone or speaking style. </param>
        /// <param name="customized"> Indicates whether the avatar is customized or not. </param>
        /// <param name="video"> Optional video configuration including resolution, bitrate, and codec. </param>
        /// <returns> A new <see cref="VoiceLive.AvatarConfig"/> instance for mocking. </returns>
        public static AvatarConfig AvatarConfig(IEnumerable<IceServer> iceServers = default, string character = default, string style = default, bool customized = default, VideoParams video = default)
        {
            iceServers ??= new ChangeTrackingList<IceServer>();

            return new AvatarConfig(
                iceServers.ToList(),
                character,
                style,
                customized,
                video,
                additionalBinaryDataProperties: null);
        }

        /// <summary> ICE server configuration for WebRTC connection negotiation. </summary>
        /// <param name="uris"> List of ICE server URLs (e.g., TURN or STUN endpoints). </param>
        /// <param name="username"> Optional username used for authentication with the ICE server. </param>
        /// <param name="credential"> Optional credential (e.g., password or token) used for authentication. </param>
        /// <returns> A new <see cref="VoiceLive.IceServer"/> instance for mocking. </returns>
        public static IceServer IceServer(IEnumerable<Uri> uris = default, string username = default, string credential = default)
        {
            uris ??= new ChangeTrackingList<Uri>();

            return new IceServer(uris.ToList(), username, credential, additionalBinaryDataProperties: null);
        }

        /// <summary> Video streaming parameters for avatar. </summary>
        /// <param name="bitrate"> Bitrate in bits per second (e.g., 2000000 for 2 Mbps). </param>
        /// <param name="codec"> Codec to use for encoding. Currently only 'h264' is supported. </param>
        /// <param name="crop"> Optional cropping settings for the video stream. </param>
        /// <param name="resolution"> Optional resolution settings for the video stream. </param>
        /// <returns> A new <see cref="VoiceLive.VideoParams"/> instance for mocking. </returns>
        public static VideoParams VideoParams(int? bitrate = default, string codec = default, VideoCrop crop = default, VideoResolution resolution = default)
        {
            return new VideoParams(bitrate, codec, crop, resolution, additionalBinaryDataProperties: null);
        }

        /// <summary> Defines a video crop rectangle using top-left and bottom-right coordinates. </summary>
        /// <param name="topLeftInternal"> Top-left corner of the crop region. </param>
        /// <param name="bottomRightInternal"> Bottom-right corner of the crop region. </param>
        /// <returns> A new <see cref="VoiceLive.VideoCrop"/> instance for mocking. </returns>
        public static VideoCrop VideoCrop(IEnumerable<int> topLeftInternal = default, IEnumerable<int> bottomRightInternal = default)
        {
            topLeftInternal ??= new ChangeTrackingList<int>();
            bottomRightInternal ??= new ChangeTrackingList<int>();

            return new VideoCrop(topLeftInternal.ToList(), bottomRightInternal.ToList(), additionalBinaryDataProperties: null);
        }

        /// <summary> Resolution of the video feed in pixels. </summary>
        /// <param name="width"> Width of the video in pixels. Must be greater than 0. </param>
        /// <param name="height"> Height of the video in pixels. Must be greater than 0. </param>
        /// <returns> A new <see cref="VoiceLive.VideoResolution"/> instance for mocking. </returns>
        public static VideoResolution VideoResolution(int width = default, int height = default)
        {
            return new VideoResolution(width, height, additionalBinaryDataProperties: null);
        }

        /// <summary> Configuration for input audio transcription. </summary>
        /// <param name="model"> The model used for transcription. E.g., 'whisper-1', 'azure-fast-transcription', 's2s-ingraph'. </param>
        /// <param name="language"> The language code to use for transcription, if specified. </param>
        /// <param name="enabled"> Whether transcription is enabled. </param>
        /// <param name="customModel"> Whether a custom model is being used. </param>
        /// <returns> A new <see cref="VoiceLive.AudioInputTranscriptionSettings"/> instance for mocking. </returns>
        public static AudioInputTranscriptionSettings AudioInputTranscriptionSettings(AudioInputTranscriptionSettingsModel model = default, string language = default, bool enabled = default, bool customModel = default)
        {
            return new AudioInputTranscriptionSettings(model, language, enabled, customModel, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// The base representation of a voicelive tool definition.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="FunctionTool"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <returns> A new <see cref="VoiceLive.ToolCall"/> instance for mocking. </returns>
        public static ToolCall ToolCall(string @type = default)
        {
            return new UnknownToolCall(new ToolType(@type), additionalBinaryDataProperties: null);
        }

        /// <summary> The definition of a function tool as used by the voicelive endpoint. </summary>
        /// <param name="name"></param>
        /// <param name="description"></param>
        /// <param name="parameters"></param>
        /// <returns> A new <see cref="VoiceLive.FunctionTool"/> instance for mocking. </returns>
        public static FunctionTool FunctionTool(string name = default, string description = default, BinaryData parameters = default)
        {
            return new FunctionTool(ToolType.Function, additionalBinaryDataProperties: null, name, description, parameters);
        }

        /// <summary>
        /// A base representation for a voicelive tool_choice selecting a named tool.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="ToolChoiceFunctionObject"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <returns> A new <see cref="VoiceLive.ToolChoiceObject"/> instance for mocking. </returns>
        public static ToolChoiceObject ToolChoiceObject(string @type = default)
        {
            return new UnknownToolChoiceObject(new ToolType(@type), additionalBinaryDataProperties: null);
        }

        /// <summary> The representation of a voicelive tool_choice selecting a named function tool. </summary>
        /// <param name="function"></param>
        /// <returns> A new <see cref="VoiceLive.ToolChoiceFunctionObject"/> instance for mocking. </returns>
        public static ToolChoiceFunctionObject ToolChoiceFunctionObject(ToolChoiceFunctionObjectFunction function = default)
        {
            return new ToolChoiceFunctionObject(ToolType.Function, additionalBinaryDataProperties: null, function);
        }

        /// <summary> The ToolChoiceFunctionObjectFunction. </summary>
        /// <param name="name"></param>
        /// <returns> A new <see cref="VoiceLive.ToolChoiceFunctionObjectFunction"/> instance for mocking. </returns>
        public static ToolChoiceFunctionObjectFunction ToolChoiceFunctionObjectFunction(string name = default)
        {
            return new ToolChoiceFunctionObjectFunction(name, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// A voicelive client event.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="ClientEventSessionUpdate"/>, <see cref="ClientEventInputAudioBufferAppend"/>, <see cref="ClientEventInputAudioBufferCommit"/>, <see cref="ClientEventInputAudioBufferClear"/>, <see cref="ClientEventInputAudioTurnStart"/>, <see cref="ClientEventInputAudioTurnAppend"/>, <see cref="ClientEventInputAudioTurnEnd"/>, <see cref="ClientEventInputAudioTurnCancel"/>, <see cref="ClientEventInputAudioClear"/>, <see cref="ClientEventConversationItemCreate"/>, <see cref="ClientEventConversationItemRetrieve"/>, <see cref="ClientEventConversationItemTruncate"/>, <see cref="ClientEventConversationItemDelete"/>, <see cref="ClientEventResponseCreate"/>, <see cref="ClientEventResponseCancel"/>, and <see cref="ClientEventSessionAvatarConnect"/>.
        /// </summary>
        /// <param name="type"> The type of event. </param>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ClientEvent"/> instance for mocking. </returns>
        public static ClientEvent ClientEvent(string @type = default, string eventId = default)
        {
            return new UnknownClientEvent(@type, eventId, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Send this event to append audio bytes to the input audio buffer. The audio
        /// buffer is temporary storage you can write to and later commit. In Server VAD
        /// mode, the audio buffer is used to detect speech and the server will decide
        /// when to commit. When Server VAD is disabled, you must commit the audio buffer
        /// manually.
        /// 
        /// The client may choose how much audio to place in each event up to a maximum
        /// of 15 MiB, for example streaming smaller chunks from the client may allow the
        /// VAD to be more responsive. Unlike made other client events, the server will
        /// not send a confirmation response to this event.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="audio">
        /// Base64-encoded audio. This must be in the format specified by the
        /// `input_audio_format` field in the session configuration.
        /// </param>
        /// <returns> A new <see cref="VoiceLive.ClientEventInputAudioBufferAppend"/> instance for mocking. </returns>
        public static ClientEventInputAudioBufferAppend ClientEventInputAudioBufferAppend(string eventId = default, string audio = default)
        {
            return new ClientEventInputAudioBufferAppend("input_audio_buffer.append", eventId, additionalBinaryDataProperties: null, audio);
        }

        /// <summary>
        /// Send this event to commit the user input audio buffer, which will create a
        /// new user message item in the conversation. This event will produce an error
        /// if the input audio buffer is empty. When in Server VAD mode, the client does
        /// not need to send this event, the server will commit the audio buffer
        /// automatically.
        /// 
        /// Committing the input audio buffer will trigger input audio transcription
        /// (if enabled in session configuration), but it will not create a response
        /// from the model. The server will respond with an `input_audio_buffer.committed`
        /// event.
        /// </summary>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ClientEventInputAudioBufferCommit"/> instance for mocking. </returns>
        public static ClientEventInputAudioBufferCommit ClientEventInputAudioBufferCommit(string eventId = default)
        {
            return new ClientEventInputAudioBufferCommit("input_audio_buffer.commit", eventId, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Send this event to clear the audio bytes in the buffer. The server will
        /// respond with an `input_audio_buffer.cleared` event.
        /// </summary>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ClientEventInputAudioBufferClear"/> instance for mocking. </returns>
        public static ClientEventInputAudioBufferClear ClientEventInputAudioBufferClear(string eventId = default)
        {
            return new ClientEventInputAudioBufferClear("input_audio_buffer.clear", eventId, additionalBinaryDataProperties: null);
        }

        /// <summary>   Indicates the start of a new audio input turn. </summary>
        /// <param name="eventId"></param>
        /// <param name="turnId"> Unique identifier for the input audio turn. </param>
        /// <returns> A new <see cref="VoiceLive.ClientEventInputAudioTurnStart"/> instance for mocking. </returns>
        public static ClientEventInputAudioTurnStart ClientEventInputAudioTurnStart(string eventId = default, string turnId = default)
        {
            return new ClientEventInputAudioTurnStart("input_audio.turn.start", eventId, additionalBinaryDataProperties: null, turnId);
        }

        /// <summary>   Appends audio data to an ongoing input turn. </summary>
        /// <param name="eventId"></param>
        /// <param name="turnId"> The ID of the turn this audio is part of. </param>
        /// <param name="audio"> Base64-encoded audio chunk. </param>
        /// <returns> A new <see cref="VoiceLive.ClientEventInputAudioTurnAppend"/> instance for mocking. </returns>
        public static ClientEventInputAudioTurnAppend ClientEventInputAudioTurnAppend(string eventId = default, string turnId = default, string audio = default)
        {
            return new ClientEventInputAudioTurnAppend("input_audio.turn.append", eventId, additionalBinaryDataProperties: null, turnId, audio);
        }

        /// <summary>   Marks the end of an audio input turn. </summary>
        /// <param name="eventId"></param>
        /// <param name="turnId"> The ID of the audio turn being ended. </param>
        /// <returns> A new <see cref="VoiceLive.ClientEventInputAudioTurnEnd"/> instance for mocking. </returns>
        public static ClientEventInputAudioTurnEnd ClientEventInputAudioTurnEnd(string eventId = default, string turnId = default)
        {
            return new ClientEventInputAudioTurnEnd("input_audio.turn.end", eventId, additionalBinaryDataProperties: null, turnId);
        }

        /// <summary>   Cancels an in-progress input audio turn. </summary>
        /// <param name="eventId"></param>
        /// <param name="turnId"> The ID of the turn to cancel. </param>
        /// <returns> A new <see cref="VoiceLive.ClientEventInputAudioTurnCancel"/> instance for mocking. </returns>
        public static ClientEventInputAudioTurnCancel ClientEventInputAudioTurnCancel(string eventId = default, string turnId = default)
        {
            return new ClientEventInputAudioTurnCancel("input_audio.turn.cancel", eventId, additionalBinaryDataProperties: null, turnId);
        }

        /// <summary>   Clears all input audio currently being streamed. </summary>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ClientEventInputAudioClear"/> instance for mocking. </returns>
        public static ClientEventInputAudioClear ClientEventInputAudioClear(string eventId = default)
        {
            return new ClientEventInputAudioClear("input_audio.clear", eventId, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Add a new Item to the Conversation's context, including messages, function
        /// calls, and function call responses. This event can be used both to populate a
        /// "history" of the conversation and to add new items mid-stream, but has the
        /// current limitation that it cannot populate assistant audio messages.
        /// 
        /// If successful, the server will respond with a `conversation.item.created`
        /// event, otherwise an `error` event will be sent.
        /// </summary>
        /// <param name="eventId"> Optional client-generated ID used to identify this event. </param>
        /// <param name="previousItemId">
        /// The ID of the preceding item after which the new item will be inserted.
        /// If not set, the new item will be appended to the end of the conversation.
        /// If set to `root`, the new item will be added to the beginning of the conversation.
        /// If set to an existing ID, it allows an item to be inserted mid-conversation. If the
        /// ID cannot be found, an error will be returned and the item will not be added.
        /// </param>
        /// <param name="item"></param>
        /// <returns> A new <see cref="VoiceLive.ClientEventConversationItemCreate"/> instance for mocking. </returns>
        public static ClientEventConversationItemCreate ClientEventConversationItemCreate(string eventId = default, string previousItemId = default, ConversationItemWithReference item = default)
        {
            return new ClientEventConversationItemCreate("conversation.item.create", additionalBinaryDataProperties: null, eventId, previousItemId, item);
        }

        /// <summary> The item to add to the conversation. </summary>
        /// <param name="id">
        /// For an item of type (`message` | `function_call` | `function_call_output`)
        /// this field allows the client to assign the unique ID of the item. It is
        /// not required because the server will generate one if not provided.
        /// 
        /// For an item of type `item_reference`, this field is required and is a
        /// reference to any item that has previously existed in the conversation.
        /// </param>
        /// <param name="type"> The type of the item (`message`, `function_call`, `function_call_output`, `item_reference`). </param>
        /// <param name="object"> Identifier for the API object being returned - always `realtime.item`. </param>
        /// <param name="status">
        /// The status of the item (`completed`, `incomplete`). These have no effect
        /// on the conversation, but are accepted for consistency with the
        /// `conversation.item.created` event.
        /// </param>
        /// <param name="role">
        /// The role of the message sender (`user`, `assistant`, `system`), only
        /// applicable for `message` items.
        /// </param>
        /// <param name="content">
        /// The content of the message, applicable for `message` items.
        /// - Message items of role `system` support only `input_text` content
        /// - Message items of role `user` support `input_text` and `input_audio`
        ///   content
        /// - Message items of role `assistant` support `text` content.
        /// </param>
        /// <param name="callId">
        /// The ID of the function call (for `function_call` and
        /// `function_call_output` items). If passed on a `function_call_output`
        /// item, the server will check that a `function_call` item with the same
        /// ID exists in the conversation history.
        /// </param>
        /// <param name="name"> The name of the function being called (for `function_call` items). </param>
        /// <param name="arguments"> The arguments of the function call (for `function_call` items). </param>
        /// <param name="output"> The output of the function call (for `function_call_output` items). </param>
        /// <returns> A new <see cref="VoiceLive.ConversationItemWithReference"/> instance for mocking. </returns>
        public static ConversationItemWithReference ConversationItemWithReference(string id = default, ConversationItemWithReferenceType? @type = default, string @object = default, ConversationItemWithReferenceStatus? status = default, MessageRole? role = default, IEnumerable<ConversationItemWithReferenceContent> content = default, string callId = default, string name = default, string arguments = default, string output = default)
        {
            content ??= new ChangeTrackingList<ConversationItemWithReferenceContent>();

            return new ConversationItemWithReference(
                id,
                @type,
                @object,
                status,
                role,
                content.ToList(),
                callId,
                name,
                arguments,
                output,
                additionalBinaryDataProperties: null);
        }

        /// <summary> The ConversationItemWithReferenceContent. </summary>
        /// <param name="type"> The content type (`input_text`, `input_audio`, `item_reference`, `text`). </param>
        /// <param name="text"> The text content, used for `input_text` and `text` content types. </param>
        /// <param name="id">
        /// ID of a previous conversation item to reference (for `item_reference`
        /// content types in `response.create` events). These can reference both
        /// client and server created items.
        /// </param>
        /// <param name="audio"> Base64-encoded audio bytes, used for `input_audio` content type. </param>
        /// <param name="transcript"> The transcript of the audio, used for `input_audio` content type. </param>
        /// <returns> A new <see cref="VoiceLive.ConversationItemWithReferenceContent"/> instance for mocking. </returns>
        public static ConversationItemWithReferenceContent ConversationItemWithReferenceContent(ConversationItemWithReferenceContentType? @type = default, string text = default, string id = default, string audio = default, string transcript = default)
        {
            return new ConversationItemWithReferenceContent(
                @type,
                text,
                id,
                audio,
                transcript,
                additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Send this event when you want to retrieve the server's representation of a specific item in the conversation history. This is useful, for example, to inspect user audio after noise cancellation and VAD.
        /// The server will respond with a `conversation.item.retrieved` event,
        /// unless the item does not exist in the conversation history, in which case the
        /// server will respond with an error.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="itemId"> The ID of the item to retrieve. </param>
        /// <returns> A new <see cref="VoiceLive.ClientEventConversationItemRetrieve"/> instance for mocking. </returns>
        public static ClientEventConversationItemRetrieve ClientEventConversationItemRetrieve(string eventId = default, string itemId = default)
        {
            return new ClientEventConversationItemRetrieve("conversation.item.retrieve", eventId, additionalBinaryDataProperties: null, itemId);
        }

        /// <summary>
        /// Send this event to truncate a previous assistant message’s audio. The server
        /// will produce audio faster than voicelive, so this event is useful when the user
        /// interrupts to truncate audio that has already been sent to the client but not
        /// yet played. This will synchronize the server's understanding of the audio with
        /// the client's playback.
        /// 
        /// Truncating audio will delete the server-side text transcript to ensure there
        /// is not text in the context that hasn't been heard by the user.
        /// 
        /// If successful, the server will respond with a `conversation.item.truncated`
        /// event.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="itemId">
        /// The ID of the assistant message item to truncate. Only assistant message
        /// items can be truncated.
        /// </param>
        /// <param name="contentIndex"> The index of the content part to truncate. Set this to 0. </param>
        /// <param name="audioEndMs">
        /// Inclusive duration up to which audio is truncated, in milliseconds. If
        /// the audio_end_ms is greater than the actual audio duration, the server
        /// will respond with an error.
        /// </param>
        /// <returns> A new <see cref="VoiceLive.ClientEventConversationItemTruncate"/> instance for mocking. </returns>
        public static ClientEventConversationItemTruncate ClientEventConversationItemTruncate(string eventId = default, string itemId = default, int contentIndex = default, int audioEndMs = default)
        {
            return new ClientEventConversationItemTruncate(
                "conversation.item.truncate",
                eventId,
                additionalBinaryDataProperties: null,
                itemId,
                contentIndex,
                audioEndMs);
        }

        /// <summary>
        /// Send this event when you want to remove any item from the conversation
        /// history. The server will respond with a `conversation.item.deleted` event,
        /// unless the item does not exist in the conversation history, in which case the
        /// server will respond with an error.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="itemId"> The ID of the item to delete. </param>
        /// <returns> A new <see cref="VoiceLive.ClientEventConversationItemDelete"/> instance for mocking. </returns>
        public static ClientEventConversationItemDelete ClientEventConversationItemDelete(string eventId = default, string itemId = default)
        {
            return new ClientEventConversationItemDelete("conversation.item.delete", eventId, additionalBinaryDataProperties: null, itemId);
        }

        /// <summary>
        /// This event instructs the server to create a Response, which means triggering
        /// model inference. When in Server VAD mode, the server will create Responses
        /// automatically.
        /// 
        /// A Response will include at least one Item, and may have two, in which case
        /// the second will be a function call. These Items will be appended to the
        /// conversation history.
        /// 
        /// The server will respond with a `response.created` event, events for Items
        /// and content created, and finally a `response.done` event to indicate the
        /// Response is complete.
        /// 
        /// The `response.create` event includes inference configuration like
        /// `instructions`, and `temperature`. These fields will override the Session's
        /// configuration for this Response only.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="response"></param>
        /// <param name="additionalInstructions"> additional instructions (system prompt) appended to the default instructions of the session. Only affects this response only. </param>
        /// <returns> A new <see cref="VoiceLive.ClientEventResponseCreate"/> instance for mocking. </returns>
        public static ClientEventResponseCreate ClientEventResponseCreate(string eventId = default, ResponseCreateParams response = default, string additionalInstructions = default)
        {
            return new ClientEventResponseCreate("response.create", eventId, additionalBinaryDataProperties: null, response, additionalInstructions);
        }

        /// <summary> Create a new VoiceLive response with these parameters. </summary>
        /// <param name="commit"> Whether to commit the response to the conversation. Defaults to true. </param>
        /// <param name="cancelPrevious"> Whether to cancel any ongoing generation before starting this one. Defaults to true. </param>
        /// <param name="appendInputItems"> Input items to append to the conversation context before generating a response. </param>
        /// <param name="inputItems">
        /// Input items to be used as the context for this response.
        /// An empty array clears previous context.
        /// </param>
        /// <param name="modalities">
        /// The set of modalities the model can respond with. To disable audio,
        /// set this to ["text"].
        /// </param>
        /// <param name="instructions">
        /// The default system instructions (i.e. system message) prepended to model
        /// calls. This field allows the client to guide the model on desired
        /// responses. The model can be instructed on response content and format,
        /// (e.g. "be extremely succinct", "act friendly", "here are examples of good
        /// responses") and on audio behavior (e.g. "talk quickly", "inject emotion
        /// into your voice", "laugh frequently"). The instructions are not guaranteed
        /// to be followed by the model, but they provide guidance to the model on the
        /// desired behavior.
        /// 
        /// Note that the server sets default instructions which will be used if this
        /// field is not set and are visible in the `session.created` event at the
        /// start of the session.
        /// </param>
        /// <param name="voice"> supported voice identifiers and configurations. </param>
        /// <param name="outputAudioFormat"> The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. </param>
        /// <param name="tools"> Tools (functions) available to the model. </param>
        /// <param name="toolChoice">
        /// How the model chooses tools. Options are `auto`, `none`, `required`, or
        /// specify a function, like `{"type": "function", "function": {"name": "my_function"}}`.
        /// </param>
        /// <param name="temperature"> Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8. </param>
        /// <param name="maxOutputTokens">
        /// Maximum number of output tokens for a single assistant response,
        /// inclusive of tool calls. Provide an integer between 1 and 4096 to
        /// limit output tokens, or `inf` for the maximum available tokens for a
        /// given model. Defaults to `inf`.
        /// </param>
        /// <returns> A new <see cref="VoiceLive.ResponseCreateParams"/> instance for mocking. </returns>
        public static ResponseCreateParams ResponseCreateParams(bool? commit = default, bool? cancelPrevious = default, IEnumerable<ConversationRequestItem> appendInputItems = default, IEnumerable<ConversationRequestItem> inputItems = default, IEnumerable<InputModality> modalities = default, string instructions = default, BinaryData voice = default, AudioFormat? outputAudioFormat = default, IEnumerable<ToolCall> tools = default, string toolChoice = default, float? temperature = default, BinaryData maxOutputTokens = default)
        {
            appendInputItems ??= new ChangeTrackingList<ConversationRequestItem>();
            inputItems ??= new ChangeTrackingList<ConversationRequestItem>();
            modalities ??= new ChangeTrackingList<InputModality>();
            tools ??= new ChangeTrackingList<ToolCall>();

            return new ResponseCreateParams(
                commit,
                cancelPrevious,
                appendInputItems.ToList(),
                inputItems.ToList(),
                modalities.ToList(),
                instructions,
                voice,
                outputAudioFormat,
                tools.ToList(),
                toolChoice,
                temperature,
                maxOutputTokens,
                additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// The ConversationRequestItem.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="RequestMessageItem"/>, <see cref="RequestFunctionCallItem"/>, and <see cref="RequestFunctionCallOutputItem"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <param name="id"></param>
        /// <returns> A new <see cref="VoiceLive.ConversationRequestItem"/> instance for mocking. </returns>
        public static ConversationRequestItem ConversationRequestItem(string @type = default, string id = default)
        {
            return new UnknownConversationRequestItem(@type, id, additionalBinaryDataProperties: null);
        }

        /// <summary> The RequestMessageItem. </summary>
        /// <param name="id"></param>
        /// <param name="status"></param>
        /// <returns> A new <see cref="VoiceLive.RequestMessageItem"/> instance for mocking. </returns>
        public static RequestMessageItem RequestMessageItem(string id = default, ItemStatus? status = default)
        {
            return new RequestMessageItem("message", id, additionalBinaryDataProperties: null, "message", status);
        }

        /// <summary> The RequestSystemMessageItem. </summary>
        /// <param name="id"></param>
        /// <param name="status"></param>
        /// <param name="content"></param>
        /// <returns> A new <see cref="VoiceLive.RequestSystemMessageItem"/> instance for mocking. </returns>
        public static RequestSystemMessageItem RequestSystemMessageItem(string id = default, ItemStatus? status = default, IEnumerable<RequestTextContentPart> content = default)
        {
            content ??= new ChangeTrackingList<RequestTextContentPart>();

            return new RequestSystemMessageItem(
                "system",
                id,
                additionalBinaryDataProperties: null,
                "system",
                status,
                content.ToList());
        }

        /// <summary> The RequestTextContentPart. </summary>
        /// <param name="text"></param>
        /// <returns> A new <see cref="VoiceLive.RequestTextContentPart"/> instance for mocking. </returns>
        public static RequestTextContentPart RequestTextContentPart(string text = default)
        {
            return new RequestTextContentPart(ContentPartType.InputText, additionalBinaryDataProperties: null, text);
        }

        /// <summary>
        /// The ContentPart.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="RequestTextContentPart"/>, <see cref="RequestAudioContentPart"/>, <see cref="ResponseTextContentPart"/>, and <see cref="ResponseAudioContentPart"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <returns> A new <see cref="VoiceLive.ContentPart"/> instance for mocking. </returns>
        public static ContentPart ContentPart(string @type = default)
        {
            return new UnknownContentPart(new ContentPartType(@type), additionalBinaryDataProperties: null);
        }

        /// <summary> The RequestAudioContentPart. </summary>
        /// <param name="transcript"></param>
        /// <returns> A new <see cref="VoiceLive.RequestAudioContentPart"/> instance for mocking. </returns>
        public static RequestAudioContentPart RequestAudioContentPart(string transcript = default)
        {
            return new RequestAudioContentPart(ContentPartType.InputAudio, additionalBinaryDataProperties: null, transcript);
        }

        /// <summary> The ResponseTextContentPart. </summary>
        /// <param name="text"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseTextContentPart"/> instance for mocking. </returns>
        public static ResponseTextContentPart ResponseTextContentPart(string text = default)
        {
            return new ResponseTextContentPart(ContentPartType.Text, additionalBinaryDataProperties: null, text);
        }

        /// <summary> The ResponseAudioContentPart. </summary>
        /// <param name="transcript"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseAudioContentPart"/> instance for mocking. </returns>
        public static ResponseAudioContentPart ResponseAudioContentPart(string transcript = default)
        {
            return new ResponseAudioContentPart(ContentPartType.Audio, additionalBinaryDataProperties: null, transcript);
        }

        /// <summary> The RequestUserMessageItem. </summary>
        /// <param name="id"></param>
        /// <param name="status"></param>
        /// <param name="content"></param>
        /// <returns> A new <see cref="VoiceLive.RequestUserMessageItem"/> instance for mocking. </returns>
        public static RequestUserMessageItem RequestUserMessageItem(string id = default, ItemStatus? status = default, IEnumerable<BinaryData> content = default)
        {
            content ??= new ChangeTrackingList<BinaryData>();

            return new RequestUserMessageItem(
                "user",
                id,
                additionalBinaryDataProperties: null,
                "user",
                status,
                content.ToList());
        }

        /// <summary> The RequestAssistantMessageItem. </summary>
        /// <param name="id"></param>
        /// <param name="status"></param>
        /// <param name="content"></param>
        /// <returns> A new <see cref="VoiceLive.RequestAssistantMessageItem"/> instance for mocking. </returns>
        public static RequestAssistantMessageItem RequestAssistantMessageItem(string id = default, ItemStatus? status = default, IEnumerable<RequestTextContentPart> content = default)
        {
            content ??= new ChangeTrackingList<RequestTextContentPart>();

            return new RequestAssistantMessageItem(
                "assistant",
                id,
                additionalBinaryDataProperties: null,
                "assistant",
                status,
                content.ToList());
        }

        /// <summary> The RequestFunctionCallItem. </summary>
        /// <param name="id"></param>
        /// <param name="name"></param>
        /// <param name="callId"></param>
        /// <param name="arguments"></param>
        /// <param name="status"></param>
        /// <returns> A new <see cref="VoiceLive.RequestFunctionCallItem"/> instance for mocking. </returns>
        public static RequestFunctionCallItem RequestFunctionCallItem(string id = default, string name = default, string callId = default, string arguments = default, ItemStatus? status = default)
        {
            return new RequestFunctionCallItem(
                "function_call",
                id,
                additionalBinaryDataProperties: null,
                name,
                callId,
                arguments,
                status);
        }

        /// <summary> The RequestFunctionCallOutputItem. </summary>
        /// <param name="id"></param>
        /// <param name="callId"></param>
        /// <param name="output"></param>
        /// <returns> A new <see cref="VoiceLive.RequestFunctionCallOutputItem"/> instance for mocking. </returns>
        public static RequestFunctionCallOutputItem RequestFunctionCallOutputItem(string id = default, string callId = default, string output = default)
        {
            return new RequestFunctionCallOutputItem("function_call_output", id, additionalBinaryDataProperties: null, callId, output);
        }

        /// <summary>
        /// Send this event to cancel an in-progress response. The server will respond
        /// with a `response.cancelled` event or an error if there is no response to
        /// cancel.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId">
        /// A specific response ID to cancel - if not provided, will cancel an
        /// in-progress response in the default conversation.
        /// </param>
        /// <returns> A new <see cref="VoiceLive.ClientEventResponseCancel"/> instance for mocking. </returns>
        public static ClientEventResponseCancel ClientEventResponseCancel(string eventId = default, string responseId = default)
        {
            return new ClientEventResponseCancel("response.cancel", eventId, additionalBinaryDataProperties: null, responseId);
        }

        /// <summary>
        ///   Sent when the client connects and provides its SDP (Session Description Protocol)
        ///   for avatar-related media negotiation.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="clientSdp"> The client's SDP offer. </param>
        /// <returns> A new <see cref="VoiceLive.ClientEventSessionAvatarConnect"/> instance for mocking. </returns>
        public static ClientEventSessionAvatarConnect ClientEventSessionAvatarConnect(string eventId = default, string clientSdp = default)
        {
            return new ClientEventSessionAvatarConnect("session.avatar.connect", eventId, additionalBinaryDataProperties: null, clientSdp);
        }

        /// <summary> Sent when the server is in the process of establishing an avatar media connection and provides its SDP answer. </summary>
        /// <param name="eventId"></param>
        /// <param name="serverSdp"> The server's SDP answer for the avatar connection. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventSessionAvatarConnecting"/> instance for mocking. </returns>
        public static ServerEventSessionAvatarConnecting ServerEventSessionAvatarConnecting(string eventId = default, string serverSdp = default)
        {
            return new ServerEventSessionAvatarConnecting(ServerEventType.SessionAvatarConnecting, eventId, additionalBinaryDataProperties: null, serverSdp);
        }

        /// <summary>
        /// A voicelive server event.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="ServerEventSessionAvatarConnecting"/>, <see cref="ServerEventSessionCreated"/>, <see cref="ServerEventSessionUpdated"/>, <see cref="ServerEventError"/>, <see cref="ServerEventResponseTextDelta"/>, <see cref="ServerEventResponseAudioDelta"/>, <see cref="ServerEventConversationItemCreated"/>, <see cref="ServerEventConversationItemDeleted"/>, <see cref="ServerEventConversationItemRetrieved"/>, <see cref="ServerEventConversationItemTruncated"/>, <see cref="ServerEventConversationItemInputAudioTranscriptionCompleted"/>, <see cref="ServerEventConversationItemInputAudioTranscriptionDelta"/>, <see cref="ServerEventConversationItemInputAudioTranscriptionFailed"/>, <see cref="ServerEventInputAudioBufferCommitted"/>, <see cref="ServerEventInputAudioBufferCleared"/>, <see cref="ServerEventInputAudioBufferSpeechStarted"/>, <see cref="ServerEventInputAudioBufferSpeechStopped"/>, <see cref="ServerEventResponseCreated"/>, <see cref="ServerEventResponseDone"/>, <see cref="ServerEventResponseOutputItemAdded"/>, <see cref="ServerEventResponseOutputItemDone"/>, <see cref="ServerEventResponseContentPartAdded"/>, <see cref="ServerEventResponseContentPartDone"/>, <see cref="ServerEventResponseTextDone"/>, <see cref="ServerEventResponseAudioTranscriptDelta"/>, <see cref="ServerEventResponseAudioTranscriptDone"/>, <see cref="ServerEventResponseAudioDone"/>, <see cref="ServerEventResponseFunctionCallArgumentsDelta"/>, <see cref="ServerEventResponseFunctionCallArgumentsDone"/>, <see cref="ResponseAnimationBlendshapeDeltaEvent"/>, <see cref="ResponseAnimationBlendshapeDoneEvent"/>, <see cref="ResponseEmotionHypothesis"/>, <see cref="ResponseAudioTimestampDeltaEvent"/>, <see cref="ResponseAudioTimestampDoneEvent"/>, <see cref="ResponseAnimationVisemeDeltaEvent"/>, and <see cref="ResponseAnimationVisemeDoneEvent"/>.
        /// </summary>
        /// <param name="type"> The type of event. </param>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEvent"/> instance for mocking. </returns>
        public static ServerEvent ServerEvent(string @type = default, string eventId = default)
        {
            return new UnknownServerEvent(new ServerEventType(@type), eventId, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Returned when a Session is created. Emitted automatically when a new
        /// connection is established as the first server event. This event will contain
        /// the default Session configuration.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="session"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventSessionCreated"/> instance for mocking. </returns>
        public static ServerEventSessionCreated ServerEventSessionCreated(string eventId = default, ResponseSession session = default)
        {
            return new ServerEventSessionCreated(ServerEventType.SessionCreated, eventId, additionalBinaryDataProperties: null, session);
        }

        /// <summary> The ResponseSession. </summary>
        /// <param name="id"></param>
        /// <param name="model"></param>
        /// <param name="modalities"></param>
        /// <param name="instructions"></param>
        /// <param name="animation"></param>
        /// <param name="voice"></param>
        /// <param name="inputAudio"></param>
        /// <param name="inputAudioFormat"></param>
        /// <param name="outputAudioFormat"></param>
        /// <param name="inputAudioSamplingRate"></param>
        /// <param name="turnDetection"></param>
        /// <param name="inputAudioNoiseReduction"></param>
        /// <param name="inputAudioEchoCancellation"></param>
        /// <param name="avatar"></param>
        /// <param name="inputAudioTranscription"></param>
        /// <param name="outputAudioTimestampTypes"></param>
        /// <param name="tools"></param>
        /// <param name="toolChoice"></param>
        /// <param name="temperature"></param>
        /// <param name="maxResponseOutputTokens"></param>
        /// <param name="agent"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseSession"/> instance for mocking. </returns>
        public static ResponseSession ResponseSession(string id = default, string model = default, IEnumerable<InputModality> modalities = default, string instructions = default, AnimationOptions animation = default, BinaryData voice = default, InputAudio inputAudio = default, AudioFormat? inputAudioFormat = default, AudioFormat? outputAudioFormat = default, int? inputAudioSamplingRate = default, TurnDetection turnDetection = default, AudioNoiseReduction inputAudioNoiseReduction = default, AudioEchoCancellation inputAudioEchoCancellation = default, AvatarConfig avatar = default, AudioInputTranscriptionSettings inputAudioTranscription = default, IEnumerable<AudioTimestampType> outputAudioTimestampTypes = default, IEnumerable<ToolCall> tools = default, BinaryData toolChoice = default, float? temperature = default, BinaryData maxResponseOutputTokens = default, AgentConfig agent = default)
        {
            modalities ??= new ChangeTrackingList<InputModality>();
            outputAudioTimestampTypes ??= new ChangeTrackingList<AudioTimestampType>();
            tools ??= new ChangeTrackingList<ToolCall>();

            return new ResponseSession(
                id,
                model,
                modalities.ToList(),
                instructions,
                animation,
                voice,
                inputAudio,
                inputAudioFormat,
                outputAudioFormat,
                inputAudioSamplingRate,
                turnDetection,
                inputAudioNoiseReduction,
                inputAudioEchoCancellation,
                avatar,
                inputAudioTranscription,
                outputAudioTimestampTypes.ToList(),
                tools.ToList(),
                toolChoice,
                temperature,
                maxResponseOutputTokens,
                agent,
                additionalBinaryDataProperties: null);
        }

        /// <summary> The AgentConfig. </summary>
        /// <param name="type"></param>
        /// <param name="name"></param>
        /// <param name="description"></param>
        /// <param name="agentId"></param>
        /// <param name="threadId"></param>
        /// <returns> A new <see cref="VoiceLive.AgentConfig"/> instance for mocking. </returns>
        public static AgentConfig AgentConfig(string @type = default, string name = default, string description = default, string agentId = default, string threadId = default)
        {
            return new AgentConfig(
                @type,
                name,
                description,
                agentId,
                threadId,
                additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Returned when a session is updated with a `session.update` event, unless
        /// there is an error.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="session"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventSessionUpdated"/> instance for mocking. </returns>
        public static ServerEventSessionUpdated ServerEventSessionUpdated(string eventId = default, ResponseSession session = default)
        {
            return new ServerEventSessionUpdated(ServerEventType.SessionUpdated, eventId, additionalBinaryDataProperties: null, session);
        }

        /// <summary>
        /// Returned when an error occurs, which could be a client problem or a server
        /// problem. Most errors are recoverable and the session will stay open, we
        /// recommend to implementors to monitor and log error messages by default.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="error"> Details of the error. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventError"/> instance for mocking. </returns>
        public static ServerEventError ServerEventError(string eventId = default, ServerEventErrorError error = default)
        {
            return new ServerEventError(ServerEventType.Error, eventId, additionalBinaryDataProperties: null, error);
        }

        /// <summary> The ServerEventErrorError. </summary>
        /// <param name="type"> The type of error (e.g., "invalid_request_error", "server_error"). </param>
        /// <param name="code"> Error code, if any. </param>
        /// <param name="message"> A human-readable error message. </param>
        /// <param name="param"> Parameter related to the error, if any. </param>
        /// <param name="eventId"> The event_id of the client event that caused the error, if applicable. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventErrorError"/> instance for mocking. </returns>
        public static ServerEventErrorError ServerEventErrorError(string @type = default, string code = default, string message = default, string @param = default, string eventId = default)
        {
            return new ServerEventErrorError(
                @type,
                code,
                message,
                @param,
                eventId,
                additionalBinaryDataProperties: null);
        }

        /// <summary> Returned when the text value of a "text" content part is updated. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="delta"> The text delta. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseTextDelta"/> instance for mocking. </returns>
        public static ServerEventResponseTextDelta ServerEventResponseTextDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, string delta = default)
        {
            return new ServerEventResponseTextDelta(
                ServerEventType.ResponseTextDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                delta);
        }

        /// <summary> Returned when the model-generated audio is updated. </summary>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="delta"> Base64-encoded audio data delta. </param>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseAudioDelta"/> instance for mocking. </returns>
        public static ServerEventResponseAudioDelta ServerEventResponseAudioDelta(string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, BinaryData delta = default, string eventId = default)
        {
            return new ServerEventResponseAudioDelta(
                ServerEventType.ResponseAudioDelta,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                delta,
                eventId);
        }

        /// <summary>
        /// Returned when a conversation item is created. There are several scenarios that produce this event:
        ///   - The server is generating a Response, which if successful will produce
        ///     either one or two Items, which will be of type `message`
        ///     (role `assistant`) or type `function_call`.
        ///   - The input audio buffer has been committed, either by the client or the
        ///     server (in `server_vad` mode). The server will take the content of the
        ///     input audio buffer and add it to a new user message Item.
        ///   - The client has sent a `conversation.item.create` event to add a new Item
        ///     to the Conversation.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="previousItemId">
        /// The ID of the preceding item in the Conversation context, allows the
        /// client to understand the order of the conversation.
        /// </param>
        /// <param name="item"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventConversationItemCreated"/> instance for mocking. </returns>
        public static ServerEventConversationItemCreated ServerEventConversationItemCreated(string eventId = default, string previousItemId = default, ConversationItemWithReference item = default)
        {
            return new ServerEventConversationItemCreated(ServerEventType.ConversationItemCreated, eventId, additionalBinaryDataProperties: null, previousItemId, item);
        }

        /// <summary>
        /// Returned when an item in the conversation is deleted by the client with a
        /// `conversation.item.delete` event. This event is used to synchronize the
        /// server's understanding of the conversation history with the client's view.
        /// </summary>
        /// <param name="itemId"> The ID of the item that was deleted. </param>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventConversationItemDeleted"/> instance for mocking. </returns>
        public static ServerEventConversationItemDeleted ServerEventConversationItemDeleted(string itemId = default, string eventId = default)
        {
            return new ServerEventConversationItemDeleted(ServerEventType.ConversationItemDeleted, additionalBinaryDataProperties: null, itemId, eventId);
        }

        /// <summary> Returned when a conversation item is retrieved with `conversation.item.retrieve`. </summary>
        /// <param name="itemId"></param>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventConversationItemRetrieved"/> instance for mocking. </returns>
        public static ServerEventConversationItemRetrieved ServerEventConversationItemRetrieved(string itemId = default, string eventId = default)
        {
            return new ServerEventConversationItemRetrieved(ServerEventType.ConversationItemRetrieved, additionalBinaryDataProperties: null, itemId, eventId);
        }

        /// <summary>
        /// Returned when an earlier assistant audio message item is truncated by the
        /// client with a `conversation.item.truncate` event. This event is used to
        /// synchronize the server's understanding of the audio with the client's playback.
        /// 
        /// This action will truncate the audio and remove the server-side text transcript
        /// to ensure there is no text in the context that hasn't been heard by the user.
        /// </summary>
        /// <param name="itemId"> The ID of the assistant message item that was truncated. </param>
        /// <param name="contentIndex"> The index of the content part that was truncated. </param>
        /// <param name="audioEndMs"> The duration up to which the audio was truncated, in milliseconds. </param>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventConversationItemTruncated"/> instance for mocking. </returns>
        public static ServerEventConversationItemTruncated ServerEventConversationItemTruncated(string itemId = default, int contentIndex = default, int audioEndMs = default, string eventId = default)
        {
            return new ServerEventConversationItemTruncated(
                ServerEventType.ConversationItemTruncated,
                additionalBinaryDataProperties: null,
                itemId,
                contentIndex,
                audioEndMs,
                eventId);
        }

        /// <summary>
        /// This event is the output of audio transcription for user audio written to the
        /// user audio buffer. Transcription begins when the input audio buffer is
        /// committed by the client or server (in `server_vad` mode). Transcription runs
        /// asynchronously with Response creation, so this event may come before or after
        /// the Response events.
        /// 
        /// VoiceLive API models accept audio natively, and thus input transcription is a
        /// separate process run on a separate ASR (Automatic Speech Recognition) model.
        /// The transcript may diverge somewhat from the model's interpretation, and
        /// should be treated as a rough guide.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="itemId"> The ID of the user message item containing the audio. </param>
        /// <param name="contentIndex"> The index of the content part containing the audio. </param>
        /// <param name="transcript"> The transcribed text. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventConversationItemInputAudioTranscriptionCompleted"/> instance for mocking. </returns>
        public static ServerEventConversationItemInputAudioTranscriptionCompleted ServerEventConversationItemInputAudioTranscriptionCompleted(string eventId = default, string itemId = default, int contentIndex = default, string transcript = default)
        {
            return new ServerEventConversationItemInputAudioTranscriptionCompleted(
                ServerEventType.ConversationItemInputAudioTranscriptionCompleted,
                eventId,
                additionalBinaryDataProperties: null,
                itemId,
                contentIndex,
                transcript);
        }

        /// <summary> Returned when the text value of an input audio transcription content part is updated. </summary>
        /// <param name="eventId"></param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="delta"> The text delta. </param>
        /// <param name="logprobs"> The log probabilities of the transcription. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventConversationItemInputAudioTranscriptionDelta"/> instance for mocking. </returns>
        public static ServerEventConversationItemInputAudioTranscriptionDelta ServerEventConversationItemInputAudioTranscriptionDelta(string eventId = default, string itemId = default, int? contentIndex = default, string delta = default, IEnumerable<LogProbProperties> logprobs = default)
        {
            logprobs ??= new ChangeTrackingList<LogProbProperties>();

            return new ServerEventConversationItemInputAudioTranscriptionDelta(
                ServerEventType.ConversationItemInputAudioTranscriptionDelta,
                eventId,
                additionalBinaryDataProperties: null,
                itemId,
                contentIndex,
                delta,
                logprobs.ToList());
        }

        /// <summary> A single log probability entry for a token. </summary>
        /// <param name="token"> The token that was used to generate the log probability. </param>
        /// <param name="logprob"> The log probability of the token. </param>
        /// <param name="bytes"> The bytes that were used to generate the log probability. </param>
        /// <returns> A new <see cref="VoiceLive.LogProbProperties"/> instance for mocking. </returns>
        public static LogProbProperties LogProbProperties(string token = default, float logprob = default, IEnumerable<int> bytes = default)
        {
            bytes ??= new ChangeTrackingList<int>();

            return new LogProbProperties(token, logprob, bytes.ToList(), additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Returned when input audio transcription is configured, and a transcription
        /// request for a user message failed. These events are separate from other
        /// `error` events so that the client can identify the related Item.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="itemId"> The ID of the user message item. </param>
        /// <param name="contentIndex"> The index of the content part containing the audio. </param>
        /// <param name="error"> Details of the transcription error. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventConversationItemInputAudioTranscriptionFailed"/> instance for mocking. </returns>
        public static ServerEventConversationItemInputAudioTranscriptionFailed ServerEventConversationItemInputAudioTranscriptionFailed(string eventId = default, string itemId = default, int contentIndex = default, VoiceLiveErrorDetails error = default)
        {
            return new ServerEventConversationItemInputAudioTranscriptionFailed(
                ServerEventType.ConversationItemInputAudioTranscriptionFailed,
                eventId,
                additionalBinaryDataProperties: null,
                itemId,
                contentIndex,
                error);
        }

        /// <summary> Error object returned in case of API failure. </summary>
        /// <param name="code"> Error code, or null if unspecified. </param>
        /// <param name="message"> Human-readable error message. </param>
        /// <param name="param"> Parameter name related to the error, if applicable. </param>
        /// <param name="type"> Type or category of the error. </param>
        /// <param name="eventId"> Event id of the error. </param>
        /// <returns> A new <see cref="VoiceLive.VoiceLiveErrorDetails"/> instance for mocking. </returns>
        public static VoiceLiveErrorDetails VoiceLiveErrorDetails(string code = default, string message = default, string @param = default, string @type = default, string eventId = default)
        {
            return new VoiceLiveErrorDetails(
                code,
                message,
                @param,
                @type,
                eventId,
                additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Returned when an input audio buffer is committed, either by the client or
        /// automatically in server VAD mode. The `item_id` property is the ID of the user
        /// message item that will be created, thus a `conversation.item.created` event
        /// will also be sent to the client.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="previousItemId"> The ID of the preceding item after which the new item will be inserted. </param>
        /// <param name="itemId"> The ID of the user message item that will be created. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventInputAudioBufferCommitted"/> instance for mocking. </returns>
        public static ServerEventInputAudioBufferCommitted ServerEventInputAudioBufferCommitted(string eventId = default, string previousItemId = default, string itemId = default)
        {
            return new ServerEventInputAudioBufferCommitted(ServerEventType.InputAudioBufferCommitted, eventId, additionalBinaryDataProperties: null, previousItemId, itemId);
        }

        /// <summary>
        /// Returned when the input audio buffer is cleared by the client with a
        /// `input_audio_buffer.clear` event.
        /// </summary>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventInputAudioBufferCleared"/> instance for mocking. </returns>
        public static ServerEventInputAudioBufferCleared ServerEventInputAudioBufferCleared(string eventId = default)
        {
            return new ServerEventInputAudioBufferCleared(ServerEventType.InputAudioBufferCleared, eventId, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Sent by the server when in `server_vad` mode to indicate that speech has been
        /// detected in the audio buffer. This can happen any time audio is added to the
        /// buffer (unless speech is already detected). The client may want to use this
        /// event to interrupt audio playback or provide visual feedback to the user.
        /// 
        /// The client should expect to receive a `input_audio_buffer.speech_stopped` event
        /// when speech stops. The `item_id` property is the ID of the user message item
        /// that will be created when speech stops and will also be included in the
        /// `input_audio_buffer.speech_stopped` event (unless the client manually commits
        /// the audio buffer during VAD activation).
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="audioStartMs">
        /// Milliseconds from the start of all audio written to the buffer during the
        /// session when speech was first detected. This will correspond to the
        /// beginning of audio sent to the model, and thus includes the
        /// `prefix_padding_ms` configured in the Session.
        /// </param>
        /// <param name="itemId"> The ID of the user message item that will be created when speech stops. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventInputAudioBufferSpeechStarted"/> instance for mocking. </returns>
        public static ServerEventInputAudioBufferSpeechStarted ServerEventInputAudioBufferSpeechStarted(string eventId = default, int audioStartMs = default, string itemId = default)
        {
            return new ServerEventInputAudioBufferSpeechStarted(ServerEventType.InputAudioBufferSpeechStarted, eventId, additionalBinaryDataProperties: null, audioStartMs, itemId);
        }

        /// <summary>
        /// Returned in `server_vad` mode when the server detects the end of speech in
        /// the audio buffer. The server will also send an `conversation.item.created`
        /// event with the user message item that is created from the audio buffer.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="audioEndMs">
        /// Milliseconds since the session started when speech stopped. This will
        /// correspond to the end of audio sent to the model, and thus includes the
        /// `min_silence_duration_ms` configured in the Session.
        /// </param>
        /// <param name="itemId"> The ID of the user message item that will be created. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventInputAudioBufferSpeechStopped"/> instance for mocking. </returns>
        public static ServerEventInputAudioBufferSpeechStopped ServerEventInputAudioBufferSpeechStopped(string eventId = default, int audioEndMs = default, string itemId = default)
        {
            return new ServerEventInputAudioBufferSpeechStopped(ServerEventType.InputAudioBufferSpeechStopped, eventId, additionalBinaryDataProperties: null, audioEndMs, itemId);
        }

        /// <summary>
        /// Returned when a new Response is created. The first event of response creation,
        /// where the response is in an initial state of `in_progress`.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="response"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseCreated"/> instance for mocking. </returns>
        public static ServerEventResponseCreated ServerEventResponseCreated(string eventId = default, VoiceLiveResponse response = default)
        {
            return new ServerEventResponseCreated(ServerEventType.ResponseCreated, eventId, additionalBinaryDataProperties: null, response);
        }

        /// <summary> The response resource. </summary>
        /// <param name="id"> The unique ID of the response. </param>
        /// <param name="object"> The object type, must be `realtime.response`. </param>
        /// <param name="status">
        /// The final status of the response (`completed`, `cancelled`, `failed`, or
        /// `incomplete`).
        /// </param>
        /// <param name="statusDetails"> Additional details about the status. </param>
        /// <param name="output"> The list of output items generated by the response. </param>
        /// <param name="usage">
        /// Usage statistics for the Response, this will correspond to billing. A
        /// VoiceLive API session will maintain a conversation context and append new
        /// Items to the Conversation, thus output from previous turns (text and
        /// audio tokens) will become the input for later turns.
        /// </param>
        /// <param name="conversationId">
        /// Which conversation the response is added to, determined by the `conversation`
        /// field in the `response.create` event. If `auto`, the response will be added to
        /// the default conversation and the value of `conversation_id` will be an id like
        /// `conv_1234`. If `none`, the response will not be added to any conversation and
        /// the value of `conversation_id` will be `null`. If responses are being triggered
        /// by server VAD, the response will be added to the default conversation, thus
        /// the `conversation_id` will be an id like `conv_1234`.
        /// </param>
        /// <param name="voice"> supported voice identifiers and configurations. </param>
        /// <param name="modalities">
        /// The set of modalities the model used to respond. If there are multiple modalities,
        /// the model will pick one, for example if `modalities` is `["text", "audio"]`, the model
        /// could be responding in either text or audio.
        /// </param>
        /// <param name="outputAudioFormat"> The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. </param>
        /// <param name="temperature"> Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8. </param>
        /// <param name="maxOutputTokens">
        /// Maximum number of output tokens for a single assistant response,
        /// inclusive of tool calls, that was used in this response.
        /// </param>
        /// <returns> A new <see cref="VoiceLive.VoiceLiveResponse"/> instance for mocking. </returns>
        public static VoiceLiveResponse VoiceLiveResponse(string id = default, string @object = default, ItemStatus? status = default, ResponseStatusDetails statusDetails = default, IEnumerable<ConversationResponseItem> output = default, ResponseUsage usage = default, string conversationId = default, BinaryData voice = default, IEnumerable<ResponseModality> modalities = default, ResponseOutputAudioFormat? outputAudioFormat = default, float? temperature = default, BinaryData maxOutputTokens = default)
        {
            output ??= new ChangeTrackingList<ConversationResponseItem>();
            modalities ??= new ChangeTrackingList<ResponseModality>();

            return new VoiceLiveResponse(
                id,
                @object,
                status,
                statusDetails,
                output.ToList(),
                usage,
                conversationId,
                voice,
                modalities.ToList(),
                outputAudioFormat,
                temperature,
                maxOutputTokens,
                additionalBinaryDataProperties: null);
        }

        /// <summary> The ResponseStatusDetails. </summary>
        /// <param name="type">
        /// The type of error that caused the response to fail, corresponding
        /// with the `status` field (`completed`, `cancelled`, `incomplete`,
        /// `failed`).
        /// </param>
        /// <param name="reason">
        /// The reason the Response did not complete. For a `cancelled` Response,
        /// one of `turn_detected` (the server VAD detected a new start of speech)
        /// or `client_cancelled` (the client sent a cancel event). For an
        /// `incomplete` Response, one of `max_output_tokens` or `content_filter`
        /// (the server-side safety filter activated and cut off the response).
        /// </param>
        /// <param name="error">
        /// A description of the error that caused the response to fail,
        /// populated when the `status` is `failed`.
        /// </param>
        /// <returns> A new <see cref="VoiceLive.ResponseStatusDetails"/> instance for mocking. </returns>
        public static ResponseStatusDetails ResponseStatusDetails(ResponseStatusDetailsType? @type = default, ResponseStatusDetailsReason? reason = default, ResponseStatusDetailsError error = default)
        {
            return new ResponseStatusDetails(@type, reason, error, additionalBinaryDataProperties: null);
        }

        /// <summary> The ResponseStatusDetailsError. </summary>
        /// <param name="type"> The type of error. </param>
        /// <param name="code"> Error code, if any. </param>
        /// <returns> A new <see cref="VoiceLive.ResponseStatusDetailsError"/> instance for mocking. </returns>
        public static ResponseStatusDetailsError ResponseStatusDetailsError(string @type = default, string code = default)
        {
            return new ResponseStatusDetailsError(@type, code, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// The ConversationResponseItem.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="ResponseMessageItem"/>, <see cref="ResponseFunctionCallItem"/>, and <see cref="ResponseFunctionCallOutputItem"/>.
        /// </summary>
        /// <param name="object"></param>
        /// <param name="type"></param>
        /// <param name="id"></param>
        /// <returns> A new <see cref="VoiceLive.ConversationResponseItem"/> instance for mocking. </returns>
        public static ConversationResponseItem ConversationResponseItem(string @object = default, string @type = default, string id = default)
        {
            return new UnknownConversationResponseItem(@object, @type, id, additionalBinaryDataProperties: null);
        }

        /// <summary> The ResponseMessageItem. </summary>
        /// <param name="object"></param>
        /// <param name="id"></param>
        /// <param name="role"></param>
        /// <param name="content"></param>
        /// <param name="status"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseMessageItem"/> instance for mocking. </returns>
        public static ResponseMessageItem ResponseMessageItem(string @object = default, string id = default, MessageRole role = default, IEnumerable<ContentPart> content = default, ItemStatus status = default)
        {
            content ??= new ChangeTrackingList<ContentPart>();

            return new ResponseMessageItem(
                @object,
                "message",
                id,
                additionalBinaryDataProperties: null,
                role,
                content.ToList(),
                status);
        }

        /// <summary> The ResponseFunctionCallItem. </summary>
        /// <param name="object"></param>
        /// <param name="id"></param>
        /// <param name="name"></param>
        /// <param name="callId"></param>
        /// <param name="arguments"></param>
        /// <param name="status"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseFunctionCallItem"/> instance for mocking. </returns>
        public static ResponseFunctionCallItem ResponseFunctionCallItem(string @object = default, string id = default, string name = default, string callId = default, string arguments = default, ItemStatus status = default)
        {
            return new ResponseFunctionCallItem(
                @object,
                "function_call",
                id,
                additionalBinaryDataProperties: null,
                name,
                callId,
                arguments,
                status);
        }

        /// <summary> The ResponseFunctionCallOutputItem. </summary>
        /// <param name="object"></param>
        /// <param name="id"></param>
        /// <param name="callId"></param>
        /// <param name="output"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseFunctionCallOutputItem"/> instance for mocking. </returns>
        public static ResponseFunctionCallOutputItem ResponseFunctionCallOutputItem(string @object = default, string id = default, string callId = default, string output = default)
        {
            return new ResponseFunctionCallOutputItem(
                @object,
                "function_call_output",
                id,
                additionalBinaryDataProperties: null,
                callId,
                output);
        }

        /// <summary> The ResponseUsage. </summary>
        /// <param name="totalTokens">
        /// The total number of tokens in the Response including input and output
        /// text and audio tokens.
        /// </param>
        /// <param name="inputTokens">
        /// The number of input tokens used in the Response, including text and
        /// audio tokens.
        /// </param>
        /// <param name="outputTokens">
        /// The number of output tokens sent in the Response, including text and
        /// audio tokens.
        /// </param>
        /// <param name="inputTokenDetails"> Details about the input tokens used in the Response. </param>
        /// <param name="outputTokenDetails"> Details about the output tokens used in the Response. </param>
        /// <returns> A new <see cref="VoiceLive.ResponseUsage"/> instance for mocking. </returns>
        public static ResponseUsage ResponseUsage(int? totalTokens = default, int? inputTokens = default, int? outputTokens = default, ResponseUsageInputTokenDetails inputTokenDetails = default, ResponseUsageOutputTokenDetails outputTokenDetails = default)
        {
            return new ResponseUsage(
                totalTokens,
                inputTokens,
                outputTokens,
                inputTokenDetails,
                outputTokenDetails,
                additionalBinaryDataProperties: null);
        }

        /// <summary> The ResponseUsageInputTokenDetails. </summary>
        /// <param name="cachedTokens"> The number of cached tokens used in the Response. </param>
        /// <param name="textTokens"> The number of text tokens used in the Response. </param>
        /// <param name="audioTokens"> The number of audio tokens used in the Response. </param>
        /// <returns> A new <see cref="VoiceLive.ResponseUsageInputTokenDetails"/> instance for mocking. </returns>
        public static ResponseUsageInputTokenDetails ResponseUsageInputTokenDetails(int? cachedTokens = default, int? textTokens = default, int? audioTokens = default)
        {
            return new ResponseUsageInputTokenDetails(cachedTokens, textTokens, audioTokens, additionalBinaryDataProperties: null);
        }

        /// <summary> The ResponseUsageOutputTokenDetails. </summary>
        /// <param name="textTokens"> The number of text tokens used in the Response. </param>
        /// <param name="audioTokens"> The number of audio tokens used in the Response. </param>
        /// <returns> A new <see cref="VoiceLive.ResponseUsageOutputTokenDetails"/> instance for mocking. </returns>
        public static ResponseUsageOutputTokenDetails ResponseUsageOutputTokenDetails(int? textTokens = default, int? audioTokens = default)
        {
            return new ResponseUsageOutputTokenDetails(textTokens, audioTokens, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Returned when a Response is done streaming. Always emitted, no matter the
        /// final state. The Response object included in the `response.done` event will
        /// include all output Items in the Response but will omit the raw audio data.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="response"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseDone"/> instance for mocking. </returns>
        public static ServerEventResponseDone ServerEventResponseDone(string eventId = default, VoiceLiveResponse response = default)
        {
            return new ServerEventResponseDone(ServerEventType.ResponseDone, eventId, additionalBinaryDataProperties: null, response);
        }

        /// <summary> Returned when a new Item is created during Response generation. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the Response to which the item belongs. </param>
        /// <param name="outputIndex"> The index of the output item in the Response. </param>
        /// <param name="item"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseOutputItemAdded"/> instance for mocking. </returns>
        public static ServerEventResponseOutputItemAdded ServerEventResponseOutputItemAdded(string eventId = default, string responseId = default, int outputIndex = default, ConversationItemWithReference item = default)
        {
            return new ServerEventResponseOutputItemAdded(
                ServerEventType.ResponseOutputItemAdded,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                outputIndex,
                item);
        }

        /// <summary>
        /// Returned when an Item is done streaming. Also emitted when a Response is
        /// interrupted, incomplete, or cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the Response to which the item belongs. </param>
        /// <param name="outputIndex"> The index of the output item in the Response. </param>
        /// <param name="item"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseOutputItemDone"/> instance for mocking. </returns>
        public static ServerEventResponseOutputItemDone ServerEventResponseOutputItemDone(string eventId = default, string responseId = default, int outputIndex = default, ConversationResponseItem item = default)
        {
            return new ServerEventResponseOutputItemDone(
                ServerEventType.ResponseOutputItemDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                outputIndex,
                item);
        }

        /// <summary>
        /// Returned when a new content part is added to an assistant message item during
        /// response generation.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item to which the content part was added. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="part"> The content part that was added. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseContentPartAdded"/> instance for mocking. </returns>
        public static ServerEventResponseContentPartAdded ServerEventResponseContentPartAdded(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, ContentPart part = default)
        {
            return new ServerEventResponseContentPartAdded(
                ServerEventType.ResponseContentPartAdded,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                part);
        }

        /// <summary>
        /// Returned when a content part is done streaming in an assistant message item.
        /// Also emitted when a Response is interrupted, incomplete, or cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="part"> The content part that is done. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseContentPartDone"/> instance for mocking. </returns>
        public static ServerEventResponseContentPartDone ServerEventResponseContentPartDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, ContentPart part = default)
        {
            return new ServerEventResponseContentPartDone(
                ServerEventType.ResponseContentPartDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                part);
        }

        /// <summary>
        /// Returned when the text value of a "text" content part is done streaming. Also
        /// emitted when a Response is interrupted, incomplete, or cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="text"> The final text content. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseTextDone"/> instance for mocking. </returns>
        public static ServerEventResponseTextDone ServerEventResponseTextDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, string text = default)
        {
            return new ServerEventResponseTextDone(
                ServerEventType.ResponseTextDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                text);
        }

        /// <summary> Returned when the model-generated transcription of audio output is updated. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="delta"> The transcript delta. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseAudioTranscriptDelta"/> instance for mocking. </returns>
        public static ServerEventResponseAudioTranscriptDelta ServerEventResponseAudioTranscriptDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, string delta = default)
        {
            return new ServerEventResponseAudioTranscriptDelta(
                ServerEventType.ResponseAudioTranscriptDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                delta);
        }

        /// <summary>
        /// Returned when the model-generated transcription of audio output is done
        /// streaming. Also emitted when a Response is interrupted, incomplete, or
        /// cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="transcript"> The final transcript of the audio. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseAudioTranscriptDone"/> instance for mocking. </returns>
        public static ServerEventResponseAudioTranscriptDone ServerEventResponseAudioTranscriptDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, string transcript = default)
        {
            return new ServerEventResponseAudioTranscriptDone(
                ServerEventType.ResponseAudioTranscriptDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                transcript);
        }

        /// <summary>
        /// Returned when the model-generated audio is done. Also emitted when a Response
        /// is interrupted, incomplete, or cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseAudioDone"/> instance for mocking. </returns>
        public static ServerEventResponseAudioDone ServerEventResponseAudioDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default)
        {
            return new ServerEventResponseAudioDone(
                ServerEventType.ResponseAudioDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex);
        }

        /// <summary> Returned when the model-generated function call arguments are updated. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the function call item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="callId"> The ID of the function call. </param>
        /// <param name="delta"> The arguments delta as a JSON string. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseFunctionCallArgumentsDelta"/> instance for mocking. </returns>
        public static ServerEventResponseFunctionCallArgumentsDelta ServerEventResponseFunctionCallArgumentsDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, string callId = default, string delta = default)
        {
            return new ServerEventResponseFunctionCallArgumentsDelta(
                ServerEventType.ResponseFunctionCallArgumentsDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                callId,
                delta);
        }

        /// <summary>
        /// Returned when the model-generated function call arguments are done streaming.
        /// Also emitted when a Response is interrupted, incomplete, or cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the function call item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="callId"> The ID of the function call. </param>
        /// <param name="arguments"> The final arguments as a JSON string. </param>
        /// <param name="name"> The name of the function call. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseFunctionCallArgumentsDone"/> instance for mocking. </returns>
        public static ServerEventResponseFunctionCallArgumentsDone ServerEventResponseFunctionCallArgumentsDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, string callId = default, string arguments = default, string name = default)
        {
            return new ServerEventResponseFunctionCallArgumentsDone(
                ServerEventType.ResponseFunctionCallArgumentsDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                callId,
                arguments,
                name);
        }

        /// <summary> Represents a delta update of blendshape animation frames for a specific output of a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <param name="frames"></param>
        /// <param name="frameIndex"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseAnimationBlendshapeDeltaEvent"/> instance for mocking. </returns>
        public static ResponseAnimationBlendshapeDeltaEvent ResponseAnimationBlendshapeDeltaEvent(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, BinaryData frames = default, int frameIndex = default)
        {
            return new ResponseAnimationBlendshapeDeltaEvent(
                ServerEventType.ResponseAnimationBlendshapesDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                frames,
                frameIndex);
        }

        /// <summary> Indicates the completion of blendshape animation processing for a specific output of a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseAnimationBlendshapeDoneEvent"/> instance for mocking. </returns>
        public static ResponseAnimationBlendshapeDoneEvent ResponseAnimationBlendshapeDoneEvent(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default)
        {
            return new ResponseAnimationBlendshapeDoneEvent(
                ServerEventType.ResponseAnimationBlendshapesDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex);
        }

        /// <summary> Represents an emotion hypothesis detected from response audio with multiple candidates. </summary>
        /// <param name="eventId"></param>
        /// <param name="emotion"></param>
        /// <param name="candidates"></param>
        /// <param name="audioOffsetMs"></param>
        /// <param name="audioDurationMs"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseEmotionHypothesis"/> instance for mocking. </returns>
        public static ResponseEmotionHypothesis ResponseEmotionHypothesis(string eventId = default, string emotion = default, IEnumerable<EmotionCandidate> candidates = default, int audioOffsetMs = default, int audioDurationMs = default, string responseId = default, string itemId = default)
        {
            candidates ??= new ChangeTrackingList<EmotionCandidate>();

            return new ResponseEmotionHypothesis(
                ServerEventType.ResponseEmotionHypothesis,
                eventId,
                additionalBinaryDataProperties: null,
                emotion,
                candidates.ToList(),
                audioOffsetMs,
                audioDurationMs,
                responseId,
                itemId);
        }

        /// <summary> The EmotionCandidate. </summary>
        /// <param name="emotion"></param>
        /// <param name="confidence"></param>
        /// <returns> A new <see cref="VoiceLive.EmotionCandidate"/> instance for mocking. </returns>
        public static EmotionCandidate EmotionCandidate(string emotion = default, float confidence = default)
        {
            return new EmotionCandidate(emotion, confidence, additionalBinaryDataProperties: null);
        }

        /// <summary> Represents a word-level audio timestamp delta for a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <param name="audioOffsetMs"></param>
        /// <param name="audioDurationMs"></param>
        /// <param name="text"></param>
        /// <param name="timestampType"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseAudioTimestampDeltaEvent"/> instance for mocking. </returns>
        public static ResponseAudioTimestampDeltaEvent ResponseAudioTimestampDeltaEvent(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, int audioOffsetMs = default, int audioDurationMs = default, string text = default, string timestampType = default)
        {
            return new ResponseAudioTimestampDeltaEvent(
                ServerEventType.ResponseAudioTimestampDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                audioOffsetMs,
                audioDurationMs,
                text,
                timestampType);
        }

        /// <summary> Indicates completion of audio timestamp delivery for a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseAudioTimestampDoneEvent"/> instance for mocking. </returns>
        public static ResponseAudioTimestampDoneEvent ResponseAudioTimestampDoneEvent(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default)
        {
            return new ResponseAudioTimestampDoneEvent(
                ServerEventType.ResponseAudioTimestampDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex);
        }

        /// <summary> Represents a viseme ID delta update for animation based on audio. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <param name="audioOffsetMs"></param>
        /// <param name="visemeId"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseAnimationVisemeDeltaEvent"/> instance for mocking. </returns>
        public static ResponseAnimationVisemeDeltaEvent ResponseAnimationVisemeDeltaEvent(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, int audioOffsetMs = default, int visemeId = default)
        {
            return new ResponseAnimationVisemeDeltaEvent(
                ServerEventType.ResponseAnimationVisemeDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                audioOffsetMs,
                visemeId);
        }

        /// <summary> Indicates completion of viseme animation delivery for a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseAnimationVisemeDoneEvent"/> instance for mocking. </returns>
        public static ResponseAnimationVisemeDoneEvent ResponseAnimationVisemeDoneEvent(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default)
        {
            return new ResponseAnimationVisemeDoneEvent(
                ServerEventType.ResponseAnimationVisemeDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex);
        }
    }
}
