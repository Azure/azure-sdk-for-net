// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

// <auto-generated/>

#nullable disable

using System;
using System.Collections.Generic;
using System.Linq;

namespace Azure.AI.VoiceLive
{
    /// <summary> A factory class for creating instances of the models for mocking. </summary>
    public static partial class VoiceLiveModelFactory
    {
        /// <summary> The RequestSession. </summary>
        /// <param name="model"></param>
        /// <param name="modalities"></param>
        /// <param name="animation"></param>
        /// <param name="instructions"></param>
        /// <param name="inputAudio"></param>
        /// <param name="inputAudioSamplingRate"></param>
        /// <param name="inputAudioFormat"></param>
        /// <param name="outputAudioFormat"></param>
        /// <param name="turnDetection"></param>
        /// <param name="inputAudioNoiseReduction"></param>
        /// <param name="inputAudioEchoCancellation"></param>
        /// <param name="avatar"></param>
        /// <param name="inputAudioTranscription"></param>
        /// <param name="outputAudioTimestampTypes"></param>
        /// <param name="tools"></param>
        /// <param name="temperature"></param>
        /// <param name="serviceVoice"></param>
        /// <param name="maxResponseOutputTokens"></param>
        /// <param name="toolChoice"></param>
        /// <returns> A new <see cref="VoiceLive.RequestSession"/> instance for mocking. </returns>
        public static RequestSession RequestSession(string model = default, IEnumerable<InputModality> modalities = default, AnimationOptions animation = default, string instructions = default, InputAudio inputAudio = default, int? inputAudioSamplingRate = default, AudioFormat? inputAudioFormat = default, AudioFormat? outputAudioFormat = default, TurnDetection turnDetection = default, AudioNoiseReduction inputAudioNoiseReduction = default, AudioEchoCancellation inputAudioEchoCancellation = default, AvatarConfig avatar = default, AudioInputTranscriptionSettings inputAudioTranscription = default, IEnumerable<AudioTimestampType> outputAudioTimestampTypes = default, IEnumerable<VoiceLiveToolDefinition> tools = default, float? temperature = default, BinaryData serviceVoice = default, BinaryData maxResponseOutputTokens = default, BinaryData toolChoice = default)
        {
            modalities ??= new ChangeTrackingList<InputModality>();
            outputAudioTimestampTypes ??= new ChangeTrackingList<AudioTimestampType>();
            tools ??= new ChangeTrackingList<VoiceLiveToolDefinition>();

            return new RequestSession(
                model,
                modalities.ToList(),
                animation,
                instructions,
                inputAudio,
                inputAudioSamplingRate,
                inputAudioFormat,
                outputAudioFormat,
                turnDetection,
                inputAudioNoiseReduction,
                inputAudioEchoCancellation,
                avatar,
                inputAudioTranscription,
                outputAudioTimestampTypes.ToList(),
                tools.ToList(),
                temperature,
                serviceVoice,
                maxResponseOutputTokens,
                toolChoice,
                additionalBinaryDataProperties: null);
        }

        /// <summary> Configuration for animation outputs including blendshapes, visemes, and emotion metadata. </summary>
        /// <param name="modelName"> The name of the animation model to use. </param>
        /// <param name="outputs"> Set of output data types requested from the animation system. </param>
        /// <param name="emotionDetectionIntervalMs"> Interval for emotion detection in milliseconds. If not set, emotion detection is disabled. </param>
        /// <returns> A new <see cref="VoiceLive.AnimationOptions"/> instance for mocking. </returns>
        public static AnimationOptions AnimationOptions(string modelName = default, IEnumerable<AnimationOutputType> outputs = default, int? emotionDetectionIntervalMs = default)
        {
            outputs ??= new ChangeTrackingList<AnimationOutputType>();

            return new AnimationOptions(modelName, outputs.ToList(), emotionDetectionIntervalMs, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// OpenAI voice configuration with explicit type field.
        /// 
        /// This provides a unified interface for OpenAI voices, complementing the
        /// existing string-based OAIVoice for backward compatibility.
        /// </summary>
        /// <param name="type"></param>
        /// <param name="name"></param>
        /// <returns> A new <see cref="VoiceLive.OpenAIVoice"/> instance for mocking. </returns>
        public static OpenAIVoice OpenAIVoice(string @type = default, OAIVoice name = default)
        {
            return new OpenAIVoice(@type, name, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Base for Azure voice configurations.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="AzureCustomVoice"/>, <see cref="AzureStandardVoice"/>, <see cref="AzurePlatformVoice"/>, and <see cref="AzurePersonalVoice"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <returns> A new <see cref="VoiceLive.AzureVoice"/> instance for mocking. </returns>
        public static AzureVoice AzureVoice(string @type = default)
        {
            return new UnknownAzureVoice(@type, additionalBinaryDataProperties: null);
        }

        /// <summary> Azure custom voice configuration (preferred). </summary>
        /// <param name="name"> Voice name cannot be empty. </param>
        /// <param name="endpointId"> Endpoint ID cannot be empty. </param>
        /// <param name="temperature"> Temperature must be between 0.0 and 1.0. </param>
        /// <param name="customLexiconUri"></param>
        /// <param name="preferLocales"></param>
        /// <param name="locale"></param>
        /// <param name="style"></param>
        /// <param name="pitch"></param>
        /// <param name="rate"></param>
        /// <param name="volume"></param>
        /// <returns> A new <see cref="VoiceLive.AzureCustomVoice"/> instance for mocking. </returns>
        public static AzureCustomVoice AzureCustomVoice(string name = default, string endpointId = default, float? temperature = default, string customLexiconUri = default, IEnumerable<string> preferLocales = default, string locale = default, string style = default, string pitch = default, string rate = default, string volume = default)
        {
            preferLocales ??= new ChangeTrackingList<string>();

            return new AzureCustomVoice(
                "azure-custom",
                additionalBinaryDataProperties: null,
                name,
                endpointId,
                temperature,
                customLexiconUri,
                preferLocales.ToList(),
                locale,
                style,
                pitch,
                rate,
                volume);
        }

        /// <summary> Azure standard voice configuration. </summary>
        /// <param name="name"> Voice name cannot be empty. </param>
        /// <param name="temperature"> Temperature must be between 0.0 and 1.0. </param>
        /// <param name="customLexiconUrl"></param>
        /// <param name="preferLocales"></param>
        /// <param name="locale"></param>
        /// <param name="style"></param>
        /// <param name="pitch"></param>
        /// <param name="rate"></param>
        /// <param name="volume"></param>
        /// <returns> A new <see cref="VoiceLive.AzureStandardVoice"/> instance for mocking. </returns>
        public static AzureStandardVoice AzureStandardVoice(string name = default, float? temperature = default, string customLexiconUrl = default, IEnumerable<string> preferLocales = default, string locale = default, string style = default, string pitch = default, string rate = default, string volume = default)
        {
            preferLocales ??= new ChangeTrackingList<string>();

            return new AzureStandardVoice(
                "azure-standard",
                additionalBinaryDataProperties: null,
                name,
                temperature,
                customLexiconUrl,
                preferLocales.ToList(),
                locale,
                style,
                pitch,
                rate,
                volume);
        }

        /// <summary> Azure platform voice configuration (variant of standard). </summary>
        /// <param name="name"> Voice name cannot be empty. </param>
        /// <param name="temperature"> Temperature must be between 0.0 and 1.0. </param>
        /// <param name="customLexiconUrl"></param>
        /// <param name="preferLocales"></param>
        /// <param name="locale"></param>
        /// <param name="style"></param>
        /// <param name="pitch"></param>
        /// <param name="rate"></param>
        /// <param name="volume"></param>
        /// <returns> A new <see cref="VoiceLive.AzurePlatformVoice"/> instance for mocking. </returns>
        public static AzurePlatformVoice AzurePlatformVoice(string name = default, float? temperature = default, string customLexiconUrl = default, IEnumerable<string> preferLocales = default, string locale = default, string style = default, string pitch = default, string rate = default, string volume = default)
        {
            preferLocales ??= new ChangeTrackingList<string>();

            return new AzurePlatformVoice(
                "azure-platform",
                additionalBinaryDataProperties: null,
                name,
                temperature,
                customLexiconUrl,
                preferLocales.ToList(),
                locale,
                style,
                pitch,
                rate,
                volume);
        }

        /// <summary> Azure personal voice configuration. </summary>
        /// <param name="name"> Voice name cannot be empty. </param>
        /// <param name="temperature"> Temperature must be between 0.0 and 1.0. </param>
        /// <param name="model"> Underlying neural model to use for personal voice. </param>
        /// <returns> A new <see cref="VoiceLive.AzurePersonalVoice"/> instance for mocking. </returns>
        public static AzurePersonalVoice AzurePersonalVoice(string name = default, float? temperature = default, AzurePersonalVoiceModel model = default)
        {
            return new AzurePersonalVoice("azure-personal", additionalBinaryDataProperties: null, name, temperature, model);
        }

        /// <summary> Voice configuration for LLM (Large Language Model) voices. </summary>
        /// <param name="type"></param>
        /// <param name="name"></param>
        /// <returns> A new <see cref="VoiceLive.LLMVoice"/> instance for mocking. </returns>
        public static LLMVoice LLMVoice(string @type = default, LLMVoiceName name = default)
        {
            return new LLMVoice(@type, name, additionalBinaryDataProperties: null);
        }

        /// <summary> Configuration for client audio input. Used to specify the audio model and optional phrase list. </summary>
        /// <param name="model"> The name of the model to use for input audio (currently only 'azure-standard' is supported). </param>
        /// <param name="phraseList"> Optional list of phrases to bias the speech recognition engine. </param>
        /// <returns> A new <see cref="VoiceLive.InputAudio"/> instance for mocking. </returns>
        public static InputAudio InputAudio(string model = default, IEnumerable<string> phraseList = default)
        {
            phraseList ??= new ChangeTrackingList<string>();

            return new InputAudio(model, phraseList.ToList(), additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Top-level union for turn detection configuration.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="NoTurnDetection"/>, <see cref="ServerVad"/>, <see cref="AzureSemanticVad"/>, <see cref="AzureSemanticVadEn"/>, <see cref="AzureSemanticVadServer"/>, and <see cref="AzureMultilingualSemanticVad"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <returns> A new <see cref="VoiceLive.TurnDetection"/> instance for mocking. </returns>
        public static TurnDetection TurnDetection(string @type = default)
        {
            return new UnknownTurnDetection(@type.ToTurnDetectionType(), additionalBinaryDataProperties: null);
        }

        /// <summary> Disables turn detection. </summary>
        /// <returns> A new <see cref="VoiceLive.NoTurnDetection"/> instance for mocking. </returns>
        public static NoTurnDetection NoTurnDetection()
        {
            return new NoTurnDetection(TurnDetectionType.None, additionalBinaryDataProperties: null);
        }

        /// <summary> Base model for VAD-based turn detection. </summary>
        /// <param name="threshold"></param>
        /// <param name="prefixPaddingMs"></param>
        /// <param name="silenceDurationMs"></param>
        /// <param name="endOfUtteranceDetection"></param>
        /// <param name="autoTruncate"></param>
        /// <returns> A new <see cref="VoiceLive.ServerVad"/> instance for mocking. </returns>
        public static ServerVad ServerVad(float? threshold = default, int? prefixPaddingMs = default, int? silenceDurationMs = default, EOUDetection endOfUtteranceDetection = default, bool? autoTruncate = default)
        {
            return new ServerVad(
                TurnDetectionType.ServerVad,
                additionalBinaryDataProperties: null,
                threshold,
                prefixPaddingMs,
                silenceDurationMs,
                endOfUtteranceDetection,
                autoTruncate);
        }

        /// <summary>
        /// Top-level union for end-of-utterance (EOU) semantic detection configuration.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="AzureSemanticDetection"/>, <see cref="AzureSemanticDetectionEn"/>, and <see cref="AzureSemanticDetectionMultilingual"/>.
        /// </summary>
        /// <param name="model"></param>
        /// <returns> A new <see cref="VoiceLive.EOUDetection"/> instance for mocking. </returns>
        public static EOUDetection EOUDetection(string model = default)
        {
            return new UnknownEOUDetection(model.ToEOUDetectionModel(), additionalBinaryDataProperties: null);
        }

        /// <summary> Azure semantic end-of-utterance detection (default). </summary>
        /// <param name="threshold"></param>
        /// <param name="timeout"></param>
        /// <param name="secondaryThreshold"></param>
        /// <param name="secondaryTimeout"></param>
        /// <param name="disableRules"></param>
        /// <param name="srBoost"></param>
        /// <param name="extraImendCheck"></param>
        /// <returns> A new <see cref="VoiceLive.AzureSemanticDetection"/> instance for mocking. </returns>
        public static AzureSemanticDetection AzureSemanticDetection(float? threshold = default, float? timeout = default, float? secondaryThreshold = default, float? secondaryTimeout = default, bool? disableRules = default, float? srBoost = default, bool? extraImendCheck = default)
        {
            return new AzureSemanticDetection(
                EOUDetectionModel.SemanticDetectionV1,
                additionalBinaryDataProperties: null,
                threshold,
                timeout,
                secondaryThreshold,
                secondaryTimeout,
                disableRules,
                srBoost,
                extraImendCheck);
        }

        /// <summary> Azure semantic end-of-utterance detection (English-optimized). </summary>
        /// <param name="threshold"></param>
        /// <param name="timeout"></param>
        /// <param name="secondaryThreshold"></param>
        /// <param name="secondaryTimeout"></param>
        /// <param name="disableRules"></param>
        /// <param name="srBoost"></param>
        /// <param name="extraImendCheck"></param>
        /// <returns> A new <see cref="VoiceLive.AzureSemanticDetectionEn"/> instance for mocking. </returns>
        public static AzureSemanticDetectionEn AzureSemanticDetectionEn(float? threshold = default, float? timeout = default, float? secondaryThreshold = default, float? secondaryTimeout = default, bool? disableRules = default, float? srBoost = default, bool? extraImendCheck = default)
        {
            return new AzureSemanticDetectionEn(
                EOUDetectionModel.SemanticDetectionV1En,
                additionalBinaryDataProperties: null,
                threshold,
                timeout,
                secondaryThreshold,
                secondaryTimeout,
                disableRules,
                srBoost,
                extraImendCheck);
        }

        /// <summary> Azure semantic end-of-utterance detection (multilingual). </summary>
        /// <param name="threshold"></param>
        /// <param name="timeout"></param>
        /// <param name="secondaryThreshold"></param>
        /// <param name="secondaryTimeout"></param>
        /// <param name="disableRules"></param>
        /// <param name="srBoost"></param>
        /// <param name="extraImendCheck"></param>
        /// <returns> A new <see cref="VoiceLive.AzureSemanticDetectionMultilingual"/> instance for mocking. </returns>
        public static AzureSemanticDetectionMultilingual AzureSemanticDetectionMultilingual(float? threshold = default, float? timeout = default, float? secondaryThreshold = default, float? secondaryTimeout = default, bool? disableRules = default, float? srBoost = default, bool? extraImendCheck = default)
        {
            return new AzureSemanticDetectionMultilingual(
                EOUDetectionModel.SemanticDetectionV1Multilingual,
                additionalBinaryDataProperties: null,
                threshold,
                timeout,
                secondaryThreshold,
                secondaryTimeout,
                disableRules,
                srBoost,
                extraImendCheck);
        }

        /// <summary> Server Speech Detection (Azure semantic VAD, default variant). </summary>
        /// <param name="threshold"></param>
        /// <param name="prefixPaddingMs"></param>
        /// <param name="silenceDurationMs"></param>
        /// <param name="endOfUtteranceDetection"></param>
        /// <param name="negThreshold"></param>
        /// <param name="speechDurationMs"></param>
        /// <param name="windowSize"></param>
        /// <param name="distinctCiPhones"></param>
        /// <param name="requireVowel"></param>
        /// <param name="removeFillerWords"></param>
        /// <param name="languages"></param>
        /// <param name="autoTruncate"></param>
        /// <returns> A new <see cref="VoiceLive.AzureSemanticVad"/> instance for mocking. </returns>
        public static AzureSemanticVad AzureSemanticVad(float? threshold = default, int? prefixPaddingMs = default, int? silenceDurationMs = default, EOUDetection endOfUtteranceDetection = default, float? negThreshold = default, int? speechDurationMs = default, int? windowSize = default, int? distinctCiPhones = default, bool? requireVowel = default, bool? removeFillerWords = default, IEnumerable<string> languages = default, bool? autoTruncate = default)
        {
            languages ??= new ChangeTrackingList<string>();

            return new AzureSemanticVad(
                TurnDetectionType.AzureSemanticVad,
                additionalBinaryDataProperties: null,
                threshold,
                prefixPaddingMs,
                silenceDurationMs,
                endOfUtteranceDetection,
                negThreshold,
                speechDurationMs,
                windowSize,
                distinctCiPhones,
                requireVowel,
                removeFillerWords,
                languages.ToList(),
                autoTruncate);
        }

        /// <summary> Server Speech Detection (Azure semantic VAD, English-only). </summary>
        /// <param name="threshold"></param>
        /// <param name="prefixPaddingMs"></param>
        /// <param name="silenceDurationMs"></param>
        /// <param name="endOfUtteranceDetection"></param>
        /// <param name="negThreshold"></param>
        /// <param name="speechDurationMs"></param>
        /// <param name="windowSize"></param>
        /// <param name="distinctCiPhones"></param>
        /// <param name="requireVowel"></param>
        /// <param name="removeFillerWords"></param>
        /// <param name="languages"></param>
        /// <param name="autoTruncate"></param>
        /// <returns> A new <see cref="VoiceLive.AzureSemanticVadEn"/> instance for mocking. </returns>
        public static AzureSemanticVadEn AzureSemanticVadEn(float? threshold = default, int? prefixPaddingMs = default, int? silenceDurationMs = default, EOUDetection endOfUtteranceDetection = default, float? negThreshold = default, int? speechDurationMs = default, int? windowSize = default, int? distinctCiPhones = default, bool? requireVowel = default, bool? removeFillerWords = default, IEnumerable<string> languages = default, bool? autoTruncate = default)
        {
            languages ??= new ChangeTrackingList<string>();

            return new AzureSemanticVadEn(
                TurnDetectionType.AzureSemanticVadEn,
                additionalBinaryDataProperties: null,
                threshold,
                prefixPaddingMs,
                silenceDurationMs,
                endOfUtteranceDetection,
                negThreshold,
                speechDurationMs,
                windowSize,
                distinctCiPhones,
                requireVowel,
                removeFillerWords,
                languages.ToList(),
                autoTruncate);
        }

        /// <summary> Server Speech Detection (legacy `server_sd` alias). </summary>
        /// <param name="threshold"></param>
        /// <param name="prefixPaddingMs"></param>
        /// <param name="silenceDurationMs"></param>
        /// <param name="endOfUtteranceDetection"></param>
        /// <param name="negThreshold"></param>
        /// <param name="speechDurationMs"></param>
        /// <param name="windowSize"></param>
        /// <param name="distinctCiPhones"></param>
        /// <param name="requireVowel"></param>
        /// <param name="removeFillerWords"></param>
        /// <param name="languages"></param>
        /// <param name="autoTruncate"></param>
        /// <returns> A new <see cref="VoiceLive.AzureSemanticVadServer"/> instance for mocking. </returns>
        public static AzureSemanticVadServer AzureSemanticVadServer(float? threshold = default, int? prefixPaddingMs = default, int? silenceDurationMs = default, EOUDetection endOfUtteranceDetection = default, float? negThreshold = default, int? speechDurationMs = default, int? windowSize = default, int? distinctCiPhones = default, bool? requireVowel = default, bool? removeFillerWords = default, IEnumerable<string> languages = default, bool? autoTruncate = default)
        {
            languages ??= new ChangeTrackingList<string>();

            return new AzureSemanticVadServer(
                TurnDetectionType.ServerSd,
                additionalBinaryDataProperties: null,
                threshold,
                prefixPaddingMs,
                silenceDurationMs,
                endOfUtteranceDetection,
                negThreshold,
                speechDurationMs,
                windowSize,
                distinctCiPhones,
                requireVowel,
                removeFillerWords,
                languages.ToList(),
                autoTruncate);
        }

        /// <summary> Server Speech Detection (Azure semantic VAD). </summary>
        /// <param name="threshold"></param>
        /// <param name="prefixPaddingMs"></param>
        /// <param name="silenceDurationMs"></param>
        /// <param name="endOfUtteranceDetection"></param>
        /// <param name="negThreshold"></param>
        /// <param name="speechDurationMs"></param>
        /// <param name="windowSize"></param>
        /// <param name="distinctCiPhones"></param>
        /// <param name="requireVowel"></param>
        /// <param name="removeFillerWords"></param>
        /// <param name="languages"></param>
        /// <param name="autoTruncate"></param>
        /// <returns> A new <see cref="VoiceLive.AzureMultilingualSemanticVad"/> instance for mocking. </returns>
        public static AzureMultilingualSemanticVad AzureMultilingualSemanticVad(float? threshold = default, int? prefixPaddingMs = default, int? silenceDurationMs = default, EOUDetection endOfUtteranceDetection = default, float? negThreshold = default, int? speechDurationMs = default, int? windowSize = default, int? distinctCiPhones = default, bool? requireVowel = default, bool? removeFillerWords = default, IEnumerable<string> languages = default, bool? autoTruncate = default)
        {
            languages ??= new ChangeTrackingList<string>();

            return new AzureMultilingualSemanticVad(
                TurnDetectionType.AzureSemanticVadMultilingual,
                additionalBinaryDataProperties: null,
                threshold,
                prefixPaddingMs,
                silenceDurationMs,
                endOfUtteranceDetection,
                negThreshold,
                speechDurationMs,
                windowSize,
                distinctCiPhones,
                requireVowel,
                removeFillerWords,
                languages.ToList(),
                autoTruncate);
        }

        /// <summary> Configuration for input audio noise reduction. </summary>
        /// <param name="type"> The type of noise reduction model. </param>
        /// <returns> A new <see cref="VoiceLive.AudioNoiseReduction"/> instance for mocking. </returns>
        public static AudioNoiseReduction AudioNoiseReduction(string @type = default)
        {
            return new AudioNoiseReduction(@type, additionalBinaryDataProperties: null);
        }

        /// <summary> Echo cancellation configuration for server-side audio processing. </summary>
        /// <param name="type"> The type of echo cancellation model to use. </param>
        /// <returns> A new <see cref="VoiceLive.AudioEchoCancellation"/> instance for mocking. </returns>
        public static AudioEchoCancellation AudioEchoCancellation(string @type = default)
        {
            return new AudioEchoCancellation(@type, additionalBinaryDataProperties: null);
        }

        /// <summary> Configuration for avatar streaming and behavior during the session. </summary>
        /// <param name="iceServers"> Optional list of ICE servers to use for WebRTC connection establishment. </param>
        /// <param name="character"> The character name or ID used for the avatar. </param>
        /// <param name="style"> Optional avatar style, such as emotional tone or speaking style. </param>
        /// <param name="customized"> Indicates whether the avatar is customized or not. </param>
        /// <param name="video"> Optional video configuration including resolution, bitrate, and codec. </param>
        /// <returns> A new <see cref="VoiceLive.AvatarConfig"/> instance for mocking. </returns>
        public static AvatarConfig AvatarConfig(IEnumerable<IceServer> iceServers = default, string character = default, string style = default, bool customized = default, VideoParams video = default)
        {
            iceServers ??= new ChangeTrackingList<IceServer>();

            return new AvatarConfig(
                iceServers.ToList(),
                character,
                style,
                customized,
                video,
                additionalBinaryDataProperties: null);
        }

        /// <summary> ICE server configuration for WebRTC connection negotiation. </summary>
        /// <param name="uris"> List of ICE server URLs (e.g., TURN or STUN endpoints). </param>
        /// <param name="username"> Optional username used for authentication with the ICE server. </param>
        /// <param name="credential"> Optional credential (e.g., password or token) used for authentication. </param>
        /// <returns> A new <see cref="VoiceLive.IceServer"/> instance for mocking. </returns>
        public static IceServer IceServer(IEnumerable<Uri> uris = default, string username = default, string credential = default)
        {
            uris ??= new ChangeTrackingList<Uri>();

            return new IceServer(uris.ToList(), username, credential, additionalBinaryDataProperties: null);
        }

        /// <summary> Video streaming parameters for avatar. </summary>
        /// <param name="bitrate"> Bitrate in bits per second (e.g., 2000000 for 2 Mbps). </param>
        /// <param name="codec"> Codec to use for encoding. Currently only 'h264' is supported. </param>
        /// <param name="crop"> Optional cropping settings for the video stream. </param>
        /// <param name="resolution"> Optional resolution settings for the video stream. </param>
        /// <returns> A new <see cref="VoiceLive.VideoParams"/> instance for mocking. </returns>
        public static VideoParams VideoParams(int? bitrate = default, string codec = default, VideoCrop crop = default, VideoResolution resolution = default)
        {
            return new VideoParams(bitrate, codec, crop, resolution, additionalBinaryDataProperties: null);
        }

        /// <summary> Defines a video crop rectangle using top-left and bottom-right coordinates. </summary>
        /// <param name="topLeftInternal"> Top-left corner of the crop region. </param>
        /// <param name="bottomRightInternal"> Bottom-right corner of the crop region. </param>
        /// <returns> A new <see cref="VoiceLive.VideoCrop"/> instance for mocking. </returns>
        public static VideoCrop VideoCrop(IEnumerable<int> topLeftInternal = default, IEnumerable<int> bottomRightInternal = default)
        {
            topLeftInternal ??= new ChangeTrackingList<int>();
            bottomRightInternal ??= new ChangeTrackingList<int>();

            return new VideoCrop(topLeftInternal.ToList(), bottomRightInternal.ToList(), additionalBinaryDataProperties: null);
        }

        /// <summary> Resolution of the video feed in pixels. </summary>
        /// <param name="width"> Width of the video in pixels. Must be greater than 0. </param>
        /// <param name="height"> Height of the video in pixels. Must be greater than 0. </param>
        /// <returns> A new <see cref="VoiceLive.VideoResolution"/> instance for mocking. </returns>
        public static VideoResolution VideoResolution(int width = default, int height = default)
        {
            return new VideoResolution(width, height, additionalBinaryDataProperties: null);
        }

        /// <summary> Configuration for input audio transcription. </summary>
        /// <param name="model"> The model used for transcription. E.g., 'whisper-1', 'azure-fast-transcription', 's2s-ingraph'. </param>
        /// <param name="language"> The language code to use for transcription, if specified. </param>
        /// <param name="enabled"> Whether transcription is enabled. </param>
        /// <param name="customModel"> Whether a custom model is being used. </param>
        /// <returns> A new <see cref="VoiceLive.AudioInputTranscriptionSettings"/> instance for mocking. </returns>
        public static AudioInputTranscriptionSettings AudioInputTranscriptionSettings(AudioInputTranscriptionSettingsModel model = default, string language = default, bool enabled = default, bool customModel = default)
        {
            return new AudioInputTranscriptionSettings(model, language, enabled, customModel, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// The base representation of a voicelive tool definition.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="VoiceLiveFunctionDefinition"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <returns> A new <see cref="VoiceLive.VoiceLiveToolDefinition"/> instance for mocking. </returns>
        public static VoiceLiveToolDefinition VoiceLiveToolDefinition(string @type = default)
        {
            return new UnknownVoiceLiveToolDefinition(new ToolType(@type), additionalBinaryDataProperties: null);
        }

        /// <summary> The definition of a function tool as used by the voicelive endpoint. </summary>
        /// <param name="name"></param>
        /// <param name="description"></param>
        /// <param name="parameters"></param>
        /// <returns> A new <see cref="VoiceLive.VoiceLiveFunctionDefinition"/> instance for mocking. </returns>
        public static VoiceLiveFunctionDefinition VoiceLiveFunctionDefinition(string name = default, string description = default, BinaryData parameters = default)
        {
            return new VoiceLiveFunctionDefinition(ToolType.Function, additionalBinaryDataProperties: null, name, description, parameters);
        }

        /// <summary>
        /// A base representation for a voicelive tool_choice selecting a named tool.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="ToolChoiceFunctionObject"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <returns> A new <see cref="VoiceLive.ToolChoiceObject"/> instance for mocking. </returns>
        public static ToolChoiceObject ToolChoiceObject(string @type = default)
        {
            return new UnknownToolChoiceObject(new ToolType(@type), additionalBinaryDataProperties: null);
        }

        /// <summary> The representation of a voicelive tool_choice selecting a named function tool. </summary>
        /// <param name="function"></param>
        /// <returns> A new <see cref="VoiceLive.ToolChoiceFunctionObject"/> instance for mocking. </returns>
        public static ToolChoiceFunctionObject ToolChoiceFunctionObject(ToolChoiceFunctionObjectFunction function = default)
        {
            return new ToolChoiceFunctionObject(ToolType.Function, additionalBinaryDataProperties: null, function);
        }

        /// <summary> The ToolChoiceFunctionObjectFunction. </summary>
        /// <param name="name"></param>
        /// <returns> A new <see cref="VoiceLive.ToolChoiceFunctionObjectFunction"/> instance for mocking. </returns>
        public static ToolChoiceFunctionObjectFunction ToolChoiceFunctionObjectFunction(string name = default)
        {
            return new ToolChoiceFunctionObjectFunction(name, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Base for any response item; discriminated by `type`.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="MessageItem"/>, <see cref="FunctionCallItem"/>, and <see cref="FunctionCallOutputItem"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <param name="id"></param>
        /// <returns> A new <see cref="VoiceLive.ConversationRequestItem"/> instance for mocking. </returns>
        public static ConversationRequestItem ConversationRequestItem(string @type = default, string id = default)
        {
            return new UnknownConversationRequestItem(new ItemType(@type), id, additionalBinaryDataProperties: null);
        }

        /// <summary> The MessageItem. </summary>
        /// <param name="id"></param>
        /// <param name="status"></param>
        /// <returns> A new <see cref="VoiceLive.MessageItem"/> instance for mocking. </returns>
        public static MessageItem MessageItem(string id = default, ItemParamStatus? status = default)
        {
            return new MessageItem(ItemType.Message, id, additionalBinaryDataProperties: null, default, status);
        }

        /// <summary> The SystemMessageItem. </summary>
        /// <param name="id"></param>
        /// <param name="status"></param>
        /// <param name="content"></param>
        /// <returns> A new <see cref="VoiceLive.SystemMessageItem"/> instance for mocking. </returns>
        public static SystemMessageItem SystemMessageItem(string id = default, ItemParamStatus? status = default, IEnumerable<InputTextContentPart> content = default)
        {
            content ??= new ChangeTrackingList<InputTextContentPart>();

            return new SystemMessageItem(
                ItemType.Message,
                id,
                additionalBinaryDataProperties: null,
                "system",
                status,
                content.ToList());
        }

        /// <summary> The InputTextContentPart. </summary>
        /// <param name="text"></param>
        /// <returns> A new <see cref="VoiceLive.InputTextContentPart"/> instance for mocking. </returns>
        public static InputTextContentPart InputTextContentPart(string text = default)
        {
            return new InputTextContentPart("input_text", additionalBinaryDataProperties: null, text);
        }

        /// <summary>
        /// The UserContentPart.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="InputTextContentPart"/> and <see cref="InputAudioContentPart"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <returns> A new <see cref="VoiceLive.UserContentPart"/> instance for mocking. </returns>
        public static UserContentPart UserContentPart(string @type = default)
        {
            return new UnknownUserContentPart(@type, additionalBinaryDataProperties: null);
        }

        /// <summary> The InputAudioContentPart. </summary>
        /// <param name="audio"></param>
        /// <param name="transcript"></param>
        /// <returns> A new <see cref="VoiceLive.InputAudioContentPart"/> instance for mocking. </returns>
        public static InputAudioContentPart InputAudioContentPart(string audio = default, string transcript = default)
        {
            return new InputAudioContentPart("input_audio", additionalBinaryDataProperties: null, audio, transcript);
        }

        /// <summary> The UserMessageItem. </summary>
        /// <param name="id"></param>
        /// <param name="status"></param>
        /// <param name="content"></param>
        /// <returns> A new <see cref="VoiceLive.UserMessageItem"/> instance for mocking. </returns>
        public static UserMessageItem UserMessageItem(string id = default, ItemParamStatus? status = default, IEnumerable<UserContentPart> content = default)
        {
            content ??= new ChangeTrackingList<UserContentPart>();

            return new UserMessageItem(
                ItemType.Message,
                id,
                additionalBinaryDataProperties: null,
                "user",
                status,
                content.ToList());
        }

        /// <summary> The AssistantMessageItem. </summary>
        /// <param name="id"></param>
        /// <param name="status"></param>
        /// <param name="content"></param>
        /// <returns> A new <see cref="VoiceLive.AssistantMessageItem"/> instance for mocking. </returns>
        public static AssistantMessageItem AssistantMessageItem(string id = default, ItemParamStatus? status = default, IEnumerable<OutputTextContentPart> content = default)
        {
            content ??= new ChangeTrackingList<OutputTextContentPart>();

            return new AssistantMessageItem(
                ItemType.Message,
                id,
                additionalBinaryDataProperties: null,
                "assistant",
                status,
                content.ToList());
        }

        /// <summary> Output text content part. </summary>
        /// <param name="type"></param>
        /// <param name="text"></param>
        /// <returns> A new <see cref="VoiceLive.OutputTextContentPart"/> instance for mocking. </returns>
        public static OutputTextContentPart OutputTextContentPart(string @type = default, string text = default)
        {
            return new OutputTextContentPart(@type, text, additionalBinaryDataProperties: null);
        }

        /// <summary> The FunctionCallItem. </summary>
        /// <param name="id"></param>
        /// <param name="name"></param>
        /// <param name="callId"></param>
        /// <param name="arguments"></param>
        /// <param name="status"></param>
        /// <returns> A new <see cref="VoiceLive.FunctionCallItem"/> instance for mocking. </returns>
        public static FunctionCallItem FunctionCallItem(string id = default, string name = default, string callId = default, string arguments = default, ItemParamStatus? status = default)
        {
            return new FunctionCallItem(
                ItemType.FunctionCall,
                id,
                additionalBinaryDataProperties: null,
                name,
                callId,
                arguments,
                status);
        }

        /// <summary> The FunctionCallOutputItem. </summary>
        /// <param name="id"></param>
        /// <param name="callId"></param>
        /// <param name="output"></param>
        /// <param name="status"></param>
        /// <returns> A new <see cref="VoiceLive.FunctionCallOutputItem"/> instance for mocking. </returns>
        public static FunctionCallOutputItem FunctionCallOutputItem(string id = default, string callId = default, string output = default, ItemParamStatus? status = default)
        {
            return new FunctionCallOutputItem(
                ItemType.FunctionCallOutput,
                id,
                additionalBinaryDataProperties: null,
                callId,
                output,
                status);
        }

        /// <summary> Sent when the server is in the process of establishing an avatar media connection and provides its SDP answer. </summary>
        /// <param name="eventId"></param>
        /// <param name="serverSdp"> The server's SDP answer for the avatar connection. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventSessionAvatarConnecting"/> instance for mocking. </returns>
        public static ServerEventSessionAvatarConnecting ServerEventSessionAvatarConnecting(string eventId = default, string serverSdp = default)
        {
            return new ServerEventSessionAvatarConnecting(ServerEventType.SessionAvatarConnecting, eventId, additionalBinaryDataProperties: null, serverSdp);
        }

        /// <summary>
        /// A voicelive server event.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="ServerEventError"/>, <see cref="ServerEventSessionCreated"/>, <see cref="ServerEventSessionUpdated"/>, <see cref="ServerEventSessionAvatarConnecting"/>, <see cref="ServerEventInputAudioBufferCommitted"/>, <see cref="ServerEventInputAudioBufferCleared"/>, <see cref="ServerEventInputAudioBufferSpeechStarted"/>, <see cref="ServerEventInputAudioBufferSpeechStopped"/>, <see cref="ServerEventConversationItemCreated"/>, <see cref="ServerEventConversationItemInputAudioTranscriptionCompleted"/>, <see cref="ServerEventConversationItemInputAudioTranscriptionFailed"/>, <see cref="ServerEventConversationItemTruncated"/>, <see cref="ServerEventConversationItemDeleted"/>, <see cref="ServerEventResponseCreated"/>, <see cref="ServerEventResponseDone"/>, <see cref="ServerEventResponseOutputItemAdded"/>, <see cref="ServerEventResponseOutputItemDone"/>, <see cref="ServerEventResponseContentPartAdded"/>, <see cref="ServerEventResponseContentPartDone"/>, <see cref="ServerEventResponseTextDelta"/>, <see cref="ServerEventResponseTextDone"/>, <see cref="ServerEventResponseAudioTranscriptDelta"/>, <see cref="ServerEventResponseAudioTranscriptDone"/>, <see cref="ServerEventResponseAudioDelta"/>, <see cref="ServerEventResponseAudioDone"/>, <see cref="ServerEventResponseAnimationBlendshapeDelta"/>, <see cref="ServerEventResponseAnimationBlendshapeDone"/>, <see cref="ServerEventResponseEmotionHypothesis"/>, <see cref="ServerEventResponseAudioTimestampDelta"/>, <see cref="ServerEventResponseAudioTimestampDone"/>, <see cref="ServerEventResponseAnimationVisemeDelta"/>, <see cref="ServerEventResponseAnimationVisemeDone"/>, <see cref="ServerEventConversationItemInputAudioTranscriptionDelta"/>, <see cref="ServerEventConversationItemRetrieved"/>, <see cref="ServerEventResponseFunctionCallArgumentsDelta"/>, and <see cref="ServerEventResponseFunctionCallArgumentsDone"/>.
        /// </summary>
        /// <param name="type"> The type of event. </param>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventBase"/> instance for mocking. </returns>
        public static ServerEventBase ServerEventBase(string @type = default, string eventId = default)
        {
            return new UnknownServerEventBase(new ServerEventType(@type), eventId, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Returned when an error occurs, which could be a client problem or a server
        /// problem. Most errors are recoverable and the session will stay open, we
        /// recommend to implementors to monitor and log error messages by default.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="error"> Details of the error. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventError"/> instance for mocking. </returns>
        public static ServerEventError ServerEventError(string eventId = default, ServerEventErrorError error = default)
        {
            return new ServerEventError(ServerEventType.Error, eventId, additionalBinaryDataProperties: null, error);
        }

        /// <summary> The ServerEventErrorError. </summary>
        /// <param name="type"> The type of error (e.g., "invalid_request_error", "server_error"). </param>
        /// <param name="code"> Error code, if any. </param>
        /// <param name="message"> A human-readable error message. </param>
        /// <param name="param"> Parameter related to the error, if any. </param>
        /// <param name="eventId"> The event_id of the client event that caused the error, if applicable. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventErrorError"/> instance for mocking. </returns>
        public static ServerEventErrorError ServerEventErrorError(string @type = default, string code = default, string message = default, string @param = default, string eventId = default)
        {
            return new ServerEventErrorError(
                @type,
                code,
                message,
                @param,
                eventId,
                additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Returned when a Session is created. Emitted automatically when a new
        /// connection is established as the first server event. This event will contain
        /// the default Session configuration.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="session"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventSessionCreated"/> instance for mocking. </returns>
        public static ServerEventSessionCreated ServerEventSessionCreated(string eventId = default, ResponseSession session = default)
        {
            return new ServerEventSessionCreated(ServerEventType.SessionCreated, eventId, additionalBinaryDataProperties: null, session);
        }

        /// <summary> The ResponseSession. </summary>
        /// <param name="id"></param>
        /// <param name="model"></param>
        /// <param name="modalities"></param>
        /// <param name="instructions"></param>
        /// <param name="animation"></param>
        /// <param name="voiceInternal">
        /// Gets the Voice.
        ///      To assign an object to this property use .  To assign an already formatted json string to this property use . 
        ///     Supported types:
        ///     . . . . . 
        ///     Examples:
        ///      BinaryData.FromObjectAsJson("foo").  Creates a payload of "foo".  BinaryData.FromString("\"foo\"").  Creates a payload of "foo".  BinaryData.FromObjectAsJson(new { key = "value" }).  Creates a payload of { "key": "value" }.  BinaryData.FromString("{\"key\": \"value\"}").  Creates a payload of { "key": "value" }.
        /// </param>
        /// <param name="inputAudio"></param>
        /// <param name="inputAudioFormat"></param>
        /// <param name="outputAudioFormat"></param>
        /// <param name="inputAudioSamplingRate"></param>
        /// <param name="turnDetection"></param>
        /// <param name="inputAudioNoiseReduction"></param>
        /// <param name="inputAudioEchoCancellation"></param>
        /// <param name="avatar"></param>
        /// <param name="inputAudioTranscription"></param>
        /// <param name="outputAudioTimestampTypes"></param>
        /// <param name="tools"></param>
        /// <param name="toolChoice"></param>
        /// <param name="temperature"></param>
        /// <param name="maxResponseOutputTokens"></param>
        /// <param name="agent"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseSession"/> instance for mocking. </returns>
        public static ResponseSession ResponseSession(string id = default, string model = default, IEnumerable<InputModality> modalities = default, string instructions = default, AnimationOptions animation = default, BinaryData voiceInternal = default, InputAudio inputAudio = default, AudioFormat? inputAudioFormat = default, AudioFormat? outputAudioFormat = default, int? inputAudioSamplingRate = default, TurnDetection turnDetection = default, AudioNoiseReduction inputAudioNoiseReduction = default, AudioEchoCancellation inputAudioEchoCancellation = default, AvatarConfig avatar = default, AudioInputTranscriptionSettings inputAudioTranscription = default, IEnumerable<AudioTimestampType> outputAudioTimestampTypes = default, IEnumerable<VoiceLiveToolDefinition> tools = default, BinaryData toolChoice = default, float? temperature = default, BinaryData maxResponseOutputTokens = default, RespondingAgentConfig agent = default)
        {
            modalities ??= new ChangeTrackingList<InputModality>();
            outputAudioTimestampTypes ??= new ChangeTrackingList<AudioTimestampType>();
            tools ??= new ChangeTrackingList<VoiceLiveToolDefinition>();

            return new ResponseSession(
                id,
                model,
                modalities.ToList(),
                instructions,
                animation,
                voiceInternal,
                inputAudio,
                inputAudioFormat,
                outputAudioFormat,
                inputAudioSamplingRate,
                turnDetection,
                inputAudioNoiseReduction,
                inputAudioEchoCancellation,
                avatar,
                inputAudioTranscription,
                outputAudioTimestampTypes.ToList(),
                tools.ToList(),
                toolChoice,
                temperature,
                maxResponseOutputTokens,
                agent,
                additionalBinaryDataProperties: null);
        }

        /// <summary> The RespondingAgentConfig. </summary>
        /// <param name="type"></param>
        /// <param name="name"></param>
        /// <param name="description"></param>
        /// <param name="agentId"></param>
        /// <param name="threadId"></param>
        /// <returns> A new <see cref="VoiceLive.RespondingAgentConfig"/> instance for mocking. </returns>
        public static RespondingAgentConfig RespondingAgentConfig(string @type = default, string name = default, string description = default, string agentId = default, string threadId = default)
        {
            return new RespondingAgentConfig(
                @type,
                name,
                description,
                agentId,
                threadId,
                additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Returned when a session is updated with a `session.update` event, unless
        /// there is an error.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="session"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventSessionUpdated"/> instance for mocking. </returns>
        public static ServerEventSessionUpdated ServerEventSessionUpdated(string eventId = default, ResponseSession session = default)
        {
            return new ServerEventSessionUpdated(ServerEventType.SessionUpdated, eventId, additionalBinaryDataProperties: null, session);
        }

        /// <summary>
        /// Returned when an input audio buffer is committed, either by the client or
        /// automatically in server VAD mode. The `item_id` property is the ID of the user
        /// message item that will be created, thus a `conversation.item.created` event
        /// will also be sent to the client.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="previousItemId"> The ID of the preceding item after which the new item will be inserted. </param>
        /// <param name="itemId"> The ID of the user message item that will be created. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventInputAudioBufferCommitted"/> instance for mocking. </returns>
        public static ServerEventInputAudioBufferCommitted ServerEventInputAudioBufferCommitted(string eventId = default, string previousItemId = default, string itemId = default)
        {
            return new ServerEventInputAudioBufferCommitted(ServerEventType.InputAudioBufferCommitted, eventId, additionalBinaryDataProperties: null, previousItemId, itemId);
        }

        /// <summary>
        /// Returned when the input audio buffer is cleared by the client with a
        /// `input_audio_buffer.clear` event.
        /// </summary>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventInputAudioBufferCleared"/> instance for mocking. </returns>
        public static ServerEventInputAudioBufferCleared ServerEventInputAudioBufferCleared(string eventId = default)
        {
            return new ServerEventInputAudioBufferCleared(ServerEventType.InputAudioBufferCleared, eventId, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Sent by the server when in `server_vad` mode to indicate that speech has been
        /// detected in the audio buffer. This can happen any time audio is added to the
        /// buffer (unless speech is already detected). The client may want to use this
        /// event to interrupt audio playback or provide visual feedback to the user.
        /// 
        /// The client should expect to receive a `input_audio_buffer.speech_stopped` event
        /// when speech stops. The `item_id` property is the ID of the user message item
        /// that will be created when speech stops and will also be included in the
        /// `input_audio_buffer.speech_stopped` event (unless the client manually commits
        /// the audio buffer during VAD activation).
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="audioStartMs">
        /// Milliseconds from the start of all audio written to the buffer during the
        /// session when speech was first detected. This will correspond to the
        /// beginning of audio sent to the model, and thus includes the
        /// `prefix_padding_ms` configured in the Session.
        /// </param>
        /// <param name="itemId"> The ID of the user message item that will be created when speech stops. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventInputAudioBufferSpeechStarted"/> instance for mocking. </returns>
        public static ServerEventInputAudioBufferSpeechStarted ServerEventInputAudioBufferSpeechStarted(string eventId = default, int audioStartMs = default, string itemId = default)
        {
            return new ServerEventInputAudioBufferSpeechStarted(ServerEventType.InputAudioBufferSpeechStarted, eventId, additionalBinaryDataProperties: null, audioStartMs, itemId);
        }

        /// <summary>
        /// Returned in `server_vad` mode when the server detects the end of speech in
        /// the audio buffer. The server will also send an `conversation.item.created`
        /// event with the user message item that is created from the audio buffer.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="audioEndMs">
        /// Milliseconds since the session started when speech stopped. This will
        /// correspond to the end of audio sent to the model, and thus includes the
        /// `min_silence_duration_ms` configured in the Session.
        /// </param>
        /// <param name="itemId"> The ID of the user message item that will be created. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventInputAudioBufferSpeechStopped"/> instance for mocking. </returns>
        public static ServerEventInputAudioBufferSpeechStopped ServerEventInputAudioBufferSpeechStopped(string eventId = default, int audioEndMs = default, string itemId = default)
        {
            return new ServerEventInputAudioBufferSpeechStopped(ServerEventType.InputAudioBufferSpeechStopped, eventId, additionalBinaryDataProperties: null, audioEndMs, itemId);
        }

        /// <summary>
        /// Returned when a conversation item is created. There are several scenarios that produce this event:
        ///   - The server is generating a Response, which if successful will produce
        ///     either one or two Items, which will be of type `message`
        ///     (role `assistant`) or type `function_call`.
        ///   - The input audio buffer has been committed, either by the client or the
        ///     server (in `server_vad` mode). The server will take the content of the
        ///     input audio buffer and add it to a new user message Item.
        ///   - The client has sent a `conversation.item.create` event to add a new Item
        ///     to the Conversation.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="previousItemId">
        /// The ID of the preceding item in the Conversation context, allows the
        /// client to understand the order of the conversation.
        /// </param>
        /// <param name="item"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventConversationItemCreated"/> instance for mocking. </returns>
        public static ServerEventConversationItemCreated ServerEventConversationItemCreated(string eventId = default, string previousItemId = default, ResponseItem item = default)
        {
            return new ServerEventConversationItemCreated(ServerEventType.ConversationItemCreated, eventId, additionalBinaryDataProperties: null, previousItemId, item);
        }

        /// <summary>
        /// The ResponseItem.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="ResponseMessageItem"/>, <see cref="ResponseFunctionCallItem"/>, and <see cref="ResponseFunctionCallOutputItem"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <param name="id"></param>
        /// <param name="object"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseItem"/> instance for mocking. </returns>
        public static ResponseItem ResponseItem(string @type = default, string id = default, string @object = default)
        {
            return new UnknownResponseItem(new ItemType(@type), id, @object, additionalBinaryDataProperties: null);
        }

        /// <summary> The ResponseMessageItem. </summary>
        /// <param name="id"></param>
        /// <param name="object"></param>
        /// <param name="role"></param>
        /// <param name="content"></param>
        /// <param name="status"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseMessageItem"/> instance for mocking. </returns>
        public static ResponseMessageItem ResponseMessageItem(string id = default, string @object = default, ResponseMessageRole role = default, IEnumerable<VoiceLiveContentPart> content = default, ResponseItemStatus status = default)
        {
            content ??= new ChangeTrackingList<VoiceLiveContentPart>();

            return new ResponseMessageItem(
                ItemType.Message,
                id,
                @object,
                additionalBinaryDataProperties: null,
                role,
                content.ToList(),
                status);
        }

        /// <summary>
        /// The VoiceLiveContentPart.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="RequestTextContentPart"/>, <see cref="RequestAudioContentPart"/>, <see cref="ResponseTextContentPart"/>, and <see cref="ResponseAudioContentPart"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <returns> A new <see cref="VoiceLive.VoiceLiveContentPart"/> instance for mocking. </returns>
        public static VoiceLiveContentPart VoiceLiveContentPart(string @type = default)
        {
            return new UnknownVoiceLiveContentPart(new ContentPartType(@type), additionalBinaryDataProperties: null);
        }

        /// <summary> The RequestTextContentPart. </summary>
        /// <param name="text"></param>
        /// <returns> A new <see cref="VoiceLive.RequestTextContentPart"/> instance for mocking. </returns>
        public static RequestTextContentPart RequestTextContentPart(string text = default)
        {
            return new RequestTextContentPart(ContentPartType.InputText, additionalBinaryDataProperties: null, text);
        }

        /// <summary> The RequestAudioContentPart. </summary>
        /// <param name="transcript"></param>
        /// <returns> A new <see cref="VoiceLive.RequestAudioContentPart"/> instance for mocking. </returns>
        public static RequestAudioContentPart RequestAudioContentPart(string transcript = default)
        {
            return new RequestAudioContentPart(ContentPartType.InputAudio, additionalBinaryDataProperties: null, transcript);
        }

        /// <summary> The ResponseTextContentPart. </summary>
        /// <param name="text"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseTextContentPart"/> instance for mocking. </returns>
        public static ResponseTextContentPart ResponseTextContentPart(string text = default)
        {
            return new ResponseTextContentPart(ContentPartType.Text, additionalBinaryDataProperties: null, text);
        }

        /// <summary> The ResponseAudioContentPart. </summary>
        /// <param name="transcript"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseAudioContentPart"/> instance for mocking. </returns>
        public static ResponseAudioContentPart ResponseAudioContentPart(string transcript = default)
        {
            return new ResponseAudioContentPart(ContentPartType.Audio, additionalBinaryDataProperties: null, transcript);
        }

        /// <summary> The ResponseFunctionCallItem. </summary>
        /// <param name="id"></param>
        /// <param name="object"></param>
        /// <param name="name"></param>
        /// <param name="callId"></param>
        /// <param name="arguments"></param>
        /// <param name="status"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseFunctionCallItem"/> instance for mocking. </returns>
        public static ResponseFunctionCallItem ResponseFunctionCallItem(string id = default, string @object = default, string name = default, string callId = default, string arguments = default, ResponseItemStatus status = default)
        {
            return new ResponseFunctionCallItem(
                ItemType.FunctionCall,
                id,
                @object,
                additionalBinaryDataProperties: null,
                name,
                callId,
                arguments,
                status);
        }

        /// <summary> The ResponseFunctionCallOutputItem. </summary>
        /// <param name="id"></param>
        /// <param name="object"></param>
        /// <param name="callId"></param>
        /// <param name="output"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseFunctionCallOutputItem"/> instance for mocking. </returns>
        public static ResponseFunctionCallOutputItem ResponseFunctionCallOutputItem(string id = default, string @object = default, string callId = default, string output = default)
        {
            return new ResponseFunctionCallOutputItem(
                ItemType.FunctionCallOutput,
                id,
                @object,
                additionalBinaryDataProperties: null,
                callId,
                output);
        }

        /// <summary>
        /// This event is the output of audio transcription for user audio written to the
        /// user audio buffer. Transcription begins when the input audio buffer is
        /// committed by the client or server (in `server_vad` mode). Transcription runs
        /// asynchronously with Response creation, so this event may come before or after
        /// the Response events.
        /// 
        /// VoiceLive API models accept audio natively, and thus input transcription is a
        /// separate process run on a separate ASR (Automatic Speech Recognition) model.
        /// The transcript may diverge somewhat from the model's interpretation, and
        /// should be treated as a rough guide.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="itemId"> The ID of the user message item containing the audio. </param>
        /// <param name="contentIndex"> The index of the content part containing the audio. </param>
        /// <param name="transcript"> The transcribed text. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventConversationItemInputAudioTranscriptionCompleted"/> instance for mocking. </returns>
        public static ServerEventConversationItemInputAudioTranscriptionCompleted ServerEventConversationItemInputAudioTranscriptionCompleted(string eventId = default, string itemId = default, int contentIndex = default, string transcript = default)
        {
            return new ServerEventConversationItemInputAudioTranscriptionCompleted(
                ServerEventType.ConversationItemInputAudioTranscriptionCompleted,
                eventId,
                additionalBinaryDataProperties: null,
                itemId,
                contentIndex,
                transcript);
        }

        /// <summary>
        /// Returned when input audio transcription is configured, and a transcription
        /// request for a user message failed. These events are separate from other
        /// `error` events so that the client can identify the related Item.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="itemId"> The ID of the user message item. </param>
        /// <param name="contentIndex"> The index of the content part containing the audio. </param>
        /// <param name="error"> Details of the transcription error. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventConversationItemInputAudioTranscriptionFailed"/> instance for mocking. </returns>
        public static ServerEventConversationItemInputAudioTranscriptionFailed ServerEventConversationItemInputAudioTranscriptionFailed(string eventId = default, string itemId = default, int contentIndex = default, VoiceLiveErrorDetails error = default)
        {
            return new ServerEventConversationItemInputAudioTranscriptionFailed(
                ServerEventType.ConversationItemInputAudioTranscriptionFailed,
                eventId,
                additionalBinaryDataProperties: null,
                itemId,
                contentIndex,
                error);
        }

        /// <summary> Error object returned in case of API failure. </summary>
        /// <param name="code"> Error code, or null if unspecified. </param>
        /// <param name="message"> Human-readable error message. </param>
        /// <param name="param"> Parameter name related to the error, if applicable. </param>
        /// <param name="type"> Type or category of the error. </param>
        /// <param name="eventId"> Event id of the error. </param>
        /// <returns> A new <see cref="VoiceLive.VoiceLiveErrorDetails"/> instance for mocking. </returns>
        public static VoiceLiveErrorDetails VoiceLiveErrorDetails(string code = default, string message = default, string @param = default, string @type = default, string eventId = default)
        {
            return new VoiceLiveErrorDetails(
                code,
                message,
                @param,
                @type,
                eventId,
                additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Returned when an earlier assistant audio message item is truncated by the
        /// client with a `conversation.item.truncate` event. This event is used to
        /// synchronize the server's understanding of the audio with the client's playback.
        /// 
        /// This action will truncate the audio and remove the server-side text transcript
        /// to ensure there is no text in the context that hasn't been heard by the user.
        /// </summary>
        /// <param name="itemId"> The ID of the assistant message item that was truncated. </param>
        /// <param name="contentIndex"> The index of the content part that was truncated. </param>
        /// <param name="audioEndMs"> The duration up to which the audio was truncated, in milliseconds. </param>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventConversationItemTruncated"/> instance for mocking. </returns>
        public static ServerEventConversationItemTruncated ServerEventConversationItemTruncated(string itemId = default, int contentIndex = default, int audioEndMs = default, string eventId = default)
        {
            return new ServerEventConversationItemTruncated(
                ServerEventType.ConversationItemTruncated,
                additionalBinaryDataProperties: null,
                itemId,
                contentIndex,
                audioEndMs,
                eventId);
        }

        /// <summary>
        /// Returned when an item in the conversation is deleted by the client with a
        /// `conversation.item.delete` event. This event is used to synchronize the
        /// server's understanding of the conversation history with the client's view.
        /// </summary>
        /// <param name="itemId"> The ID of the item that was deleted. </param>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventConversationItemDeleted"/> instance for mocking. </returns>
        public static ServerEventConversationItemDeleted ServerEventConversationItemDeleted(string itemId = default, string eventId = default)
        {
            return new ServerEventConversationItemDeleted(ServerEventType.ConversationItemDeleted, additionalBinaryDataProperties: null, itemId, eventId);
        }

        /// <summary>
        /// Returned when a new Response is created. The first event of response creation,
        /// where the response is in an initial state of `in_progress`.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="response"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseCreated"/> instance for mocking. </returns>
        public static ServerEventResponseCreated ServerEventResponseCreated(string eventId = default, VoiceLiveResponse response = default)
        {
            return new ServerEventResponseCreated(ServerEventType.ResponseCreated, eventId, additionalBinaryDataProperties: null, response);
        }

        /// <summary> The response resource. </summary>
        /// <param name="id"> The unique ID of the response. </param>
        /// <param name="object"> The object type, must be `realtime.response`. </param>
        /// <param name="status">
        /// The final status of the response.
        /// One of: `completed`, `cancelled`, `failed`, `incomplete`, or `in_progress`.
        /// </param>
        /// <param name="statusDetails"> Additional details about the status. </param>
        /// <param name="output"> The list of output items generated by the response. </param>
        /// <param name="usage">
        /// Usage statistics for the Response, this will correspond to billing. A
        /// VoiceLive API session will maintain a conversation context and append new
        /// Items to the Conversation, thus output from previous turns (text and
        /// audio tokens) will become the input for later turns.
        /// </param>
        /// <param name="conversationId">
        /// Which conversation the response is added to, determined by the `conversation`
        /// field in the `response.create` event. If `auto`, the response will be added to
        /// the default conversation and the value of `conversation_id` will be an id like
        /// `conv_1234`. If `none`, the response will not be added to any conversation and
        /// the value of `conversation_id` will be `null`. If responses are being triggered
        /// by server VAD, the response will be added to the default conversation, thus
        /// the `conversation_id` will be an id like `conv_1234`.
        /// </param>
        /// <param name="voiceInternal">
        /// supported voice identifiers and configurations.
        ///      To assign an object to this property use .  To assign an already formatted json string to this property use . 
        ///     Supported types:
        ///     . . . . . 
        ///     Examples:
        ///      BinaryData.FromObjectAsJson("foo").  Creates a payload of "foo".  BinaryData.FromString("\"foo\"").  Creates a payload of "foo".  BinaryData.FromObjectAsJson(new { key = "value" }).  Creates a payload of { "key": "value" }.  BinaryData.FromString("{\"key\": \"value\"}").  Creates a payload of { "key": "value" }.
        /// </param>
        /// <param name="modalities">
        /// The set of modalities the model used to respond. If there are multiple modalities,
        /// the model will pick one, for example if `modalities` is `["text", "audio"]`, the model
        /// could be responding in either text or audio.
        /// </param>
        /// <param name="outputAudioFormat"> The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`. </param>
        /// <param name="temperature"> Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8. </param>
        /// <param name="maxOutputTokens">
        /// Maximum number of output tokens for a single assistant response,
        /// inclusive of tool calls, that was used in this response.
        /// </param>
        /// <returns> A new <see cref="VoiceLive.VoiceLiveResponse"/> instance for mocking. </returns>
        public static VoiceLiveResponse VoiceLiveResponse(string id = default, string @object = default, VoiceLiveResponseStatus? status = default, ResponseStatusDetails statusDetails = default, IEnumerable<ResponseItem> output = default, ResponseTokenStatistics usage = default, string conversationId = default, BinaryData voiceInternal = default, IEnumerable<ResponseModality> modalities = default, ResponseOutputAudioFormat? outputAudioFormat = default, float? temperature = default, BinaryData maxOutputTokens = default)
        {
            output ??= new ChangeTrackingList<ResponseItem>();
            modalities ??= new ChangeTrackingList<ResponseModality>();

            return new VoiceLiveResponse(
                id,
                @object,
                status,
                statusDetails,
                output.ToList(),
                usage,
                conversationId,
                voiceInternal,
                modalities.ToList(),
                outputAudioFormat,
                temperature,
                maxOutputTokens,
                additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Base for all non-success response details.
        /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="ResponseCancelledDetails"/>, <see cref="ResponseIncompleteDetails"/>, and <see cref="ResponseFailedDetails"/>.
        /// </summary>
        /// <param name="type"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseStatusDetails"/> instance for mocking. </returns>
        public static ResponseStatusDetails ResponseStatusDetails(string @type = default)
        {
            return new UnknownResponseStatusDetails(@type, additionalBinaryDataProperties: null);
        }

        /// <summary> Details for a cancelled response. </summary>
        /// <param name="reason"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseCancelledDetails"/> instance for mocking. </returns>
        public static ResponseCancelledDetails ResponseCancelledDetails(ResponseCancelledDetailsReason reason = default)
        {
            return new ResponseCancelledDetails("cancelled", additionalBinaryDataProperties: null, reason);
        }

        /// <summary> Details for an incomplete response. </summary>
        /// <param name="reason"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseIncompleteDetails"/> instance for mocking. </returns>
        public static ResponseIncompleteDetails ResponseIncompleteDetails(ResponseIncompleteDetailsReason reason = default)
        {
            return new ResponseIncompleteDetails("incomplete", additionalBinaryDataProperties: null, reason);
        }

        /// <summary> Details for a failed response. </summary>
        /// <param name="error"></param>
        /// <returns> A new <see cref="VoiceLive.ResponseFailedDetails"/> instance for mocking. </returns>
        public static ResponseFailedDetails ResponseFailedDetails(BinaryData error = default)
        {
            return new ResponseFailedDetails("failed", additionalBinaryDataProperties: null, error);
        }

        /// <summary> Overall usage statistics for a response. </summary>
        /// <param name="totalTokens"> Total number of tokens (input + output). </param>
        /// <param name="inputTokens"> Number of input tokens. </param>
        /// <param name="outputTokens"> Number of output tokens. </param>
        /// <param name="inputTokenDetails"> Detailed breakdown of input tokens. </param>
        /// <param name="outputTokenDetails"> Detailed breakdown of output tokens. </param>
        /// <returns> A new <see cref="VoiceLive.ResponseTokenStatistics"/> instance for mocking. </returns>
        public static ResponseTokenStatistics ResponseTokenStatistics(int totalTokens = default, int inputTokens = default, int outputTokens = default, InputTokenDetails inputTokenDetails = default, OutputTokenDetails outputTokenDetails = default)
        {
            return new ResponseTokenStatistics(
                totalTokens,
                inputTokens,
                outputTokens,
                inputTokenDetails,
                outputTokenDetails,
                additionalBinaryDataProperties: null);
        }

        /// <summary> Details of input token usage. </summary>
        /// <param name="cachedTokens"> Number of cached tokens used in the input. </param>
        /// <param name="textTokens"> Number of text tokens used in the input. </param>
        /// <param name="audioTokens"> Number of audio tokens used in the input. </param>
        /// <returns> A new <see cref="VoiceLive.InputTokenDetails"/> instance for mocking. </returns>
        public static InputTokenDetails InputTokenDetails(int cachedTokens = default, int textTokens = default, int audioTokens = default)
        {
            return new InputTokenDetails(cachedTokens, textTokens, audioTokens, additionalBinaryDataProperties: null);
        }

        /// <summary> Details of output token usage. </summary>
        /// <param name="textTokens"> Number of text tokens generated in the output. </param>
        /// <param name="audioTokens"> Number of audio tokens generated in the output. </param>
        /// <returns> A new <see cref="VoiceLive.OutputTokenDetails"/> instance for mocking. </returns>
        public static OutputTokenDetails OutputTokenDetails(int textTokens = default, int audioTokens = default)
        {
            return new OutputTokenDetails(textTokens, audioTokens, additionalBinaryDataProperties: null);
        }

        /// <summary>
        /// Returned when a Response is done streaming. Always emitted, no matter the
        /// final state. The Response object included in the `response.done` event will
        /// include all output Items in the Response but will omit the raw audio data.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="response"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseDone"/> instance for mocking. </returns>
        public static ServerEventResponseDone ServerEventResponseDone(string eventId = default, VoiceLiveResponse response = default)
        {
            return new ServerEventResponseDone(ServerEventType.ResponseDone, eventId, additionalBinaryDataProperties: null, response);
        }

        /// <summary> Returned when a new Item is created during Response generation. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the Response to which the item belongs. </param>
        /// <param name="outputIndex"> The index of the output item in the Response. </param>
        /// <param name="item"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseOutputItemAdded"/> instance for mocking. </returns>
        public static ServerEventResponseOutputItemAdded ServerEventResponseOutputItemAdded(string eventId = default, string responseId = default, int outputIndex = default, ResponseItem item = default)
        {
            return new ServerEventResponseOutputItemAdded(
                ServerEventType.ResponseOutputItemAdded,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                outputIndex,
                item);
        }

        /// <summary>
        /// Returned when an Item is done streaming. Also emitted when a Response is
        /// interrupted, incomplete, or cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the Response to which the item belongs. </param>
        /// <param name="outputIndex"> The index of the output item in the Response. </param>
        /// <param name="item"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseOutputItemDone"/> instance for mocking. </returns>
        public static ServerEventResponseOutputItemDone ServerEventResponseOutputItemDone(string eventId = default, string responseId = default, int outputIndex = default, ResponseItem item = default)
        {
            return new ServerEventResponseOutputItemDone(
                ServerEventType.ResponseOutputItemDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                outputIndex,
                item);
        }

        /// <summary>
        /// Returned when a new content part is added to an assistant message item during
        /// response generation.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item to which the content part was added. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="part"> The content part that was added. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseContentPartAdded"/> instance for mocking. </returns>
        public static ServerEventResponseContentPartAdded ServerEventResponseContentPartAdded(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, VoiceLiveContentPart part = default)
        {
            return new ServerEventResponseContentPartAdded(
                ServerEventType.ResponseContentPartAdded,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                part);
        }

        /// <summary>
        /// Returned when a content part is done streaming in an assistant message item.
        /// Also emitted when a Response is interrupted, incomplete, or cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="part"> The content part that is done. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseContentPartDone"/> instance for mocking. </returns>
        public static ServerEventResponseContentPartDone ServerEventResponseContentPartDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, VoiceLiveContentPart part = default)
        {
            return new ServerEventResponseContentPartDone(
                ServerEventType.ResponseContentPartDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                part);
        }

        /// <summary> Returned when the text value of a "text" content part is updated. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="delta"> The text delta. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseTextDelta"/> instance for mocking. </returns>
        public static ServerEventResponseTextDelta ServerEventResponseTextDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, string delta = default)
        {
            return new ServerEventResponseTextDelta(
                ServerEventType.ResponseTextDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                delta);
        }

        /// <summary>
        /// Returned when the text value of a "text" content part is done streaming. Also
        /// emitted when a Response is interrupted, incomplete, or cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="text"> The final text content. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseTextDone"/> instance for mocking. </returns>
        public static ServerEventResponseTextDone ServerEventResponseTextDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, string text = default)
        {
            return new ServerEventResponseTextDone(
                ServerEventType.ResponseTextDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                text);
        }

        /// <summary> Returned when the model-generated transcription of audio output is updated. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="delta"> The transcript delta. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseAudioTranscriptDelta"/> instance for mocking. </returns>
        public static ServerEventResponseAudioTranscriptDelta ServerEventResponseAudioTranscriptDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, string delta = default)
        {
            return new ServerEventResponseAudioTranscriptDelta(
                ServerEventType.ResponseAudioTranscriptDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                delta);
        }

        /// <summary>
        /// Returned when the model-generated transcription of audio output is done
        /// streaming. Also emitted when a Response is interrupted, incomplete, or
        /// cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="transcript"> The final transcript of the audio. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseAudioTranscriptDone"/> instance for mocking. </returns>
        public static ServerEventResponseAudioTranscriptDone ServerEventResponseAudioTranscriptDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, string transcript = default)
        {
            return new ServerEventResponseAudioTranscriptDone(
                ServerEventType.ResponseAudioTranscriptDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                transcript);
        }

        /// <summary> Returned when the model-generated audio is updated. </summary>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="delta"> Base64-encoded audio data delta. </param>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseAudioDelta"/> instance for mocking. </returns>
        public static ServerEventResponseAudioDelta ServerEventResponseAudioDelta(string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, BinaryData delta = default, string eventId = default)
        {
            return new ServerEventResponseAudioDelta(
                ServerEventType.ResponseAudioDelta,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                delta,
                eventId);
        }

        /// <summary>
        /// Returned when the model-generated audio is done. Also emitted when a Response
        /// is interrupted, incomplete, or cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseAudioDone"/> instance for mocking. </returns>
        public static ServerEventResponseAudioDone ServerEventResponseAudioDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default)
        {
            return new ServerEventResponseAudioDone(
                ServerEventType.ResponseAudioDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex);
        }

        /// <summary> Represents a delta update of blendshape animation frames for a specific output of a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <param name="frames"></param>
        /// <param name="frameIndex"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseAnimationBlendshapeDelta"/> instance for mocking. </returns>
        public static ServerEventResponseAnimationBlendshapeDelta ServerEventResponseAnimationBlendshapeDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, BinaryData frames = default, int frameIndex = default)
        {
            return new ServerEventResponseAnimationBlendshapeDelta(
                ServerEventType.ResponseAnimationBlendshapesDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                frames,
                frameIndex);
        }

        /// <summary> Indicates the completion of blendshape animation processing for a specific output of a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseAnimationBlendshapeDone"/> instance for mocking. </returns>
        public static ServerEventResponseAnimationBlendshapeDone ServerEventResponseAnimationBlendshapeDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default)
        {
            return new ServerEventResponseAnimationBlendshapeDone(
                ServerEventType.ResponseAnimationBlendshapesDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex);
        }

        /// <summary> Represents an emotion hypothesis detected from response audio with multiple candidates. </summary>
        /// <param name="eventId"></param>
        /// <param name="emotion"></param>
        /// <param name="candidates"></param>
        /// <param name="audioOffsetMs"></param>
        /// <param name="audioDurationMs"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseEmotionHypothesis"/> instance for mocking. </returns>
        public static ServerEventResponseEmotionHypothesis ServerEventResponseEmotionHypothesis(string eventId = default, string emotion = default, IEnumerable<EmotionCandidate> candidates = default, int audioOffsetMs = default, int audioDurationMs = default, string responseId = default, string itemId = default)
        {
            candidates ??= new ChangeTrackingList<EmotionCandidate>();

            return new ServerEventResponseEmotionHypothesis(
                ServerEventType.ResponseEmotionHypothesis,
                eventId,
                additionalBinaryDataProperties: null,
                emotion,
                candidates.ToList(),
                audioOffsetMs,
                audioDurationMs,
                responseId,
                itemId);
        }

        /// <summary> The EmotionCandidate. </summary>
        /// <param name="emotion"></param>
        /// <param name="confidence"></param>
        /// <returns> A new <see cref="VoiceLive.EmotionCandidate"/> instance for mocking. </returns>
        public static EmotionCandidate EmotionCandidate(string emotion = default, float confidence = default)
        {
            return new EmotionCandidate(emotion, confidence, additionalBinaryDataProperties: null);
        }

        /// <summary> Represents a word-level audio timestamp delta for a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <param name="audioOffsetMs"></param>
        /// <param name="audioDurationMs"></param>
        /// <param name="text"></param>
        /// <param name="timestampType"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseAudioTimestampDelta"/> instance for mocking. </returns>
        public static ServerEventResponseAudioTimestampDelta ServerEventResponseAudioTimestampDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, int audioOffsetMs = default, int audioDurationMs = default, string text = default, string timestampType = default)
        {
            return new ServerEventResponseAudioTimestampDelta(
                ServerEventType.ResponseAudioTimestampDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                audioOffsetMs,
                audioDurationMs,
                text,
                timestampType);
        }

        /// <summary> Indicates completion of audio timestamp delivery for a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseAudioTimestampDone"/> instance for mocking. </returns>
        public static ServerEventResponseAudioTimestampDone ServerEventResponseAudioTimestampDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default)
        {
            return new ServerEventResponseAudioTimestampDone(
                ServerEventType.ResponseAudioTimestampDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex);
        }

        /// <summary> Represents a viseme ID delta update for animation based on audio. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <param name="audioOffsetMs"></param>
        /// <param name="visemeId"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseAnimationVisemeDelta"/> instance for mocking. </returns>
        public static ServerEventResponseAnimationVisemeDelta ServerEventResponseAnimationVisemeDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default, int audioOffsetMs = default, int visemeId = default)
        {
            return new ServerEventResponseAnimationVisemeDelta(
                ServerEventType.ResponseAnimationVisemeDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex,
                audioOffsetMs,
                visemeId);
        }

        /// <summary> Indicates completion of viseme animation delivery for a response. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"></param>
        /// <param name="itemId"></param>
        /// <param name="outputIndex"></param>
        /// <param name="contentIndex"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseAnimationVisemeDone"/> instance for mocking. </returns>
        public static ServerEventResponseAnimationVisemeDone ServerEventResponseAnimationVisemeDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, int contentIndex = default)
        {
            return new ServerEventResponseAnimationVisemeDone(
                ServerEventType.ResponseAnimationVisemeDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                contentIndex);
        }

        /// <summary> Returned when the text value of an input audio transcription content part is updated. </summary>
        /// <param name="eventId"></param>
        /// <param name="itemId"> The ID of the item. </param>
        /// <param name="contentIndex"> The index of the content part in the item's content array. </param>
        /// <param name="delta"> The text delta. </param>
        /// <param name="logprobs"> The log probabilities of the transcription. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventConversationItemInputAudioTranscriptionDelta"/> instance for mocking. </returns>
        public static ServerEventConversationItemInputAudioTranscriptionDelta ServerEventConversationItemInputAudioTranscriptionDelta(string eventId = default, string itemId = default, int? contentIndex = default, string delta = default, IEnumerable<LogProbProperties> logprobs = default)
        {
            logprobs ??= new ChangeTrackingList<LogProbProperties>();

            return new ServerEventConversationItemInputAudioTranscriptionDelta(
                ServerEventType.ConversationItemInputAudioTranscriptionDelta,
                eventId,
                additionalBinaryDataProperties: null,
                itemId,
                contentIndex,
                delta,
                logprobs.ToList());
        }

        /// <summary> A single log probability entry for a token. </summary>
        /// <param name="token"> The token that was used to generate the log probability. </param>
        /// <param name="logprob"> The log probability of the token. </param>
        /// <param name="bytes"> The bytes that were used to generate the log probability. </param>
        /// <returns> A new <see cref="VoiceLive.LogProbProperties"/> instance for mocking. </returns>
        public static LogProbProperties LogProbProperties(string token = default, float logprob = default, IEnumerable<int> bytes = default)
        {
            bytes ??= new ChangeTrackingList<int>();

            return new LogProbProperties(token, logprob, bytes.ToList(), additionalBinaryDataProperties: null);
        }

        /// <summary> Returned when a conversation item is retrieved with `conversation.item.retrieve`. </summary>
        /// <param name="item"></param>
        /// <param name="eventId"></param>
        /// <returns> A new <see cref="VoiceLive.ServerEventConversationItemRetrieved"/> instance for mocking. </returns>
        public static ServerEventConversationItemRetrieved ServerEventConversationItemRetrieved(ResponseItem item = default, string eventId = default)
        {
            return new ServerEventConversationItemRetrieved(ServerEventType.ConversationItemRetrieved, additionalBinaryDataProperties: null, item, eventId);
        }

        /// <summary> Returned when the model-generated function call arguments are updated. </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the function call item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="callId"> The ID of the function call. </param>
        /// <param name="delta"> The arguments delta as a JSON string. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseFunctionCallArgumentsDelta"/> instance for mocking. </returns>
        public static ServerEventResponseFunctionCallArgumentsDelta ServerEventResponseFunctionCallArgumentsDelta(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, string callId = default, string delta = default)
        {
            return new ServerEventResponseFunctionCallArgumentsDelta(
                ServerEventType.ResponseFunctionCallArgumentsDelta,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                callId,
                delta);
        }

        /// <summary>
        /// Returned when the model-generated function call arguments are done streaming.
        /// Also emitted when a Response is interrupted, incomplete, or cancelled.
        /// </summary>
        /// <param name="eventId"></param>
        /// <param name="responseId"> The ID of the response. </param>
        /// <param name="itemId"> The ID of the function call item. </param>
        /// <param name="outputIndex"> The index of the output item in the response. </param>
        /// <param name="callId"> The ID of the function call. </param>
        /// <param name="arguments"> The final arguments as a JSON string. </param>
        /// <param name="name"> The name of the function call. </param>
        /// <returns> A new <see cref="VoiceLive.ServerEventResponseFunctionCallArgumentsDone"/> instance for mocking. </returns>
        public static ServerEventResponseFunctionCallArgumentsDone ServerEventResponseFunctionCallArgumentsDone(string eventId = default, string responseId = default, string itemId = default, int outputIndex = default, string callId = default, string arguments = default, string name = default)
        {
            return new ServerEventResponseFunctionCallArgumentsDone(
                ServerEventType.ResponseFunctionCallArgumentsDone,
                eventId,
                additionalBinaryDataProperties: null,
                responseId,
                itemId,
                outputIndex,
                callId,
                arguments,
                name);
        }
    }
}
