// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

// <auto-generated/>

#nullable disable

using System;
using System.ComponentModel;

namespace Azure.AI.Projects.OneDP
{
    /// <summary> Risk category for the attack objective. </summary>
    public readonly partial struct RiskCategory : IEquatable<RiskCategory>
    {
        private readonly string _value;

        /// <summary> Initializes a new instance of <see cref="RiskCategory"/>. </summary>
        /// <exception cref="ArgumentNullException"> <paramref name="value"/> is null. </exception>
        public RiskCategory(string value)
        {
            _value = value ?? throw new ArgumentNullException(nameof(value));
        }

        private const string HateUnfairnessValue = "HateUnfairness";
        private const string ViolenceValue = "Violence";
        private const string SexualValue = "Sexual";
        private const string SelfHarmValue = "SelfHarm";
        private const string ProtectedMaterialValue = "ProtectedMaterial";
        private const string CodeVulnerabilityValue = "CodeVulnerability";
        private const string UngroundedAttributesValue = "UngroundedAttributes";

        /// <summary> Represents content related to hate or unfairness. </summary>
        public static RiskCategory HateUnfairness { get; } = new RiskCategory(HateUnfairnessValue);
        /// <summary> Represents content related to violence. </summary>
        public static RiskCategory Violence { get; } = new RiskCategory(ViolenceValue);
        /// <summary> Represents content of a sexual nature. </summary>
        public static RiskCategory Sexual { get; } = new RiskCategory(SexualValue);
        /// <summary> Represents content related to self-harm. </summary>
        public static RiskCategory SelfHarm { get; } = new RiskCategory(SelfHarmValue);
        /// <summary> Represents content involving protected material. </summary>
        public static RiskCategory ProtectedMaterial { get; } = new RiskCategory(ProtectedMaterialValue);
        /// <summary> Represents content related to code vulnerabilities. </summary>
        public static RiskCategory CodeVulnerability { get; } = new RiskCategory(CodeVulnerabilityValue);
        /// <summary> Represents content with ungrounded attributes. </summary>
        public static RiskCategory UngroundedAttributes { get; } = new RiskCategory(UngroundedAttributesValue);
        /// <summary> Determines if two <see cref="RiskCategory"/> values are the same. </summary>
        public static bool operator ==(RiskCategory left, RiskCategory right) => left.Equals(right);
        /// <summary> Determines if two <see cref="RiskCategory"/> values are not the same. </summary>
        public static bool operator !=(RiskCategory left, RiskCategory right) => !left.Equals(right);
        /// <summary> Converts a <see cref="string"/> to a <see cref="RiskCategory"/>. </summary>
        public static implicit operator RiskCategory(string value) => new RiskCategory(value);

        /// <inheritdoc />
        [EditorBrowsable(EditorBrowsableState.Never)]
        public override bool Equals(object obj) => obj is RiskCategory other && Equals(other);
        /// <inheritdoc />
        public bool Equals(RiskCategory other) => string.Equals(_value, other._value, StringComparison.InvariantCultureIgnoreCase);

        /// <inheritdoc />
        [EditorBrowsable(EditorBrowsableState.Never)]
        public override int GetHashCode() => _value != null ? StringComparer.InvariantCultureIgnoreCase.GetHashCode(_value) : 0;
        /// <inheritdoc />
        public override string ToString() => _value;
    }
}
