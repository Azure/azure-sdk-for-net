// <auto-generated/>

#nullable disable

using System;
using System.Collections.Generic;
using System.Text.Json;
using OpenAI;

namespace Azure.AI.Projects
{
    internal partial class InternalPromptAgentDefinition : InternalAgentDefinition
    {
        /// <summary> Initializes a new instance of <see cref="InternalPromptAgentDefinition"/>. </summary>
        /// <param name="model"> The model deployment to use for this agent. </param>
        public InternalPromptAgentDefinition(string model) : base(AgentKind.Prompt)
        {
            Model = model;
            Tools = new ChangeTrackingList<InternalTool>();
            StructuredInputs = new ChangeTrackingDictionary<string, StructuredInputDefinition>();
        }

        /// <summary> Initializes a new instance of <see cref="InternalPromptAgentDefinition"/>. </summary>
        /// <param name="kind"></param>
        /// <param name="raiConfig"> Configuration for Responsible AI (RAI) content filtering and safety features. </param>
        /// <param name="additionalBinaryDataProperties"> Keeps track of any properties unknown to the library. </param>
        /// <param name="model"> The model deployment to use for this agent. </param>
        /// <param name="instructions"> A system (or developer) message inserted into the model's context. </param>
        /// <param name="temperature">
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        /// We generally recommend altering this or `top_p` but not both.
        /// </param>
        /// <param name="topP">
        /// An alternative to sampling with temperature, called nucleus sampling,
        /// where the model considers the results of the tokens with top_p probability
        /// mass. So 0.1 means only the tokens comprising the top 10% probability mass
        /// are considered.
        /// 
        /// We generally recommend altering this or `temperature` but not both.
        /// </param>
        /// <param name="reasoning"></param>
        /// <param name="tools">
        /// An array of tools the model may call while generating a response. You
        /// can specify which tool to use by setting the `tool_choice` parameter.
        /// </param>
        /// <param name="toolChoice">
        /// How the model should select which tool (or tools) to use when generating a response. 
        /// See the `tools` parameter to see how to specify which tools the model can call.
        /// </param>
        /// <param name="text"> Configuration options for a text response from the model. Can be plain text or structured JSON data. </param>
        /// <param name="structuredInputs"> Set of structured inputs that can participate in prompt template substitution or tool argument bindings. </param>
        internal InternalPromptAgentDefinition(AgentKind kind, RaiConfig raiConfig, IDictionary<string, BinaryData> additionalBinaryDataProperties, string model, string instructions, float? temperature, float? topP, InternalReasoning reasoning, IList<InternalTool> tools, BinaryData toolChoice, PromptAgentDefinitionText text, IDictionary<string, StructuredInputDefinition> structuredInputs) : base(kind, raiConfig, additionalBinaryDataProperties)
        {
            Model = model;
            Instructions = instructions;
            Temperature = temperature;
            TopP = topP;
            Reasoning = reasoning;
            Tools = tools;
            ToolChoice = toolChoice;
            Text = text;
            StructuredInputs = structuredInputs;
        }

        /// <summary> The model deployment to use for this agent. </summary>
        public string Model { get; set; }

        /// <summary> A system (or developer) message inserted into the model's context. </summary>
        public string Instructions { get; set; }

        /// <summary>
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        /// We generally recommend altering this or `top_p` but not both.
        /// </summary>
        public float? Temperature { get; set; }

        /// <summary>
        /// An alternative to sampling with temperature, called nucleus sampling,
        /// where the model considers the results of the tokens with top_p probability
        /// mass. So 0.1 means only the tokens comprising the top 10% probability mass
        /// are considered.
        /// 
        /// We generally recommend altering this or `temperature` but not both.
        /// </summary>
        public float? TopP { get; set; }

        /// <summary> Gets or sets the Reasoning. </summary>
        public InternalReasoning Reasoning { get; set; }

        /// <summary>
        /// An array of tools the model may call while generating a response. You
        /// can specify which tool to use by setting the `tool_choice` parameter.
        /// </summary>
        public IList<InternalTool> Tools { get; }

        /// <summary>
        /// How the model should select which tool (or tools) to use when generating a response. 
        /// See the `tools` parameter to see how to specify which tools the model can call.
        /// <para> To assign an object to this property use <see cref="BinaryData.FromObjectAsJson{T}(T, JsonSerializerOptions?)"/>. </para>
        /// <para> To assign an already formatted json string to this property use <see cref="BinaryData.FromString(string)"/>. </para>
        /// <para>
        /// <remarks>
        /// Supported types:
        /// <list type="bullet">
        /// <item>
        /// <description> <see cref="string"/>. </description>
        /// </item>
        /// <item>
        /// <description> <see cref="ToolChoiceParam"/>. </description>
        /// </item>
        /// </list>
        /// </remarks>
        /// </para>
        /// <para>
        /// Examples:
        /// <list type="bullet">
        /// <item>
        /// <term> BinaryData.FromObjectAsJson("foo"). </term>
        /// <description> Creates a payload of "foo". </description>
        /// </item>
        /// <item>
        /// <term> BinaryData.FromString("\"foo\""). </term>
        /// <description> Creates a payload of "foo". </description>
        /// </item>
        /// <item>
        /// <term> BinaryData.FromObjectAsJson(new { key = "value" }). </term>
        /// <description> Creates a payload of { "key": "value" }. </description>
        /// </item>
        /// <item>
        /// <term> BinaryData.FromString("{\"key\": \"value\"}"). </term>
        /// <description> Creates a payload of { "key": "value" }. </description>
        /// </item>
        /// </list>
        /// </para>
        /// </summary>
        public BinaryData ToolChoice { get; set; }

        /// <summary> Configuration options for a text response from the model. Can be plain text or structured JSON data. </summary>
        public PromptAgentDefinitionText Text { get; set; }

        /// <summary> Set of structured inputs that can participate in prompt template substitution or tool argument bindings. </summary>
        public IDictionary<string, StructuredInputDefinition> StructuredInputs { get; }
    }
}
