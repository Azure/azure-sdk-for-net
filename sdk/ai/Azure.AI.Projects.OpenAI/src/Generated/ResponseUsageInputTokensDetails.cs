// <auto-generated/>

#nullable disable

using System;
using System.Collections.Generic;

namespace OpenAI
{
    /// <summary> The ResponseUsageInputTokensDetails. </summary>
    internal partial class ResponseUsageInputTokensDetails
    {
        /// <summary> Keeps track of any properties unknown to the library. </summary>
        private protected readonly IDictionary<string, BinaryData> _additionalBinaryDataProperties;

        /// <summary> Initializes a new instance of <see cref="ResponseUsageInputTokensDetails"/>. </summary>
        /// <param name="cachedTokens">
        /// The number of tokens that were retrieved from the cache.
        /// [More on prompt caching](https://platform.openai.com/docs/guides/prompt-caching).
        /// </param>
        internal ResponseUsageInputTokensDetails(int cachedTokens)
        {
            CachedTokens = cachedTokens;
        }

        /// <summary> Initializes a new instance of <see cref="ResponseUsageInputTokensDetails"/>. </summary>
        /// <param name="cachedTokens">
        /// The number of tokens that were retrieved from the cache.
        /// [More on prompt caching](https://platform.openai.com/docs/guides/prompt-caching).
        /// </param>
        /// <param name="additionalBinaryDataProperties"> Keeps track of any properties unknown to the library. </param>
        internal ResponseUsageInputTokensDetails(int cachedTokens, IDictionary<string, BinaryData> additionalBinaryDataProperties)
        {
            CachedTokens = cachedTokens;
            _additionalBinaryDataProperties = additionalBinaryDataProperties;
        }

        /// <summary>
        /// The number of tokens that were retrieved from the cache.
        /// [More on prompt caching](https://platform.openai.com/docs/guides/prompt-caching).
        /// </summary>
        public int CachedTokens { get; }
    }
}
