// <auto-generated/>

#nullable disable

using System;
using System.Collections.Generic;
using System.Linq;
using System.Text.Json;
using Azure.AI.Projects.OpenAI;

namespace OpenAI
{
    internal partial class InternalAgentResponse
    {
        /// <summary> Keeps track of any properties unknown to the library. </summary>
        private protected readonly IDictionary<string, BinaryData> _additionalBinaryDataProperties;

        /// <summary> Initializes a new instance of <see cref="InternalAgentResponse"/>. </summary>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be
        /// useful for storing additional information about the object in a structured
        /// format, and querying for objects via API or the dashboard.
        /// 
        /// Keys are strings with a maximum length of 64 characters. Values are strings
        /// with a maximum length of 512 characters.
        /// </param>
        /// <param name="temperature">
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        /// We generally recommend altering this or `top_p` but not both.
        /// </param>
        /// <param name="topP">
        /// An alternative to sampling with temperature, called nucleus sampling,
        /// where the model considers the results of the tokens with top_p probability
        /// mass. So 0.1 means only the tokens comprising the top 10% probability mass
        /// are considered.
        /// 
        /// We generally recommend altering this or `temperature` but not both.
        /// </param>
        /// <param name="user"> A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more about safety best practices](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids). </param>
        /// <param name="id"> Unique identifier for this Response. </param>
        /// <param name="createdAt"> Unix timestamp (in seconds) of when this Response was created. </param>
        /// <param name="error"></param>
        /// <param name="incompleteDetails"> Details about why the response is incomplete. </param>
        /// <param name="output">
        /// An array of content items generated by the model.
        /// 
        /// - The length and order of items in the `output` array is dependent
        ///   on the model's response.
        /// - Rather than accessing the first item in the `output` array and
        ///   assuming it's an `assistant` message with the content generated by
        ///   the model, you might consider using the `output_text` property where
        ///   supported in SDKs.
        /// </param>
        /// <param name="instructions">
        /// A system (or developer) message inserted into the model's context.
        /// 
        /// When using along with `previous_response_id`, the instructions from a previous
        /// response will not be carried over to the next response. This makes it simple
        /// to swap out system (or developer) messages in new responses.
        /// </param>
        /// <param name="parallelToolCalls"> Whether to allow the model to run tool calls in parallel. </param>
        /// <param name="conversation"></param>
        internal InternalAgentResponse(IDictionary<string, string> metadata, float? temperature, float? topP, string user, string id, DateTimeOffset createdAt, InternalAgentResponseError error, ResponseIncompleteDetails1 incompleteDetails, IEnumerable<AgentResponseItem> output, BinaryData instructions, bool parallelToolCalls, ResponseConversation1 conversation)
        {
            Metadata = metadata;
            Temperature = temperature;
            TopP = topP;
            User = user;
            Tools = new ChangeTrackingList<AgentTool>();
            Id = id;
            CreatedAt = createdAt;
            Error = error;
            IncompleteDetails = incompleteDetails;
            Output = output.ToList();
            Instructions = instructions;
            ParallelToolCalls = parallelToolCalls;
            Conversation = conversation;
            StructuredInputs = new ChangeTrackingDictionary<string, BinaryData>();
        }

        /// <summary> Initializes a new instance of <see cref="InternalAgentResponse"/>. </summary>
        /// <param name="metadata">
        /// Set of 16 key-value pairs that can be attached to an object. This can be
        /// useful for storing additional information about the object in a structured
        /// format, and querying for objects via API or the dashboard.
        /// 
        /// Keys are strings with a maximum length of 64 characters. Values are strings
        /// with a maximum length of 512 characters.
        /// </param>
        /// <param name="temperature">
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        /// We generally recommend altering this or `top_p` but not both.
        /// </param>
        /// <param name="topP">
        /// An alternative to sampling with temperature, called nucleus sampling,
        /// where the model considers the results of the tokens with top_p probability
        /// mass. So 0.1 means only the tokens comprising the top 10% probability mass
        /// are considered.
        /// 
        /// We generally recommend altering this or `temperature` but not both.
        /// </param>
        /// <param name="user"> A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more about safety best practices](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids). </param>
        /// <param name="serviceTier"> Note: service_tier is not applicable to Azure OpenAI. </param>
        /// <param name="topLogprobs"> An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. </param>
        /// <param name="previousResponseId">
        /// The unique ID of the previous response to the model. Use this to
        /// create multi-turn conversations. Learn more about
        /// [managing conversation state](https://platform.openai.com/docs/guides/conversation-state).
        /// </param>
        /// <param name="model"> The model deployment to use for the creation of this response. </param>
        /// <param name="reasoning"></param>
        /// <param name="background">
        /// Whether to run the model response in the background.
        /// [Learn more about background responses](https://platform.openai.com/docs/guides/background).
        /// </param>
        /// <param name="maxOutputTokens"> An upper bound for the number of tokens that can be generated for a response, including visible output tokens and [reasoning tokens](https://platform.openai.com/docs/guides/reasoning). </param>
        /// <param name="maxToolCalls"> The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored. </param>
        /// <param name="text">
        /// Configuration options for a text response from the model. Can be plain
        /// text or structured JSON data. See [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
        /// and [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
        /// </param>
        /// <param name="tools">
        /// An array of tools the model may call while generating a response. You
        /// can specify which tool to use by setting the `tool_choice` parameter.
        /// 
        /// The two categories of tools you can provide the model are:
        /// 
        /// * **Built-in tools**: Tools that are provided by OpenAI that extend the
        ///   model's capabilities, like [web search](https://platform.openai.com/docs/guides/tools-web-search)
        ///   or [file search](https://platform.openai.com/docs/guides/tools-file-search). Learn more about
        ///   [built-in tools](https://platform.openai.com/docs/guides/tools).
        /// * **Function calls (custom tools)**: Functions that are defined by you,
        ///   enabling the model to call your own code. Learn more about
        ///   [function calling](https://platform.openai.com/docs/guides/function-calling).
        /// 
        /// </param>
        /// <param name="toolChoice">
        /// How the model should select which tool (or tools) to use when generating
        /// a response. See the `tools` parameter to see how to specify which tools
        /// the model can call.
        /// </param>
        /// <param name="prompt"></param>
        /// <param name="truncation">
        /// The truncation strategy to use for the model response.
        /// - `auto`: If the context of this response and previous ones exceeds
        ///   the model's context window size, the model will truncate the
        ///   response to fit the context window by dropping input items in the
        ///   middle of the conversation.
        /// - `disabled` (default): If a model response will exceed the context window
        ///   size for a model, the request will fail with a 400 error.
        /// </param>
        /// <param name="id"> Unique identifier for this Response. </param>
        /// <param name="object"> The object type of this resource - always set to `response`. </param>
        /// <param name="status">
        /// The status of the response generation. One of `completed`, `failed`,
        /// `in_progress`, `cancelled`, `queued`, or `incomplete`.
        /// </param>
        /// <param name="createdAt"> Unix timestamp (in seconds) of when this Response was created. </param>
        /// <param name="error"></param>
        /// <param name="incompleteDetails"> Details about why the response is incomplete. </param>
        /// <param name="output">
        /// An array of content items generated by the model.
        /// 
        /// - The length and order of items in the `output` array is dependent
        ///   on the model's response.
        /// - Rather than accessing the first item in the `output` array and
        ///   assuming it's an `assistant` message with the content generated by
        ///   the model, you might consider using the `output_text` property where
        ///   supported in SDKs.
        /// </param>
        /// <param name="instructions">
        /// A system (or developer) message inserted into the model's context.
        /// 
        /// When using along with `previous_response_id`, the instructions from a previous
        /// response will not be carried over to the next response. This makes it simple
        /// to swap out system (or developer) messages in new responses.
        /// </param>
        /// <param name="outputText">
        /// SDK-only convenience property that contains the aggregated text output
        /// from all `output_text` items in the `output` array, if any are present.
        /// Supported in the Python and JavaScript SDKs.
        /// </param>
        /// <param name="usage"></param>
        /// <param name="parallelToolCalls"> Whether to allow the model to run tool calls in parallel. </param>
        /// <param name="conversation"></param>
        /// <param name="agent"> The agent used for this response. </param>
        /// <param name="structuredInputs"> The structured inputs to the response that can participate in prompt template substitution or tool argument bindings. </param>
        /// <param name="additionalBinaryDataProperties"> Keeps track of any properties unknown to the library. </param>
        internal InternalAgentResponse(IDictionary<string, string> metadata, float? temperature, float? topP, string user, ServiceTier? serviceTier, int? topLogprobs, string previousResponseId, string model, InternalReasoning reasoning, bool? background, int? maxOutputTokens, int? maxToolCalls, ResponseText text, IList<AgentTool> tools, BinaryData toolChoice, Prompt prompt, ResponseTruncation? truncation, string id, string @object, InternalAgentResponseStatus? status, DateTimeOffset createdAt, InternalAgentResponseError error, ResponseIncompleteDetails1 incompleteDetails, IList<AgentResponseItem> output, BinaryData instructions, string outputText, ResponseUsage usage, bool parallelToolCalls, ResponseConversation1 conversation, AgentInfo agent, IDictionary<string, BinaryData> structuredInputs, IDictionary<string, BinaryData> additionalBinaryDataProperties)
        {
            Metadata = metadata;
            Temperature = temperature;
            TopP = topP;
            User = user;
            ServiceTier = serviceTier;
            TopLogprobs = topLogprobs;
            PreviousResponseId = previousResponseId;
            Model = model;
            Reasoning = reasoning;
            Background = background;
            MaxOutputTokens = maxOutputTokens;
            MaxToolCalls = maxToolCalls;
            Text = text;
            Tools = tools;
            ToolChoice = toolChoice;
            Prompt = prompt;
            Truncation = truncation;
            Id = id;
            Object = @object;
            Status = status;
            CreatedAt = createdAt;
            Error = error;
            IncompleteDetails = incompleteDetails;
            Output = output;
            Instructions = instructions;
            OutputText = outputText;
            Usage = usage;
            ParallelToolCalls = parallelToolCalls;
            Conversation = conversation;
            Agent = agent;
            StructuredInputs = structuredInputs;
            _additionalBinaryDataProperties = additionalBinaryDataProperties;
        }

        /// <summary>
        /// Set of 16 key-value pairs that can be attached to an object. This can be
        /// useful for storing additional information about the object in a structured
        /// format, and querying for objects via API or the dashboard.
        /// 
        /// Keys are strings with a maximum length of 64 characters. Values are strings
        /// with a maximum length of 512 characters.
        /// </summary>
        public IDictionary<string, string> Metadata { get; }

        /// <summary>
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        /// We generally recommend altering this or `top_p` but not both.
        /// </summary>
        public float? Temperature { get; }

        /// <summary>
        /// An alternative to sampling with temperature, called nucleus sampling,
        /// where the model considers the results of the tokens with top_p probability
        /// mass. So 0.1 means only the tokens comprising the top 10% probability mass
        /// are considered.
        /// 
        /// We generally recommend altering this or `temperature` but not both.
        /// </summary>
        public float? TopP { get; }

        /// <summary> A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more about safety best practices](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids). </summary>
        public string User { get; }

        /// <summary> Note: service_tier is not applicable to Azure OpenAI. </summary>
        public ServiceTier? ServiceTier { get; }

        /// <summary> An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. </summary>
        public int? TopLogprobs { get; }

        /// <summary>
        /// The unique ID of the previous response to the model. Use this to
        /// create multi-turn conversations. Learn more about
        /// [managing conversation state](https://platform.openai.com/docs/guides/conversation-state).
        /// </summary>
        public string PreviousResponseId { get; }

        /// <summary> The model deployment to use for the creation of this response. </summary>
        public string Model { get; }

        /// <summary> Gets the Reasoning. </summary>
        public InternalReasoning Reasoning { get; }

        /// <summary>
        /// Whether to run the model response in the background.
        /// [Learn more about background responses](https://platform.openai.com/docs/guides/background).
        /// </summary>
        public bool? Background { get; }

        /// <summary> An upper bound for the number of tokens that can be generated for a response, including visible output tokens and [reasoning tokens](https://platform.openai.com/docs/guides/reasoning). </summary>
        public int? MaxOutputTokens { get; }

        /// <summary> The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored. </summary>
        public int? MaxToolCalls { get; }

        /// <summary>
        /// Configuration options for a text response from the model. Can be plain
        /// text or structured JSON data. See [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
        /// and [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
        /// </summary>
        public ResponseText Text { get; }

        /// <summary>
        /// An array of tools the model may call while generating a response. You
        /// can specify which tool to use by setting the `tool_choice` parameter.
        /// 
        /// The two categories of tools you can provide the model are:
        /// 
        /// * **Built-in tools**: Tools that are provided by OpenAI that extend the
        ///   model's capabilities, like [web search](https://platform.openai.com/docs/guides/tools-web-search)
        ///   or [file search](https://platform.openai.com/docs/guides/tools-file-search). Learn more about
        ///   [built-in tools](https://platform.openai.com/docs/guides/tools).
        /// * **Function calls (custom tools)**: Functions that are defined by you,
        ///   enabling the model to call your own code. Learn more about
        ///   [function calling](https://platform.openai.com/docs/guides/function-calling).
        /// 
        /// </summary>
        public IList<AgentTool> Tools { get; }

        /// <summary>
        /// How the model should select which tool (or tools) to use when generating
        /// a response. See the `tools` parameter to see how to specify which tools
        /// the model can call.
        /// <para> To assign an object to this property use <see cref="BinaryData.FromObjectAsJson{T}(T, JsonSerializerOptions?)"/>. </para>
        /// <para> To assign an already formatted json string to this property use <see cref="BinaryData.FromString(string)"/>. </para>
        /// <para>
        /// <remarks>
        /// Supported types:
        /// <list type="bullet">
        /// <item>
        /// <description> <see cref="ToolChoiceOptions"/>. </description>
        /// </item>
        /// <item>
        /// <description> <see cref="InternalToolChoiceObject"/>. </description>
        /// </item>
        /// </list>
        /// </remarks>
        /// </para>
        /// <para>
        /// Examples:
        /// <list type="bullet">
        /// <item>
        /// <term> BinaryData.FromObjectAsJson("foo"). </term>
        /// <description> Creates a payload of "foo". </description>
        /// </item>
        /// <item>
        /// <term> BinaryData.FromString("\"foo\""). </term>
        /// <description> Creates a payload of "foo". </description>
        /// </item>
        /// <item>
        /// <term> BinaryData.FromObjectAsJson(new { key = "value" }). </term>
        /// <description> Creates a payload of { "key": "value" }. </description>
        /// </item>
        /// <item>
        /// <term> BinaryData.FromString("{\"key\": \"value\"}"). </term>
        /// <description> Creates a payload of { "key": "value" }. </description>
        /// </item>
        /// </list>
        /// </para>
        /// </summary>
        public BinaryData ToolChoice { get; }

        /// <summary> Gets the Prompt. </summary>
        public Prompt Prompt { get; }

        /// <summary>
        /// The truncation strategy to use for the model response.
        /// - `auto`: If the context of this response and previous ones exceeds
        ///   the model's context window size, the model will truncate the
        ///   response to fit the context window by dropping input items in the
        ///   middle of the conversation.
        /// - `disabled` (default): If a model response will exceed the context window
        ///   size for a model, the request will fail with a 400 error.
        /// </summary>
        public ResponseTruncation? Truncation { get; }

        /// <summary> Unique identifier for this Response. </summary>
        public string Id { get; }

        /// <summary> The object type of this resource - always set to `response`. </summary>
        internal string Object { get; } = "response";

        /// <summary>
        /// The status of the response generation. One of `completed`, `failed`,
        /// `in_progress`, `cancelled`, `queued`, or `incomplete`.
        /// </summary>
        public InternalAgentResponseStatus? Status { get; }

        /// <summary> Unix timestamp (in seconds) of when this Response was created. </summary>
        public DateTimeOffset CreatedAt { get; }

        /// <summary> Gets the Error. </summary>
        public InternalAgentResponseError Error { get; }

        /// <summary> Details about why the response is incomplete. </summary>
        public ResponseIncompleteDetails1 IncompleteDetails { get; }

        /// <summary>
        /// An array of content items generated by the model.
        /// 
        /// - The length and order of items in the `output` array is dependent
        ///   on the model's response.
        /// - Rather than accessing the first item in the `output` array and
        ///   assuming it's an `assistant` message with the content generated by
        ///   the model, you might consider using the `output_text` property where
        ///   supported in SDKs.
        /// </summary>
        public IList<AgentResponseItem> Output { get; }

        /// <summary>
        /// A system (or developer) message inserted into the model's context.
        /// 
        /// When using along with `previous_response_id`, the instructions from a previous
        /// response will not be carried over to the next response. This makes it simple
        /// to swap out system (or developer) messages in new responses.
        /// <para> To assign an object to this property use <see cref="BinaryData.FromObjectAsJson{T}(T, JsonSerializerOptions?)"/>. </para>
        /// <para> To assign an already formatted json string to this property use <see cref="BinaryData.FromString(string)"/>. </para>
        /// <para>
        /// <remarks>
        /// Supported types:
        /// <list type="bullet">
        /// <item>
        /// <description> <see cref="string"/>. </description>
        /// </item>
        /// <item>
        /// <description> <see cref="IList{T}"/> where <c>T</c> is of type <see cref="InternalItemParam"/>. </description>
        /// </item>
        /// </list>
        /// </remarks>
        /// </para>
        /// <para>
        /// Examples:
        /// <list type="bullet">
        /// <item>
        /// <term> BinaryData.FromObjectAsJson("foo"). </term>
        /// <description> Creates a payload of "foo". </description>
        /// </item>
        /// <item>
        /// <term> BinaryData.FromString("\"foo\""). </term>
        /// <description> Creates a payload of "foo". </description>
        /// </item>
        /// <item>
        /// <term> BinaryData.FromObjectAsJson(new { key = "value" }). </term>
        /// <description> Creates a payload of { "key": "value" }. </description>
        /// </item>
        /// <item>
        /// <term> BinaryData.FromString("{\"key\": \"value\"}"). </term>
        /// <description> Creates a payload of { "key": "value" }. </description>
        /// </item>
        /// </list>
        /// </para>
        /// </summary>
        public BinaryData Instructions { get; }

        /// <summary>
        /// SDK-only convenience property that contains the aggregated text output
        /// from all `output_text` items in the `output` array, if any are present.
        /// Supported in the Python and JavaScript SDKs.
        /// </summary>
        public string OutputText { get; }

        /// <summary> Gets the Usage. </summary>
        public ResponseUsage Usage { get; }

        /// <summary> Whether to allow the model to run tool calls in parallel. </summary>
        public bool ParallelToolCalls { get; }

        /// <summary> Gets the Conversation. </summary>
        public ResponseConversation1 Conversation { get; }

        /// <summary> The agent used for this response. </summary>
        public AgentInfo Agent { get; }

        /// <summary>
        /// The structured inputs to the response that can participate in prompt template substitution or tool argument bindings.
        /// <para> To assign an object to the value of this property use <see cref="BinaryData.FromObjectAsJson{T}(T, JsonSerializerOptions?)"/>. </para>
        /// <para> To assign an already formatted json string to this property use <see cref="BinaryData.FromString(string)"/>. </para>
        /// <para>
        /// Examples:
        /// <list type="bullet">
        /// <item>
        /// <term> BinaryData.FromObjectAsJson("foo"). </term>
        /// <description> Creates a payload of "foo". </description>
        /// </item>
        /// <item>
        /// <term> BinaryData.FromString("\"foo\""). </term>
        /// <description> Creates a payload of "foo". </description>
        /// </item>
        /// <item>
        /// <term> BinaryData.FromObjectAsJson(new { key = "value" }). </term>
        /// <description> Creates a payload of { "key": "value" }. </description>
        /// </item>
        /// <item>
        /// <term> BinaryData.FromString("{\"key\": \"value\"}"). </term>
        /// <description> Creates a payload of { "key": "value" }. </description>
        /// </item>
        /// </list>
        /// </para>
        /// </summary>
        public IDictionary<string, BinaryData> StructuredInputs { get; }
    }
}
