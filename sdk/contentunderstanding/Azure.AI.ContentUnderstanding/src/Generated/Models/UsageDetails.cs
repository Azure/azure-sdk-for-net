// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

// <auto-generated/>

#nullable disable

using System;
using System.Collections.Generic;

namespace Azure.AI.ContentUnderstanding
{
    /// <summary> Usage details. </summary>
    internal partial class UsageDetails
    {
        /// <summary> Keeps track of any properties unknown to the library. </summary>
        private protected readonly IDictionary<string, BinaryData> _additionalBinaryDataProperties;

        /// <summary> Initializes a new instance of <see cref="UsageDetails"/>. </summary>
        internal UsageDetails()
        {
            Tokens = new ChangeTrackingDictionary<string, int>();
        }

        /// <summary> Initializes a new instance of <see cref="UsageDetails"/>. </summary>
        /// <param name="documentPagesMinimal">
        /// The number of document pages processed at the minimal level.
        /// For documents without explicit pages (ex. txt, html), every 3000 UTF-16 characters is counted as one page.
        /// </param>
        /// <param name="documentPagesBasic">
        /// The number of document pages processed at the basic level.
        /// For documents without explicit pages (ex. txt, html), every 3000 UTF-16 characters is counted as one page.
        /// </param>
        /// <param name="documentPagesStandard">
        /// The number of document pages processed at the standard level.
        /// For documents without explicit pages (ex. txt, html), every 3000 UTF-16 characters is counted as one page.
        /// </param>
        /// <param name="audioHours"> The hours of audio processed. </param>
        /// <param name="videoHours"> The hours of video processed. </param>
        /// <param name="contextualizationTokens"> The number of contextualization tokens consumed for preparing context, generating confidence scores, source grounding, and output formatting. </param>
        /// <param name="tokens"> The number of LLM and embedding tokens consumed, grouped by model (ex. GTP 4.1) and type (ex. input, cached input, output). </param>
        /// <param name="additionalBinaryDataProperties"> Keeps track of any properties unknown to the library. </param>
        internal UsageDetails(int? documentPagesMinimal, int? documentPagesBasic, int? documentPagesStandard, float? audioHours, float? videoHours, int? contextualizationTokens, IDictionary<string, int> tokens, IDictionary<string, BinaryData> additionalBinaryDataProperties)
        {
            DocumentPagesMinimal = documentPagesMinimal;
            DocumentPagesBasic = documentPagesBasic;
            DocumentPagesStandard = documentPagesStandard;
            AudioHours = audioHours;
            VideoHours = videoHours;
            ContextualizationTokens = contextualizationTokens;
            Tokens = tokens;
            _additionalBinaryDataProperties = additionalBinaryDataProperties;
        }

        /// <summary>
        /// The number of document pages processed at the minimal level.
        /// For documents without explicit pages (ex. txt, html), every 3000 UTF-16 characters is counted as one page.
        /// </summary>
        public int? DocumentPagesMinimal { get; }

        /// <summary>
        /// The number of document pages processed at the basic level.
        /// For documents without explicit pages (ex. txt, html), every 3000 UTF-16 characters is counted as one page.
        /// </summary>
        public int? DocumentPagesBasic { get; }

        /// <summary>
        /// The number of document pages processed at the standard level.
        /// For documents without explicit pages (ex. txt, html), every 3000 UTF-16 characters is counted as one page.
        /// </summary>
        public int? DocumentPagesStandard { get; }

        /// <summary> The hours of audio processed. </summary>
        public float? AudioHours { get; }

        /// <summary> The hours of video processed. </summary>
        public float? VideoHours { get; }

        /// <summary> The number of contextualization tokens consumed for preparing context, generating confidence scores, source grounding, and output formatting. </summary>
        public int? ContextualizationTokens { get; }

        /// <summary> The number of LLM and embedding tokens consumed, grouped by model (ex. GTP 4.1) and type (ex. input, cached input, output). </summary>
        public IDictionary<string, int> Tokens { get; }
    }
}
