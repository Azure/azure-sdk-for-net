// <auto-generated>
// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License. See License.txt in the project root for
// license information.
//
// Code generated by Microsoft (R) AutoRest Code Generator.
// Changes may cause incorrect behavior and will be lost if the code is
// regenerated.
// </auto-generated>

namespace Microsoft.Azure.Management.DataFactory.Models
{
    using Microsoft.Rest;
    using Microsoft.Rest.Serialization;
    using Newtonsoft.Json;
    using System.Collections;
    using System.Collections.Generic;
    using System.Linq;

    /// <summary>
    /// Execute spark job activity.
    /// </summary>
    [Newtonsoft.Json.JsonObject("SparkJob")]
    [Rest.Serialization.JsonTransformation]
    public partial class SynapseSparkJobDefinitionActivity : ExecutionActivity
    {
        /// <summary>
        /// Initializes a new instance of the SynapseSparkJobDefinitionActivity
        /// class.
        /// </summary>
        public SynapseSparkJobDefinitionActivity()
        {
            SparkJob = new SynapseSparkJobReference();
            CustomInit();
        }

        /// <summary>
        /// Initializes a new instance of the SynapseSparkJobDefinitionActivity
        /// class.
        /// </summary>
        /// <param name="name">Activity name.</param>
        /// <param name="sparkJob">Synapse spark job reference.</param>
        /// <param name="additionalProperties">Unmatched properties from the
        /// message are deserialized this collection</param>
        /// <param name="description">Activity description.</param>
        /// <param name="dependsOn">Activity depends on condition.</param>
        /// <param name="userProperties">Activity user properties.</param>
        /// <param name="linkedServiceName">Linked service reference.</param>
        /// <param name="policy">Activity policy.</param>
        /// <param name="arguments">User specified arguments to
        /// SynapseSparkJobDefinitionActivity.</param>
        /// <param name="file">The main file used for the job, which will
        /// override the 'file' of the spark job definition you provide. Type:
        /// string (or Expression with resultType string).</param>
        /// <param name="scanFolder">Scanning subfolders from the root folder
        /// of the main definition file, these files will be added as reference
        /// files. The folders named 'jars', 'pyFiles', 'files' or 'archives'
        /// will be scanned, and the folders name are case sensitive. Type:
        /// boolean (or Expression with resultType boolean).</param>
        /// <param name="className">The fully-qualified identifier or the main
        /// class that is in the main definition file, which will override the
        /// 'className' of the spark job definition you provide. Type: string
        /// (or Expression with resultType string).</param>
        /// <param name="files">(Deprecated. Please use pythonCodeReference and
        /// filesV2) Additional files used for reference in the main definition
        /// file, which will override the 'files' of the spark job definition
        /// you provide.</param>
        /// <param name="pythonCodeReference">Additional python code files used
        /// for reference in the main definition file, which will override the
        /// 'pyFiles' of the spark job definition you provide.</param>
        /// <param name="filesV2">Additional files used for reference in the
        /// main definition file, which will override the 'jars' and 'files' of
        /// the spark job definition you provide.</param>
        /// <param name="targetBigDataPool">The name of the big data pool which
        /// will be used to execute the spark batch job, which will override
        /// the 'targetBigDataPool' of the spark job definition you
        /// provide.</param>
        /// <param name="executorSize">Number of core and memory to be used for
        /// executors allocated in the specified Spark pool for the job, which
        /// will be used for overriding 'executorCores' and 'executorMemory' of
        /// the spark job definition you provide. Type: string (or Expression
        /// with resultType string).</param>
        /// <param name="conf">Spark configuration properties, which will
        /// override the 'conf' of the spark job definition you
        /// provide.</param>
        /// <param name="driverSize">Number of core and memory to be used for
        /// driver allocated in the specified Spark pool for the job, which
        /// will be used for overriding 'driverCores' and 'driverMemory' of the
        /// spark job definition you provide. Type: string (or Expression with
        /// resultType string).</param>
        /// <param name="numExecutors">Number of executors to launch for this
        /// job, which will override the 'numExecutors' of the spark job
        /// definition you provide. Type: integer (or Expression with
        /// resultType integer).</param>
        /// <param name="configurationType">The type of the spark config.
        /// Possible values include: 'Default', 'Customized',
        /// 'Artifact'</param>
        /// <param name="targetSparkConfiguration">The spark configuration of
        /// the spark job.</param>
        /// <param name="sparkConfig">Spark configuration property.</param>
        public SynapseSparkJobDefinitionActivity(string name, SynapseSparkJobReference sparkJob, IDictionary<string, object> additionalProperties = default(IDictionary<string, object>), string description = default(string), IList<ActivityDependency> dependsOn = default(IList<ActivityDependency>), IList<UserProperty> userProperties = default(IList<UserProperty>), LinkedServiceReference linkedServiceName = default(LinkedServiceReference), ActivityPolicy policy = default(ActivityPolicy), IList<object> arguments = default(IList<object>), object file = default(object), object scanFolder = default(object), object className = default(object), IList<object> files = default(IList<object>), IList<object> pythonCodeReference = default(IList<object>), IList<object> filesV2 = default(IList<object>), BigDataPoolParametrizationReference targetBigDataPool = default(BigDataPoolParametrizationReference), object executorSize = default(object), object conf = default(object), object driverSize = default(object), object numExecutors = default(object), string configurationType = default(string), SparkConfigurationParametrizationReference targetSparkConfiguration = default(SparkConfigurationParametrizationReference), IDictionary<string, object> sparkConfig = default(IDictionary<string, object>))
            : base(name, additionalProperties, description, dependsOn, userProperties, linkedServiceName, policy)
        {
            SparkJob = sparkJob;
            Arguments = arguments;
            File = file;
            ScanFolder = scanFolder;
            ClassName = className;
            Files = files;
            PythonCodeReference = pythonCodeReference;
            FilesV2 = filesV2;
            TargetBigDataPool = targetBigDataPool;
            ExecutorSize = executorSize;
            Conf = conf;
            DriverSize = driverSize;
            NumExecutors = numExecutors;
            ConfigurationType = configurationType;
            TargetSparkConfiguration = targetSparkConfiguration;
            SparkConfig = sparkConfig;
            CustomInit();
        }

        /// <summary>
        /// An initialization method that performs custom operations like setting defaults
        /// </summary>
        partial void CustomInit();

        /// <summary>
        /// Gets or sets synapse spark job reference.
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.sparkJob")]
        public SynapseSparkJobReference SparkJob { get; set; }

        /// <summary>
        /// Gets or sets user specified arguments to
        /// SynapseSparkJobDefinitionActivity.
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.args")]
        public IList<object> Arguments { get; set; }

        /// <summary>
        /// Gets or sets the main file used for the job, which will override
        /// the 'file' of the spark job definition you provide. Type: string
        /// (or Expression with resultType string).
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.file")]
        public object File { get; set; }

        /// <summary>
        /// Gets or sets scanning subfolders from the root folder of the main
        /// definition file, these files will be added as reference files. The
        /// folders named 'jars', 'pyFiles', 'files' or 'archives' will be
        /// scanned, and the folders name are case sensitive. Type: boolean (or
        /// Expression with resultType boolean).
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.scanFolder")]
        public object ScanFolder { get; set; }

        /// <summary>
        /// Gets or sets the fully-qualified identifier or the main class that
        /// is in the main definition file, which will override the 'className'
        /// of the spark job definition you provide. Type: string (or
        /// Expression with resultType string).
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.className")]
        public object ClassName { get; set; }

        /// <summary>
        /// Gets or sets (Deprecated. Please use pythonCodeReference and
        /// filesV2) Additional files used for reference in the main definition
        /// file, which will override the 'files' of the spark job definition
        /// you provide.
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.files")]
        public IList<object> Files { get; set; }

        /// <summary>
        /// Gets or sets additional python code files used for reference in the
        /// main definition file, which will override the 'pyFiles' of the
        /// spark job definition you provide.
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.pythonCodeReference")]
        public IList<object> PythonCodeReference { get; set; }

        /// <summary>
        /// Gets or sets additional files used for reference in the main
        /// definition file, which will override the 'jars' and 'files' of the
        /// spark job definition you provide.
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.filesV2")]
        public IList<object> FilesV2 { get; set; }

        /// <summary>
        /// Gets or sets the name of the big data pool which will be used to
        /// execute the spark batch job, which will override the
        /// 'targetBigDataPool' of the spark job definition you provide.
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.targetBigDataPool")]
        public BigDataPoolParametrizationReference TargetBigDataPool { get; set; }

        /// <summary>
        /// Gets or sets number of core and memory to be used for executors
        /// allocated in the specified Spark pool for the job, which will be
        /// used for overriding 'executorCores' and 'executorMemory' of the
        /// spark job definition you provide. Type: string (or Expression with
        /// resultType string).
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.executorSize")]
        public object ExecutorSize { get; set; }

        /// <summary>
        /// Gets or sets spark configuration properties, which will override
        /// the 'conf' of the spark job definition you provide.
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.conf")]
        public object Conf { get; set; }

        /// <summary>
        /// Gets or sets number of core and memory to be used for driver
        /// allocated in the specified Spark pool for the job, which will be
        /// used for overriding 'driverCores' and 'driverMemory' of the spark
        /// job definition you provide. Type: string (or Expression with
        /// resultType string).
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.driverSize")]
        public object DriverSize { get; set; }

        /// <summary>
        /// Gets or sets number of executors to launch for this job, which will
        /// override the 'numExecutors' of the spark job definition you
        /// provide. Type: integer (or Expression with resultType integer).
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.numExecutors")]
        public object NumExecutors { get; set; }

        /// <summary>
        /// Gets or sets the type of the spark config. Possible values include:
        /// 'Default', 'Customized', 'Artifact'
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.configurationType")]
        public string ConfigurationType { get; set; }

        /// <summary>
        /// Gets or sets the spark configuration of the spark job.
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.targetSparkConfiguration")]
        public SparkConfigurationParametrizationReference TargetSparkConfiguration { get; set; }

        /// <summary>
        /// Gets or sets spark configuration property.
        /// </summary>
        [JsonProperty(PropertyName = "typeProperties.sparkConfig")]
        public IDictionary<string, object> SparkConfig { get; set; }

        /// <summary>
        /// Validate the object.
        /// </summary>
        /// <exception cref="ValidationException">
        /// Thrown if validation fails
        /// </exception>
        public override void Validate()
        {
            base.Validate();
            if (SparkJob == null)
            {
                throw new ValidationException(ValidationRules.CannotBeNull, "SparkJob");
            }
            if (SparkJob != null)
            {
                SparkJob.Validate();
            }
            if (TargetBigDataPool != null)
            {
                TargetBigDataPool.Validate();
            }
            if (TargetSparkConfiguration != null)
            {
                TargetSparkConfiguration.Validate();
            }
        }
    }
}
