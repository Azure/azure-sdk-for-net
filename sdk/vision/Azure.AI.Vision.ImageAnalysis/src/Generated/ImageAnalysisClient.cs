// <auto-generated/>

#nullable disable

using System;
using System.ClientModel;
using System.ClientModel.Internal;
using System.ClientModel.Primitives;
using System.ClientModel.Primitives.Pipeline;
using System.Collections.Generic;
using System.Threading;
using System.Threading.Tasks;

namespace Azure.AI.Vision.ImageAnalysis
{
    // Data plane generated client.
    /// <summary> The ImageAnalysis service client. </summary>
    public partial class ImageAnalysisClient
    {
        private const string AuthorizationHeader = "Ocp-Apim-Subscription-Key";
        private readonly KeyCredential _keyCredential;
        private readonly MessagePipeline _pipeline;
        private readonly Uri _endpoint;
        private readonly string _apiVersion;

        /// <summary> The ClientDiagnostics is used to provide tracing support for the client library. </summary>
        internal TelemetrySource ClientDiagnostics { get; }

        /// <summary> The HTTP pipeline for sending and receiving REST requests and responses. </summary>
        public virtual MessagePipeline Pipeline => _pipeline;

        /// <summary> Initializes a new instance of ImageAnalysisClient for mocking. </summary>
        protected ImageAnalysisClient()
        {
        }

        /// <summary> Initializes a new instance of ImageAnalysisClient. </summary>
        /// <param name="endpoint">
        /// Azure AI Computer Vision endpoint (protocol and hostname, for example:
        /// https://&lt;resource-name&gt;.cognitiveservices.azure.com).
        /// </param>
        /// <param name="credential"> A credential used to authenticate to an Azure Service. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="endpoint"/> or <paramref name="credential"/> is null. </exception>
        public ImageAnalysisClient(Uri endpoint, KeyCredential credential) : this(endpoint, credential, new ImageAnalysisClientOptions())
        {
        }

        /// <summary> Initializes a new instance of ImageAnalysisClient. </summary>
        /// <param name="endpoint">
        /// Azure AI Computer Vision endpoint (protocol and hostname, for example:
        /// https://&lt;resource-name&gt;.cognitiveservices.azure.com).
        /// </param>
        /// <param name="credential"> A credential used to authenticate to an Azure Service. </param>
        /// <param name="options"> The options for configuring the client. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="endpoint"/> or <paramref name="credential"/> is null. </exception>
        public ImageAnalysisClient(Uri endpoint, KeyCredential credential, ImageAnalysisClientOptions options)
        {
            Argument.AssertNotNull(endpoint, nameof(endpoint));
            Argument.AssertNotNull(credential, nameof(credential));
            options ??= new ImageAnalysisClientOptions();

            ClientDiagnostics = new TelemetrySource(options, true);
            _keyCredential = credential;
            _pipeline = MessagePipeline.Create(options, new IPipelinePolicy<PipelineMessage>[] { new KeyCredentialPolicy(_keyCredential, AuthorizationHeader) }, Array.Empty<IPipelinePolicy<PipelineMessage>>());
            _endpoint = endpoint;
            _apiVersion = options.Version;
        }

        /// <summary> Performs a single Image Analysis operation. </summary>
        /// <param name="visualFeatures">
        /// A list of visual features to analyze.
        /// Seven visual features are supported: Caption, DenseCaptions, Read (OCR), Tags, Objects, SmartCrops, and People.
        /// At least one visual feature must be specified.
        /// </param>
        /// <param name="imageContent"> The image to be analyzed. </param>
        /// <param name="language">
        /// The desired language for result generation (a two-letter language code).
        /// If this option is not specified, the default value 'en' is used (English).
        /// See https://aka.ms/cv-languages for a list of supported languages.
        /// </param>
        /// <param name="genderNeutralCaption">
        /// Boolean flag for enabling gender-neutral captioning for Caption and Dense Captions features.
        /// By default captions may contain gender terms (for example: 'man', 'woman', or 'boy', 'girl').
        /// If you set this to "true", those will be replaced with gender-neutral terms (for example: 'person' or 'child').
        /// </param>
        /// <param name="smartCropsAspectRatios">
        /// A list of aspect ratios to use for smart cropping.
        /// Aspect ratios are calculated by dividing the target crop width in pixels by the height in pixels.
        /// Supported values are between 0.75 and 1.8 (inclusive).
        /// If this parameter is not specified, the service will return one crop region with an aspect
        /// ratio it sees fit between 0.5 and 2.0 (inclusive).
        /// </param>
        /// <param name="modelVersion">
        /// The version of cloud AI-model used for analysis.
        /// The format is the following: 'latest' (default value) or 'YYYY-MM-DD' or 'YYYY-MM-DD-preview', where 'YYYY', 'MM', 'DD' are the year, month and day associated with the model.
        /// This is not commonly set, as the default always gives the latest AI model with recent improvements.
        /// If however you would like to make sure analysis results do not change over time, set this value to a specific model version.
        /// </param>
        /// <param name="cancellationToken"> The cancellation token to use. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="visualFeatures"/> or <paramref name="imageContent"/> is null. </exception>
        internal virtual async Task<Result<ImageAnalysisResult>> AnalyzeFromImageDataAsync(IEnumerable<VisualFeaturesImpl> visualFeatures, BinaryData imageContent, string language = null, bool? genderNeutralCaption = null, IEnumerable<float> smartCropsAspectRatios = null, string modelVersion = null, CancellationToken cancellationToken = default)
        {
            Argument.AssertNotNull(visualFeatures, nameof(visualFeatures));
            Argument.AssertNotNull(imageContent, nameof(imageContent));

            RequestOptions context = FromCancellationToken(cancellationToken);
            using RequestBody content = imageContent;
            Result result = await AnalyzeFromImageDataAsync(visualFeatures, content, language, genderNeutralCaption, smartCropsAspectRatios, modelVersion, context).ConfigureAwait(false);
            return Result.FromValue(ImageAnalysisResult.FromResponse(result.GetRawResponse()), result.GetRawResponse());
        }

        /// <summary> Performs a single Image Analysis operation. </summary>
        /// <param name="visualFeatures">
        /// A list of visual features to analyze.
        /// Seven visual features are supported: Caption, DenseCaptions, Read (OCR), Tags, Objects, SmartCrops, and People.
        /// At least one visual feature must be specified.
        /// </param>
        /// <param name="imageContent"> The image to be analyzed. </param>
        /// <param name="language">
        /// The desired language for result generation (a two-letter language code).
        /// If this option is not specified, the default value 'en' is used (English).
        /// See https://aka.ms/cv-languages for a list of supported languages.
        /// </param>
        /// <param name="genderNeutralCaption">
        /// Boolean flag for enabling gender-neutral captioning for Caption and Dense Captions features.
        /// By default captions may contain gender terms (for example: 'man', 'woman', or 'boy', 'girl').
        /// If you set this to "true", those will be replaced with gender-neutral terms (for example: 'person' or 'child').
        /// </param>
        /// <param name="smartCropsAspectRatios">
        /// A list of aspect ratios to use for smart cropping.
        /// Aspect ratios are calculated by dividing the target crop width in pixels by the height in pixels.
        /// Supported values are between 0.75 and 1.8 (inclusive).
        /// If this parameter is not specified, the service will return one crop region with an aspect
        /// ratio it sees fit between 0.5 and 2.0 (inclusive).
        /// </param>
        /// <param name="modelVersion">
        /// The version of cloud AI-model used for analysis.
        /// The format is the following: 'latest' (default value) or 'YYYY-MM-DD' or 'YYYY-MM-DD-preview', where 'YYYY', 'MM', 'DD' are the year, month and day associated with the model.
        /// This is not commonly set, as the default always gives the latest AI model with recent improvements.
        /// If however you would like to make sure analysis results do not change over time, set this value to a specific model version.
        /// </param>
        /// <param name="cancellationToken"> The cancellation token to use. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="visualFeatures"/> or <paramref name="imageContent"/> is null. </exception>
        internal virtual Result<ImageAnalysisResult> AnalyzeFromImageData(IEnumerable<VisualFeaturesImpl> visualFeatures, BinaryData imageContent, string language = null, bool? genderNeutralCaption = null, IEnumerable<float> smartCropsAspectRatios = null, string modelVersion = null, CancellationToken cancellationToken = default)
        {
            Argument.AssertNotNull(visualFeatures, nameof(visualFeatures));
            Argument.AssertNotNull(imageContent, nameof(imageContent));

            RequestOptions context = FromCancellationToken(cancellationToken);
            using RequestBody content = imageContent;
            Result result = AnalyzeFromImageData(visualFeatures, content, language, genderNeutralCaption, smartCropsAspectRatios, modelVersion, context);
            return Result.FromValue(ImageAnalysisResult.FromResponse(result.GetRawResponse()), result.GetRawResponse());
        }

        /// <summary>
        /// [Protocol Method] Performs a single Image Analysis operation
        /// <list type="bullet">
        /// <item>
        /// <description>
        /// This <see href="https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/core/Azure.Core/samples/ProtocolMethods.md">protocol method</see> allows explicit creation of the request and processing of the response for advanced scenarios.
        /// </description>
        /// </item>
        /// <item>
        /// <description>
        /// Please try the simpler <see cref="AnalyzeFromImageDataAsync(IEnumerable{VisualFeaturesImpl},BinaryData,string,bool?,IEnumerable{float},string,CancellationToken)"/> convenience overload with strongly typed models first.
        /// </description>
        /// </item>
        /// </list>
        /// </summary>
        /// <param name="visualFeatures">
        /// A list of visual features to analyze.
        /// Seven visual features are supported: Caption, DenseCaptions, Read (OCR), Tags, Objects, SmartCrops, and People.
        /// At least one visual feature must be specified.
        /// </param>
        /// <param name="content"> The content to send as the body of the request. </param>
        /// <param name="language">
        /// The desired language for result generation (a two-letter language code).
        /// If this option is not specified, the default value 'en' is used (English).
        /// See https://aka.ms/cv-languages for a list of supported languages.
        /// </param>
        /// <param name="genderNeutralCaption">
        /// Boolean flag for enabling gender-neutral captioning for Caption and Dense Captions features.
        /// By default captions may contain gender terms (for example: 'man', 'woman', or 'boy', 'girl').
        /// If you set this to "true", those will be replaced with gender-neutral terms (for example: 'person' or 'child').
        /// </param>
        /// <param name="smartCropsAspectRatios">
        /// A list of aspect ratios to use for smart cropping.
        /// Aspect ratios are calculated by dividing the target crop width in pixels by the height in pixels.
        /// Supported values are between 0.75 and 1.8 (inclusive).
        /// If this parameter is not specified, the service will return one crop region with an aspect
        /// ratio it sees fit between 0.5 and 2.0 (inclusive).
        /// </param>
        /// <param name="modelVersion">
        /// The version of cloud AI-model used for analysis.
        /// The format is the following: 'latest' (default value) or 'YYYY-MM-DD' or 'YYYY-MM-DD-preview', where 'YYYY', 'MM', 'DD' are the year, month and day associated with the model.
        /// This is not commonly set, as the default always gives the latest AI model with recent improvements.
        /// If however you would like to make sure analysis results do not change over time, set this value to a specific model version.
        /// </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="visualFeatures"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="MessageFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        internal virtual async Task<Result> AnalyzeFromImageDataAsync(IEnumerable<VisualFeaturesImpl> visualFeatures, RequestBody content, string language = null, bool? genderNeutralCaption = null, IEnumerable<float> smartCropsAspectRatios = null, string modelVersion = null, RequestOptions context = null)
        {
            Argument.AssertNotNull(visualFeatures, nameof(visualFeatures));
            Argument.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateSpan("ImageAnalysisClient.AnalyzeFromImageData");
            scope.Start();
            try
            {
                using PipelineMessage message = CreateAnalyzeFromImageDataRequest(visualFeatures, content, language, genderNeutralCaption, smartCropsAspectRatios, modelVersion, context);
                return Result.FromResponse(await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false));
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary>
        /// [Protocol Method] Performs a single Image Analysis operation
        /// <list type="bullet">
        /// <item>
        /// <description>
        /// This <see href="https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/core/Azure.Core/samples/ProtocolMethods.md">protocol method</see> allows explicit creation of the request and processing of the response for advanced scenarios.
        /// </description>
        /// </item>
        /// <item>
        /// <description>
        /// Please try the simpler <see cref="AnalyzeFromImageData(IEnumerable{VisualFeaturesImpl},BinaryData,string,bool?,IEnumerable{float},string,CancellationToken)"/> convenience overload with strongly typed models first.
        /// </description>
        /// </item>
        /// </list>
        /// </summary>
        /// <param name="visualFeatures">
        /// A list of visual features to analyze.
        /// Seven visual features are supported: Caption, DenseCaptions, Read (OCR), Tags, Objects, SmartCrops, and People.
        /// At least one visual feature must be specified.
        /// </param>
        /// <param name="content"> The content to send as the body of the request. </param>
        /// <param name="language">
        /// The desired language for result generation (a two-letter language code).
        /// If this option is not specified, the default value 'en' is used (English).
        /// See https://aka.ms/cv-languages for a list of supported languages.
        /// </param>
        /// <param name="genderNeutralCaption">
        /// Boolean flag for enabling gender-neutral captioning for Caption and Dense Captions features.
        /// By default captions may contain gender terms (for example: 'man', 'woman', or 'boy', 'girl').
        /// If you set this to "true", those will be replaced with gender-neutral terms (for example: 'person' or 'child').
        /// </param>
        /// <param name="smartCropsAspectRatios">
        /// A list of aspect ratios to use for smart cropping.
        /// Aspect ratios are calculated by dividing the target crop width in pixels by the height in pixels.
        /// Supported values are between 0.75 and 1.8 (inclusive).
        /// If this parameter is not specified, the service will return one crop region with an aspect
        /// ratio it sees fit between 0.5 and 2.0 (inclusive).
        /// </param>
        /// <param name="modelVersion">
        /// The version of cloud AI-model used for analysis.
        /// The format is the following: 'latest' (default value) or 'YYYY-MM-DD' or 'YYYY-MM-DD-preview', where 'YYYY', 'MM', 'DD' are the year, month and day associated with the model.
        /// This is not commonly set, as the default always gives the latest AI model with recent improvements.
        /// If however you would like to make sure analysis results do not change over time, set this value to a specific model version.
        /// </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="visualFeatures"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="MessageFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        internal virtual Result AnalyzeFromImageData(IEnumerable<VisualFeaturesImpl> visualFeatures, RequestBody content, string language = null, bool? genderNeutralCaption = null, IEnumerable<float> smartCropsAspectRatios = null, string modelVersion = null, RequestOptions context = null)
        {
            Argument.AssertNotNull(visualFeatures, nameof(visualFeatures));
            Argument.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateSpan("ImageAnalysisClient.AnalyzeFromImageData");
            scope.Start();
            try
            {
                using PipelineMessage message = CreateAnalyzeFromImageDataRequest(visualFeatures, content, language, genderNeutralCaption, smartCropsAspectRatios, modelVersion, context);
                return Result.FromResponse(_pipeline.ProcessMessage(message, context));
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Performs a single Image Analysis operation. </summary>
        /// <param name="visualFeatures">
        /// A list of visual features to analyze.
        /// Seven visual features are supported: Caption, DenseCaptions, Read (OCR), Tags, Objects, SmartCrops, and People.
        /// At least one visual feature must be specified.
        /// </param>
        /// <param name="imageContent"> The image to be analyzed. </param>
        /// <param name="language">
        /// The desired language for result generation (a two-letter language code).
        /// If this option is not specified, the default value 'en' is used (English).
        /// See https://aka.ms/cv-languages for a list of supported languages.
        /// </param>
        /// <param name="genderNeutralCaption">
        /// Boolean flag for enabling gender-neutral captioning for Caption and Dense Captions features.
        /// By default captions may contain gender terms (for example: 'man', 'woman', or 'boy', 'girl').
        /// If you set this to "true", those will be replaced with gender-neutral terms (for example: 'person' or 'child').
        /// </param>
        /// <param name="smartCropsAspectRatios">
        /// A list of aspect ratios to use for smart cropping.
        /// Aspect ratios are calculated by dividing the target crop width in pixels by the height in pixels.
        /// Supported values are between 0.75 and 1.8 (inclusive).
        /// If this parameter is not specified, the service will return one crop region with an aspect
        /// ratio it sees fit between 0.5 and 2.0 (inclusive).
        /// </param>
        /// <param name="modelVersion">
        /// The version of cloud AI-model used for analysis.
        /// The format is the following: 'latest' (default value) or 'YYYY-MM-DD' or 'YYYY-MM-DD-preview', where 'YYYY', 'MM', 'DD' are the year, month and day associated with the model.
        /// This is not commonly set, as the default always gives the latest AI model with recent improvements.
        /// If however you would like to make sure analysis results do not change over time, set this value to a specific model version.
        /// </param>
        /// <param name="cancellationToken"> The cancellation token to use. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="visualFeatures"/> or <paramref name="imageContent"/> is null. </exception>
        internal virtual async Task<Result<ImageAnalysisResult>> AnalyzeFromUrlAsync(IEnumerable<VisualFeaturesImpl> visualFeatures, ImageUrl imageContent, string language = null, bool? genderNeutralCaption = null, IEnumerable<float> smartCropsAspectRatios = null, string modelVersion = null, CancellationToken cancellationToken = default)
        {
            Argument.AssertNotNull(visualFeatures, nameof(visualFeatures));
            Argument.AssertNotNull(imageContent, nameof(imageContent));

            RequestOptions context = FromCancellationToken(cancellationToken);
            using RequestBody content = imageContent.ToRequestBody();
            Result result = await AnalyzeFromUrlAsync(visualFeatures, content, language, genderNeutralCaption, smartCropsAspectRatios, modelVersion, context).ConfigureAwait(false);
            return Result.FromValue(ImageAnalysisResult.FromResponse(result.GetRawResponse()), result.GetRawResponse());
        }

        /// <summary> Performs a single Image Analysis operation. </summary>
        /// <param name="visualFeatures">
        /// A list of visual features to analyze.
        /// Seven visual features are supported: Caption, DenseCaptions, Read (OCR), Tags, Objects, SmartCrops, and People.
        /// At least one visual feature must be specified.
        /// </param>
        /// <param name="imageContent"> The image to be analyzed. </param>
        /// <param name="language">
        /// The desired language for result generation (a two-letter language code).
        /// If this option is not specified, the default value 'en' is used (English).
        /// See https://aka.ms/cv-languages for a list of supported languages.
        /// </param>
        /// <param name="genderNeutralCaption">
        /// Boolean flag for enabling gender-neutral captioning for Caption and Dense Captions features.
        /// By default captions may contain gender terms (for example: 'man', 'woman', or 'boy', 'girl').
        /// If you set this to "true", those will be replaced with gender-neutral terms (for example: 'person' or 'child').
        /// </param>
        /// <param name="smartCropsAspectRatios">
        /// A list of aspect ratios to use for smart cropping.
        /// Aspect ratios are calculated by dividing the target crop width in pixels by the height in pixels.
        /// Supported values are between 0.75 and 1.8 (inclusive).
        /// If this parameter is not specified, the service will return one crop region with an aspect
        /// ratio it sees fit between 0.5 and 2.0 (inclusive).
        /// </param>
        /// <param name="modelVersion">
        /// The version of cloud AI-model used for analysis.
        /// The format is the following: 'latest' (default value) or 'YYYY-MM-DD' or 'YYYY-MM-DD-preview', where 'YYYY', 'MM', 'DD' are the year, month and day associated with the model.
        /// This is not commonly set, as the default always gives the latest AI model with recent improvements.
        /// If however you would like to make sure analysis results do not change over time, set this value to a specific model version.
        /// </param>
        /// <param name="cancellationToken"> The cancellation token to use. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="visualFeatures"/> or <paramref name="imageContent"/> is null. </exception>
        internal virtual Result<ImageAnalysisResult> AnalyzeFromUrl(IEnumerable<VisualFeaturesImpl> visualFeatures, ImageUrl imageContent, string language = null, bool? genderNeutralCaption = null, IEnumerable<float> smartCropsAspectRatios = null, string modelVersion = null, CancellationToken cancellationToken = default)
        {
            Argument.AssertNotNull(visualFeatures, nameof(visualFeatures));
            Argument.AssertNotNull(imageContent, nameof(imageContent));

            RequestOptions context = FromCancellationToken(cancellationToken);
            using RequestBody content = imageContent.ToRequestBody();
            Result result = AnalyzeFromUrl(visualFeatures, content, language, genderNeutralCaption, smartCropsAspectRatios, modelVersion, context);
            return Result.FromValue(ImageAnalysisResult.FromResponse(result.GetRawResponse()), result.GetRawResponse());
        }

        /// <summary>
        /// [Protocol Method] Performs a single Image Analysis operation
        /// <list type="bullet">
        /// <item>
        /// <description>
        /// This <see href="https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/core/Azure.Core/samples/ProtocolMethods.md">protocol method</see> allows explicit creation of the request and processing of the response for advanced scenarios.
        /// </description>
        /// </item>
        /// <item>
        /// <description>
        /// Please try the simpler <see cref="AnalyzeFromUrlAsync(IEnumerable{VisualFeaturesImpl},ImageUrl,string,bool?,IEnumerable{float},string,CancellationToken)"/> convenience overload with strongly typed models first.
        /// </description>
        /// </item>
        /// </list>
        /// </summary>
        /// <param name="visualFeatures">
        /// A list of visual features to analyze.
        /// Seven visual features are supported: Caption, DenseCaptions, Read (OCR), Tags, Objects, SmartCrops, and People.
        /// At least one visual feature must be specified.
        /// </param>
        /// <param name="content"> The content to send as the body of the request. </param>
        /// <param name="language">
        /// The desired language for result generation (a two-letter language code).
        /// If this option is not specified, the default value 'en' is used (English).
        /// See https://aka.ms/cv-languages for a list of supported languages.
        /// </param>
        /// <param name="genderNeutralCaption">
        /// Boolean flag for enabling gender-neutral captioning for Caption and Dense Captions features.
        /// By default captions may contain gender terms (for example: 'man', 'woman', or 'boy', 'girl').
        /// If you set this to "true", those will be replaced with gender-neutral terms (for example: 'person' or 'child').
        /// </param>
        /// <param name="smartCropsAspectRatios">
        /// A list of aspect ratios to use for smart cropping.
        /// Aspect ratios are calculated by dividing the target crop width in pixels by the height in pixels.
        /// Supported values are between 0.75 and 1.8 (inclusive).
        /// If this parameter is not specified, the service will return one crop region with an aspect
        /// ratio it sees fit between 0.5 and 2.0 (inclusive).
        /// </param>
        /// <param name="modelVersion">
        /// The version of cloud AI-model used for analysis.
        /// The format is the following: 'latest' (default value) or 'YYYY-MM-DD' or 'YYYY-MM-DD-preview', where 'YYYY', 'MM', 'DD' are the year, month and day associated with the model.
        /// This is not commonly set, as the default always gives the latest AI model with recent improvements.
        /// If however you would like to make sure analysis results do not change over time, set this value to a specific model version.
        /// </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="visualFeatures"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="MessageFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        internal virtual async Task<Result> AnalyzeFromUrlAsync(IEnumerable<VisualFeaturesImpl> visualFeatures, RequestBody content, string language = null, bool? genderNeutralCaption = null, IEnumerable<float> smartCropsAspectRatios = null, string modelVersion = null, RequestOptions context = null)
        {
            Argument.AssertNotNull(visualFeatures, nameof(visualFeatures));
            Argument.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateSpan("ImageAnalysisClient.AnalyzeFromUrl");
            scope.Start();
            try
            {
                using PipelineMessage message = CreateAnalyzeFromUrlRequest(visualFeatures, content, language, genderNeutralCaption, smartCropsAspectRatios, modelVersion, context);
                return Result.FromResponse(await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false));
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary>
        /// [Protocol Method] Performs a single Image Analysis operation
        /// <list type="bullet">
        /// <item>
        /// <description>
        /// This <see href="https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/core/Azure.Core/samples/ProtocolMethods.md">protocol method</see> allows explicit creation of the request and processing of the response for advanced scenarios.
        /// </description>
        /// </item>
        /// <item>
        /// <description>
        /// Please try the simpler <see cref="AnalyzeFromUrl(IEnumerable{VisualFeaturesImpl},ImageUrl,string,bool?,IEnumerable{float},string,CancellationToken)"/> convenience overload with strongly typed models first.
        /// </description>
        /// </item>
        /// </list>
        /// </summary>
        /// <param name="visualFeatures">
        /// A list of visual features to analyze.
        /// Seven visual features are supported: Caption, DenseCaptions, Read (OCR), Tags, Objects, SmartCrops, and People.
        /// At least one visual feature must be specified.
        /// </param>
        /// <param name="content"> The content to send as the body of the request. </param>
        /// <param name="language">
        /// The desired language for result generation (a two-letter language code).
        /// If this option is not specified, the default value 'en' is used (English).
        /// See https://aka.ms/cv-languages for a list of supported languages.
        /// </param>
        /// <param name="genderNeutralCaption">
        /// Boolean flag for enabling gender-neutral captioning for Caption and Dense Captions features.
        /// By default captions may contain gender terms (for example: 'man', 'woman', or 'boy', 'girl').
        /// If you set this to "true", those will be replaced with gender-neutral terms (for example: 'person' or 'child').
        /// </param>
        /// <param name="smartCropsAspectRatios">
        /// A list of aspect ratios to use for smart cropping.
        /// Aspect ratios are calculated by dividing the target crop width in pixels by the height in pixels.
        /// Supported values are between 0.75 and 1.8 (inclusive).
        /// If this parameter is not specified, the service will return one crop region with an aspect
        /// ratio it sees fit between 0.5 and 2.0 (inclusive).
        /// </param>
        /// <param name="modelVersion">
        /// The version of cloud AI-model used for analysis.
        /// The format is the following: 'latest' (default value) or 'YYYY-MM-DD' or 'YYYY-MM-DD-preview', where 'YYYY', 'MM', 'DD' are the year, month and day associated with the model.
        /// This is not commonly set, as the default always gives the latest AI model with recent improvements.
        /// If however you would like to make sure analysis results do not change over time, set this value to a specific model version.
        /// </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="visualFeatures"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="MessageFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        internal virtual Result AnalyzeFromUrl(IEnumerable<VisualFeaturesImpl> visualFeatures, RequestBody content, string language = null, bool? genderNeutralCaption = null, IEnumerable<float> smartCropsAspectRatios = null, string modelVersion = null, RequestOptions context = null)
        {
            Argument.AssertNotNull(visualFeatures, nameof(visualFeatures));
            Argument.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateSpan("ImageAnalysisClient.AnalyzeFromUrl");
            scope.Start();
            try
            {
                using PipelineMessage message = CreateAnalyzeFromUrlRequest(visualFeatures, content, language, genderNeutralCaption, smartCropsAspectRatios, modelVersion, context);
                return Result.FromResponse(_pipeline.ProcessMessage(message, context));
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        internal PipelineMessage CreateAnalyzeFromImageDataRequest(IEnumerable<VisualFeaturesImpl> visualFeatures, RequestBody content, string language, bool? genderNeutralCaption, IEnumerable<float> smartCropsAspectRatios, string modelVersion, RequestOptions context)
        {
            var message = _pipeline.CreateMessage(context, ResponseErrorClassifier200);
            var request = message.Request;
            request.SetMethod("POST");
            var uri = new RequestUri();
            uri.Reset(_endpoint);
            uri.AppendRaw("/computervision", false);
            uri.AppendPath("/imageanalysis:analyze", false);
            if (visualFeatures != null && !(visualFeatures is ChangeTrackingList<VisualFeaturesImpl> changeTrackingList && changeTrackingList.IsUndefined))
            {
                uri.AppendQueryDelimited("features", visualFeatures, ",", true);
            }
            if (language != null)
            {
                uri.AppendQuery("language", language, true);
            }
            if (genderNeutralCaption != null)
            {
                uri.AppendQuery("gender-neutral-caption", genderNeutralCaption.Value, true);
            }
            if (smartCropsAspectRatios != null && !(smartCropsAspectRatios is ChangeTrackingList<float> changeTrackingList0 && changeTrackingList0.IsUndefined))
            {
                uri.AppendQueryDelimited("smartcrops-aspect-ratios", smartCropsAspectRatios, ",", true);
            }
            if (modelVersion != null)
            {
                uri.AppendQuery("model-version", modelVersion, true);
            }
            request.Uri = uri.ToUri();
            request.SetHeaderValue("Accept", "application/json");
            request.SetHeaderValue("content-type", "application/octet-stream");
            request.Content = content;
            return message;
        }

        internal PipelineMessage CreateAnalyzeFromUrlRequest(IEnumerable<VisualFeaturesImpl> visualFeatures, RequestBody content, string language, bool? genderNeutralCaption, IEnumerable<float> smartCropsAspectRatios, string modelVersion, RequestOptions context)
        {
            var message = _pipeline.CreateMessage(context, ResponseErrorClassifier200);
            var request = message.Request;
            request.SetMethod("POST");
            var uri = new RequestUri();
            uri.Reset(_endpoint);
            uri.AppendRaw("/computervision", false);
            uri.AppendPath("/imageanalysis:analyze", false);
            if (visualFeatures != null && !(visualFeatures is ChangeTrackingList<VisualFeaturesImpl> changeTrackingList && changeTrackingList.IsUndefined))
            {
                uri.AppendQueryDelimited("features", visualFeatures, ",", true);
            }
            if (language != null)
            {
                uri.AppendQuery("language", language, true);
            }
            if (genderNeutralCaption != null)
            {
                uri.AppendQuery("gender-neutral-caption", genderNeutralCaption.Value, true);
            }
            if (smartCropsAspectRatios != null && !(smartCropsAspectRatios is ChangeTrackingList<float> changeTrackingList0 && changeTrackingList0.IsUndefined))
            {
                uri.AppendQueryDelimited("smartcrops-aspect-ratios", smartCropsAspectRatios, ",", true);
            }
            if (modelVersion != null)
            {
                uri.AppendQuery("model-version", modelVersion, true);
            }
            request.Uri = uri.ToUri();
            request.SetHeaderValue("Accept", "application/json");
            request.SetHeaderValue("content-type", "application/json");
            request.Content = content;
            return message;
        }

        private static RequestOptions DefaultRequestContext = new RequestOptions();
        internal static RequestOptions FromCancellationToken(CancellationToken cancellationToken = default)
        {
            if (!cancellationToken.CanBeCanceled)
            {
                return DefaultRequestContext;
            }

            return new RequestOptions() { CancellationToken = cancellationToken };
        }

        private static ResponseErrorClassifier _responseErrorClassifier200;
        private static ResponseErrorClassifier ResponseErrorClassifier200 => _responseErrorClassifier200 ??= new StatusResponseClassifier(stackalloc ushort[] { 200 });
    }
}
