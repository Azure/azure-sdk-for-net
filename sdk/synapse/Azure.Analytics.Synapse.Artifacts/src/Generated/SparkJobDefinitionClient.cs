// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

// <auto-generated/>

#nullable disable

using System;
using System.Threading.Tasks;
using Azure;
using Azure.Core;
using Azure.Core.Pipeline;

namespace Azure.Analytics.Synapse.Artifacts
{
    /// <summary> The SparkJobDefinition service client. </summary>
    public partial class SparkJobDefinitionClient
    {
        /// <summary> The HTTP pipeline for sending and receiving REST requests and responses. </summary>
        public virtual HttpPipeline Pipeline { get; }
        private readonly string[] AuthorizationScopes = { "https://dev.azuresynapse.net/.default" };
        private readonly TokenCredential _tokenCredential;
        private Uri endpoint;
        private readonly string apiVersion;
        private readonly ClientDiagnostics _clientDiagnostics;

        /// <summary> Initializes a new instance of SparkJobDefinitionClient for mocking. </summary>
        protected SparkJobDefinitionClient()
        {
        }

        /// <summary> Initializes a new instance of SparkJobDefinitionClient. </summary>
        /// <param name="endpoint"> The workspace development endpoint, for example https://myworkspace.dev.azuresynapse.net. </param>
        /// <param name="credential"> A credential used to authenticate to an Azure Service. </param>
        /// <param name="options"> The options for configuring the client. </param>
        public SparkJobDefinitionClient(Uri endpoint, TokenCredential credential, ArtifactsClientOptions options = null)
        {
            if (endpoint == null)
            {
                throw new ArgumentNullException(nameof(endpoint));
            }
            if (credential == null)
            {
                throw new ArgumentNullException(nameof(credential));
            }

            options ??= new ArtifactsClientOptions();
            _clientDiagnostics = new ClientDiagnostics(options);
            _tokenCredential = credential;
            var authPolicy = new BearerTokenAuthenticationPolicy(_tokenCredential, AuthorizationScopes);
            Pipeline = HttpPipelineBuilder.Build(options, new HttpPipelinePolicy[] { new LowLevelCallbackPolicy() }, new HttpPipelinePolicy[] { authPolicy }, new ResponseClassifier());
            this.endpoint = endpoint;
            apiVersion = options.Version;
        }

        /// <summary> Lists spark job definitions. </summary>
        /// <param name="requestOptions"> The request options. </param>
#pragma warning disable AZC0002
        public virtual async Task<Response> GetSparkJobDefinitionsByWorkspaceAsync(RequestOptions requestOptions = null)
#pragma warning restore AZC0002
        {
            requestOptions ??= new RequestOptions();
            HttpMessage message = CreateGetSparkJobDefinitionsByWorkspaceRequest(requestOptions);
            if (requestOptions.PerCallPolicy != null)
            {
                message.SetProperty("RequestOptionsPerCallPolicyCallback", requestOptions.PerCallPolicy);
            }
            using var scope = _clientDiagnostics.CreateScope("SparkJobDefinitionClient.GetSparkJobDefinitionsByWorkspace");
            scope.Start();
            try
            {
                await Pipeline.SendAsync(message, requestOptions.CancellationToken).ConfigureAwait(false);
                if (requestOptions.StatusOption == ResponseStatusOption.Default)
                {
                    switch (message.Response.Status)
                    {
                        case 200:
                            return message.Response;
                        default:
                            throw await _clientDiagnostics.CreateRequestFailedExceptionAsync(message.Response).ConfigureAwait(false);
                    }
                }
                else
                {
                    return message.Response;
                }
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Lists spark job definitions. </summary>
        /// <param name="requestOptions"> The request options. </param>
#pragma warning disable AZC0002
        public virtual Response GetSparkJobDefinitionsByWorkspace(RequestOptions requestOptions = null)
#pragma warning restore AZC0002
        {
            requestOptions ??= new RequestOptions();
            HttpMessage message = CreateGetSparkJobDefinitionsByWorkspaceRequest(requestOptions);
            if (requestOptions.PerCallPolicy != null)
            {
                message.SetProperty("RequestOptionsPerCallPolicyCallback", requestOptions.PerCallPolicy);
            }
            using var scope = _clientDiagnostics.CreateScope("SparkJobDefinitionClient.GetSparkJobDefinitionsByWorkspace");
            scope.Start();
            try
            {
                Pipeline.Send(message, requestOptions.CancellationToken);
                if (requestOptions.StatusOption == ResponseStatusOption.Default)
                {
                    switch (message.Response.Status)
                    {
                        case 200:
                            return message.Response;
                        default:
                            throw _clientDiagnostics.CreateRequestFailedException(message.Response);
                    }
                }
                else
                {
                    return message.Response;
                }
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Create Request for <see cref="GetSparkJobDefinitionsByWorkspace"/> and <see cref="GetSparkJobDefinitionsByWorkspaceAsync"/> operations. </summary>
        /// <param name="requestOptions"> The request options. </param>
        private HttpMessage CreateGetSparkJobDefinitionsByWorkspaceRequest(RequestOptions requestOptions = null)
        {
            var message = Pipeline.CreateMessage();
            var request = message.Request;
            request.Method = RequestMethod.Get;
            var uri = new RawRequestUriBuilder();
            uri.Reset(endpoint);
            uri.AppendPath("/sparkJobDefinitions", false);
            uri.AppendQuery("api-version", apiVersion, true);
            request.Uri = uri;
            request.Headers.Add("Accept", "application/json");
            return message;
        }

        /// <summary> Creates or updates a Spark Job Definition. </summary>
        /// <remarks>
        /// Schema for <c>Request Body</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>etag</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> Resource Etag. </term>
        ///   </item>
        ///   <item>
        ///     <term>id</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> Fully qualified resource ID for the resource. Ex - /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceType}/{resourceName}. </term>
        ///   </item>
        ///   <item>
        ///     <term>name</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The name of the resource. </term>
        ///   </item>
        ///   <item>
        ///     <term>type</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The type of the resource. E.g. &quot;Microsoft.Compute/virtualMachines&quot; or &quot;Microsoft.Storage/storageAccounts&quot;. </term>
        ///   </item>
        ///   <item>
        ///     <term>properties</term>
        ///     <term>SparkJobDefinition</term>
        ///     <term>Yes</term>
        ///     <term> Properties of spark job definition. </term>
        ///   </item>
        /// </list>
        /// Schema for <c>SparkJobDefinition</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>description</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The description of the Spark job definition. </term>
        ///   </item>
        ///   <item>
        ///     <term>targetBigDataPool</term>
        ///     <term>BigDataPoolReference</term>
        ///     <term>Yes</term>
        ///     <term> Big data pool reference. </term>
        ///   </item>
        ///   <item>
        ///     <term>requiredSparkVersion</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The required Spark version of the application. </term>
        ///   </item>
        ///   <item>
        ///     <term>language</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The language of the Spark application. </term>
        ///   </item>
        ///   <item>
        ///     <term>jobProperties</term>
        ///     <term>SparkJobProperties</term>
        ///     <term>Yes</term>
        ///     <term> The properties of the Spark job. </term>
        ///   </item>
        /// </list>
        /// Schema for <c>BigDataPoolReference</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>type</term>
        ///     <term>&quot;BigDataPoolReference&quot;</term>
        ///     <term>Yes</term>
        ///     <term> Big data pool reference type. </term>
        ///   </item>
        ///   <item>
        ///     <term>referenceName</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> Reference big data pool name. </term>
        ///   </item>
        /// </list>
        /// Schema for <c>SparkJobProperties</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>name</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The name of the job. </term>
        ///   </item>
        ///   <item>
        ///     <term>file</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> File containing the application to execute. </term>
        ///   </item>
        ///   <item>
        ///     <term>className</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> Main class for Java/Scala application. </term>
        ///   </item>
        ///   <item>
        ///     <term>conf</term>
        ///     <term>AnyObject</term>
        ///     <term></term>
        ///     <term> Spark configuration properties. </term>
        ///   </item>
        ///   <item>
        ///     <term>args</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> Command line arguments for the application. </term>
        ///   </item>
        ///   <item>
        ///     <term>jars</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> Jars to be used in this job. </term>
        ///   </item>
        ///   <item>
        ///     <term>files</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> files to be used in this job. </term>
        ///   </item>
        ///   <item>
        ///     <term>archives</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> Archives to be used in this job. </term>
        ///   </item>
        ///   <item>
        ///     <term>driverMemory</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> Amount of memory to use for the driver process. </term>
        ///   </item>
        ///   <item>
        ///     <term>driverCores</term>
        ///     <term>number</term>
        ///     <term>Yes</term>
        ///     <term> Number of cores to use for the driver. </term>
        ///   </item>
        ///   <item>
        ///     <term>executorMemory</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> Amount of memory to use per executor process. </term>
        ///   </item>
        ///   <item>
        ///     <term>executorCores</term>
        ///     <term>number</term>
        ///     <term>Yes</term>
        ///     <term> Number of cores to use for each executor. </term>
        ///   </item>
        ///   <item>
        ///     <term>numExecutors</term>
        ///     <term>number</term>
        ///     <term>Yes</term>
        ///     <term> Number of executors to launch for this job. </term>
        ///   </item>
        /// </list>
        /// </remarks>
        /// <param name="sparkJobDefinitionName"> The spark job definition name. </param>
        /// <param name="requestBody"> The request body. </param>
        /// <param name="ifMatch"> ETag of the Spark Job Definition entity.  Should only be specified for update, for which it should match existing entity or can be * for unconditional update. </param>
        /// <param name="requestOptions"> The request options. </param>
#pragma warning disable AZC0002
        public virtual async Task<Response> CreateOrUpdateSparkJobDefinitionAsync(string sparkJobDefinitionName, RequestContent requestBody, string ifMatch = null, RequestOptions requestOptions = null)
#pragma warning restore AZC0002
        {
            requestOptions ??= new RequestOptions();
            HttpMessage message = CreateCreateOrUpdateSparkJobDefinitionRequest(sparkJobDefinitionName, requestBody, ifMatch, requestOptions);
            if (requestOptions.PerCallPolicy != null)
            {
                message.SetProperty("RequestOptionsPerCallPolicyCallback", requestOptions.PerCallPolicy);
            }
            using var scope = _clientDiagnostics.CreateScope("SparkJobDefinitionClient.CreateOrUpdateSparkJobDefinition");
            scope.Start();
            try
            {
                await Pipeline.SendAsync(message, requestOptions.CancellationToken).ConfigureAwait(false);
                if (requestOptions.StatusOption == ResponseStatusOption.Default)
                {
                    switch (message.Response.Status)
                    {
                        case 200:
                        case 202:
                            return message.Response;
                        default:
                            throw await _clientDiagnostics.CreateRequestFailedExceptionAsync(message.Response).ConfigureAwait(false);
                    }
                }
                else
                {
                    return message.Response;
                }
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Creates or updates a Spark Job Definition. </summary>
        /// <remarks>
        /// Schema for <c>Request Body</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>etag</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> Resource Etag. </term>
        ///   </item>
        ///   <item>
        ///     <term>id</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> Fully qualified resource ID for the resource. Ex - /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceType}/{resourceName}. </term>
        ///   </item>
        ///   <item>
        ///     <term>name</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The name of the resource. </term>
        ///   </item>
        ///   <item>
        ///     <term>type</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The type of the resource. E.g. &quot;Microsoft.Compute/virtualMachines&quot; or &quot;Microsoft.Storage/storageAccounts&quot;. </term>
        ///   </item>
        ///   <item>
        ///     <term>properties</term>
        ///     <term>SparkJobDefinition</term>
        ///     <term>Yes</term>
        ///     <term> Properties of spark job definition. </term>
        ///   </item>
        /// </list>
        /// Schema for <c>SparkJobDefinition</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>description</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The description of the Spark job definition. </term>
        ///   </item>
        ///   <item>
        ///     <term>targetBigDataPool</term>
        ///     <term>BigDataPoolReference</term>
        ///     <term>Yes</term>
        ///     <term> Big data pool reference. </term>
        ///   </item>
        ///   <item>
        ///     <term>requiredSparkVersion</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The required Spark version of the application. </term>
        ///   </item>
        ///   <item>
        ///     <term>language</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The language of the Spark application. </term>
        ///   </item>
        ///   <item>
        ///     <term>jobProperties</term>
        ///     <term>SparkJobProperties</term>
        ///     <term>Yes</term>
        ///     <term> The properties of the Spark job. </term>
        ///   </item>
        /// </list>
        /// Schema for <c>BigDataPoolReference</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>type</term>
        ///     <term>&quot;BigDataPoolReference&quot;</term>
        ///     <term>Yes</term>
        ///     <term> Big data pool reference type. </term>
        ///   </item>
        ///   <item>
        ///     <term>referenceName</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> Reference big data pool name. </term>
        ///   </item>
        /// </list>
        /// Schema for <c>SparkJobProperties</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>name</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The name of the job. </term>
        ///   </item>
        ///   <item>
        ///     <term>file</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> File containing the application to execute. </term>
        ///   </item>
        ///   <item>
        ///     <term>className</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> Main class for Java/Scala application. </term>
        ///   </item>
        ///   <item>
        ///     <term>conf</term>
        ///     <term>AnyObject</term>
        ///     <term></term>
        ///     <term> Spark configuration properties. </term>
        ///   </item>
        ///   <item>
        ///     <term>args</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> Command line arguments for the application. </term>
        ///   </item>
        ///   <item>
        ///     <term>jars</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> Jars to be used in this job. </term>
        ///   </item>
        ///   <item>
        ///     <term>files</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> files to be used in this job. </term>
        ///   </item>
        ///   <item>
        ///     <term>archives</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> Archives to be used in this job. </term>
        ///   </item>
        ///   <item>
        ///     <term>driverMemory</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> Amount of memory to use for the driver process. </term>
        ///   </item>
        ///   <item>
        ///     <term>driverCores</term>
        ///     <term>number</term>
        ///     <term>Yes</term>
        ///     <term> Number of cores to use for the driver. </term>
        ///   </item>
        ///   <item>
        ///     <term>executorMemory</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> Amount of memory to use per executor process. </term>
        ///   </item>
        ///   <item>
        ///     <term>executorCores</term>
        ///     <term>number</term>
        ///     <term>Yes</term>
        ///     <term> Number of cores to use for each executor. </term>
        ///   </item>
        ///   <item>
        ///     <term>numExecutors</term>
        ///     <term>number</term>
        ///     <term>Yes</term>
        ///     <term> Number of executors to launch for this job. </term>
        ///   </item>
        /// </list>
        /// </remarks>
        /// <param name="sparkJobDefinitionName"> The spark job definition name. </param>
        /// <param name="requestBody"> The request body. </param>
        /// <param name="ifMatch"> ETag of the Spark Job Definition entity.  Should only be specified for update, for which it should match existing entity or can be * for unconditional update. </param>
        /// <param name="requestOptions"> The request options. </param>
#pragma warning disable AZC0002
        public virtual Response CreateOrUpdateSparkJobDefinition(string sparkJobDefinitionName, RequestContent requestBody, string ifMatch = null, RequestOptions requestOptions = null)
#pragma warning restore AZC0002
        {
            requestOptions ??= new RequestOptions();
            HttpMessage message = CreateCreateOrUpdateSparkJobDefinitionRequest(sparkJobDefinitionName, requestBody, ifMatch, requestOptions);
            if (requestOptions.PerCallPolicy != null)
            {
                message.SetProperty("RequestOptionsPerCallPolicyCallback", requestOptions.PerCallPolicy);
            }
            using var scope = _clientDiagnostics.CreateScope("SparkJobDefinitionClient.CreateOrUpdateSparkJobDefinition");
            scope.Start();
            try
            {
                Pipeline.Send(message, requestOptions.CancellationToken);
                if (requestOptions.StatusOption == ResponseStatusOption.Default)
                {
                    switch (message.Response.Status)
                    {
                        case 200:
                        case 202:
                            return message.Response;
                        default:
                            throw _clientDiagnostics.CreateRequestFailedException(message.Response);
                    }
                }
                else
                {
                    return message.Response;
                }
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Create Request for <see cref="CreateOrUpdateSparkJobDefinition"/> and <see cref="CreateOrUpdateSparkJobDefinitionAsync"/> operations. </summary>
        /// <param name="sparkJobDefinitionName"> The spark job definition name. </param>
        /// <param name="requestBody"> The request body. </param>
        /// <param name="ifMatch"> ETag of the Spark Job Definition entity.  Should only be specified for update, for which it should match existing entity or can be * for unconditional update. </param>
        /// <param name="requestOptions"> The request options. </param>
        private HttpMessage CreateCreateOrUpdateSparkJobDefinitionRequest(string sparkJobDefinitionName, RequestContent requestBody, string ifMatch = null, RequestOptions requestOptions = null)
        {
            var message = Pipeline.CreateMessage();
            var request = message.Request;
            request.Method = RequestMethod.Put;
            var uri = new RawRequestUriBuilder();
            uri.Reset(endpoint);
            uri.AppendPath("/sparkJobDefinitions/", false);
            uri.AppendPath(sparkJobDefinitionName, true);
            uri.AppendQuery("api-version", apiVersion, true);
            request.Uri = uri;
            if (ifMatch != null)
            {
                request.Headers.Add("If-Match", ifMatch);
            }
            request.Headers.Add("Accept", "application/json");
            request.Headers.Add("Content-Type", "application/json");
            request.Content = requestBody;
            return message;
        }

        /// <summary> Gets a Spark Job Definition. </summary>
        /// <param name="sparkJobDefinitionName"> The spark job definition name. </param>
        /// <param name="ifNoneMatch"> ETag of the Spark Job Definition entity. Should only be specified for get. If the ETag matches the existing entity tag, or if * was provided, then no content will be returned. </param>
        /// <param name="requestOptions"> The request options. </param>
#pragma warning disable AZC0002
        public virtual async Task<Response> GetSparkJobDefinitionAsync(string sparkJobDefinitionName, string ifNoneMatch = null, RequestOptions requestOptions = null)
#pragma warning restore AZC0002
        {
            requestOptions ??= new RequestOptions();
            HttpMessage message = CreateGetSparkJobDefinitionRequest(sparkJobDefinitionName, ifNoneMatch, requestOptions);
            if (requestOptions.PerCallPolicy != null)
            {
                message.SetProperty("RequestOptionsPerCallPolicyCallback", requestOptions.PerCallPolicy);
            }
            using var scope = _clientDiagnostics.CreateScope("SparkJobDefinitionClient.GetSparkJobDefinition");
            scope.Start();
            try
            {
                await Pipeline.SendAsync(message, requestOptions.CancellationToken).ConfigureAwait(false);
                if (requestOptions.StatusOption == ResponseStatusOption.Default)
                {
                    switch (message.Response.Status)
                    {
                        case 200:
                        case 304:
                            return message.Response;
                        default:
                            throw await _clientDiagnostics.CreateRequestFailedExceptionAsync(message.Response).ConfigureAwait(false);
                    }
                }
                else
                {
                    return message.Response;
                }
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Gets a Spark Job Definition. </summary>
        /// <param name="sparkJobDefinitionName"> The spark job definition name. </param>
        /// <param name="ifNoneMatch"> ETag of the Spark Job Definition entity. Should only be specified for get. If the ETag matches the existing entity tag, or if * was provided, then no content will be returned. </param>
        /// <param name="requestOptions"> The request options. </param>
#pragma warning disable AZC0002
        public virtual Response GetSparkJobDefinition(string sparkJobDefinitionName, string ifNoneMatch = null, RequestOptions requestOptions = null)
#pragma warning restore AZC0002
        {
            requestOptions ??= new RequestOptions();
            HttpMessage message = CreateGetSparkJobDefinitionRequest(sparkJobDefinitionName, ifNoneMatch, requestOptions);
            if (requestOptions.PerCallPolicy != null)
            {
                message.SetProperty("RequestOptionsPerCallPolicyCallback", requestOptions.PerCallPolicy);
            }
            using var scope = _clientDiagnostics.CreateScope("SparkJobDefinitionClient.GetSparkJobDefinition");
            scope.Start();
            try
            {
                Pipeline.Send(message, requestOptions.CancellationToken);
                if (requestOptions.StatusOption == ResponseStatusOption.Default)
                {
                    switch (message.Response.Status)
                    {
                        case 200:
                        case 304:
                            return message.Response;
                        default:
                            throw _clientDiagnostics.CreateRequestFailedException(message.Response);
                    }
                }
                else
                {
                    return message.Response;
                }
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Create Request for <see cref="GetSparkJobDefinition"/> and <see cref="GetSparkJobDefinitionAsync"/> operations. </summary>
        /// <param name="sparkJobDefinitionName"> The spark job definition name. </param>
        /// <param name="ifNoneMatch"> ETag of the Spark Job Definition entity. Should only be specified for get. If the ETag matches the existing entity tag, or if * was provided, then no content will be returned. </param>
        /// <param name="requestOptions"> The request options. </param>
        private HttpMessage CreateGetSparkJobDefinitionRequest(string sparkJobDefinitionName, string ifNoneMatch = null, RequestOptions requestOptions = null)
        {
            var message = Pipeline.CreateMessage();
            var request = message.Request;
            request.Method = RequestMethod.Get;
            var uri = new RawRequestUriBuilder();
            uri.Reset(endpoint);
            uri.AppendPath("/sparkJobDefinitions/", false);
            uri.AppendPath(sparkJobDefinitionName, true);
            uri.AppendQuery("api-version", apiVersion, true);
            request.Uri = uri;
            if (ifNoneMatch != null)
            {
                request.Headers.Add("If-None-Match", ifNoneMatch);
            }
            request.Headers.Add("Accept", "application/json");
            return message;
        }

        /// <summary> Deletes a Spark Job Definition. </summary>
        /// <param name="sparkJobDefinitionName"> The spark job definition name. </param>
        /// <param name="requestOptions"> The request options. </param>
#pragma warning disable AZC0002
        public virtual async Task<Response> DeleteSparkJobDefinitionAsync(string sparkJobDefinitionName, RequestOptions requestOptions = null)
#pragma warning restore AZC0002
        {
            requestOptions ??= new RequestOptions();
            HttpMessage message = CreateDeleteSparkJobDefinitionRequest(sparkJobDefinitionName, requestOptions);
            if (requestOptions.PerCallPolicy != null)
            {
                message.SetProperty("RequestOptionsPerCallPolicyCallback", requestOptions.PerCallPolicy);
            }
            using var scope = _clientDiagnostics.CreateScope("SparkJobDefinitionClient.DeleteSparkJobDefinition");
            scope.Start();
            try
            {
                await Pipeline.SendAsync(message, requestOptions.CancellationToken).ConfigureAwait(false);
                if (requestOptions.StatusOption == ResponseStatusOption.Default)
                {
                    switch (message.Response.Status)
                    {
                        case 200:
                        case 202:
                        case 204:
                            return message.Response;
                        default:
                            throw await _clientDiagnostics.CreateRequestFailedExceptionAsync(message.Response).ConfigureAwait(false);
                    }
                }
                else
                {
                    return message.Response;
                }
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Deletes a Spark Job Definition. </summary>
        /// <param name="sparkJobDefinitionName"> The spark job definition name. </param>
        /// <param name="requestOptions"> The request options. </param>
#pragma warning disable AZC0002
        public virtual Response DeleteSparkJobDefinition(string sparkJobDefinitionName, RequestOptions requestOptions = null)
#pragma warning restore AZC0002
        {
            requestOptions ??= new RequestOptions();
            HttpMessage message = CreateDeleteSparkJobDefinitionRequest(sparkJobDefinitionName, requestOptions);
            if (requestOptions.PerCallPolicy != null)
            {
                message.SetProperty("RequestOptionsPerCallPolicyCallback", requestOptions.PerCallPolicy);
            }
            using var scope = _clientDiagnostics.CreateScope("SparkJobDefinitionClient.DeleteSparkJobDefinition");
            scope.Start();
            try
            {
                Pipeline.Send(message, requestOptions.CancellationToken);
                if (requestOptions.StatusOption == ResponseStatusOption.Default)
                {
                    switch (message.Response.Status)
                    {
                        case 200:
                        case 202:
                        case 204:
                            return message.Response;
                        default:
                            throw _clientDiagnostics.CreateRequestFailedException(message.Response);
                    }
                }
                else
                {
                    return message.Response;
                }
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Create Request for <see cref="DeleteSparkJobDefinition"/> and <see cref="DeleteSparkJobDefinitionAsync"/> operations. </summary>
        /// <param name="sparkJobDefinitionName"> The spark job definition name. </param>
        /// <param name="requestOptions"> The request options. </param>
        private HttpMessage CreateDeleteSparkJobDefinitionRequest(string sparkJobDefinitionName, RequestOptions requestOptions = null)
        {
            var message = Pipeline.CreateMessage();
            var request = message.Request;
            request.Method = RequestMethod.Delete;
            var uri = new RawRequestUriBuilder();
            uri.Reset(endpoint);
            uri.AppendPath("/sparkJobDefinitions/", false);
            uri.AppendPath(sparkJobDefinitionName, true);
            uri.AppendQuery("api-version", apiVersion, true);
            request.Uri = uri;
            request.Headers.Add("Accept", "application/json");
            return message;
        }

        /// <summary> Executes the spark job definition. </summary>
        /// <param name="sparkJobDefinitionName"> The spark job definition name. </param>
        /// <param name="requestOptions"> The request options. </param>
#pragma warning disable AZC0002
        public virtual async Task<Response> ExecuteSparkJobDefinitionAsync(string sparkJobDefinitionName, RequestOptions requestOptions = null)
#pragma warning restore AZC0002
        {
            requestOptions ??= new RequestOptions();
            HttpMessage message = CreateExecuteSparkJobDefinitionRequest(sparkJobDefinitionName, requestOptions);
            if (requestOptions.PerCallPolicy != null)
            {
                message.SetProperty("RequestOptionsPerCallPolicyCallback", requestOptions.PerCallPolicy);
            }
            using var scope = _clientDiagnostics.CreateScope("SparkJobDefinitionClient.ExecuteSparkJobDefinition");
            scope.Start();
            try
            {
                await Pipeline.SendAsync(message, requestOptions.CancellationToken).ConfigureAwait(false);
                if (requestOptions.StatusOption == ResponseStatusOption.Default)
                {
                    switch (message.Response.Status)
                    {
                        case 200:
                        case 202:
                            return message.Response;
                        default:
                            throw await _clientDiagnostics.CreateRequestFailedExceptionAsync(message.Response).ConfigureAwait(false);
                    }
                }
                else
                {
                    return message.Response;
                }
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Executes the spark job definition. </summary>
        /// <param name="sparkJobDefinitionName"> The spark job definition name. </param>
        /// <param name="requestOptions"> The request options. </param>
#pragma warning disable AZC0002
        public virtual Response ExecuteSparkJobDefinition(string sparkJobDefinitionName, RequestOptions requestOptions = null)
#pragma warning restore AZC0002
        {
            requestOptions ??= new RequestOptions();
            HttpMessage message = CreateExecuteSparkJobDefinitionRequest(sparkJobDefinitionName, requestOptions);
            if (requestOptions.PerCallPolicy != null)
            {
                message.SetProperty("RequestOptionsPerCallPolicyCallback", requestOptions.PerCallPolicy);
            }
            using var scope = _clientDiagnostics.CreateScope("SparkJobDefinitionClient.ExecuteSparkJobDefinition");
            scope.Start();
            try
            {
                Pipeline.Send(message, requestOptions.CancellationToken);
                if (requestOptions.StatusOption == ResponseStatusOption.Default)
                {
                    switch (message.Response.Status)
                    {
                        case 200:
                        case 202:
                            return message.Response;
                        default:
                            throw _clientDiagnostics.CreateRequestFailedException(message.Response);
                    }
                }
                else
                {
                    return message.Response;
                }
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Create Request for <see cref="ExecuteSparkJobDefinition"/> and <see cref="ExecuteSparkJobDefinitionAsync"/> operations. </summary>
        /// <param name="sparkJobDefinitionName"> The spark job definition name. </param>
        /// <param name="requestOptions"> The request options. </param>
        private HttpMessage CreateExecuteSparkJobDefinitionRequest(string sparkJobDefinitionName, RequestOptions requestOptions = null)
        {
            var message = Pipeline.CreateMessage();
            var request = message.Request;
            request.Method = RequestMethod.Post;
            var uri = new RawRequestUriBuilder();
            uri.Reset(endpoint);
            uri.AppendPath("/sparkJobDefinitions/", false);
            uri.AppendPath(sparkJobDefinitionName, true);
            uri.AppendPath("/execute", false);
            uri.AppendQuery("api-version", apiVersion, true);
            request.Uri = uri;
            request.Headers.Add("Accept", "application/json");
            return message;
        }

        /// <summary> Renames a sparkJobDefinition. </summary>
        /// <remarks>
        /// Schema for <c>Request Body</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>newName</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> New name of the artifact. </term>
        ///   </item>
        /// </list>
        /// </remarks>
        /// <param name="sparkJobDefinitionName"> The spark job definition name. </param>
        /// <param name="requestBody"> The request body. </param>
        /// <param name="requestOptions"> The request options. </param>
#pragma warning disable AZC0002
        public virtual async Task<Response> RenameSparkJobDefinitionAsync(string sparkJobDefinitionName, RequestContent requestBody, RequestOptions requestOptions = null)
#pragma warning restore AZC0002
        {
            requestOptions ??= new RequestOptions();
            HttpMessage message = CreateRenameSparkJobDefinitionRequest(sparkJobDefinitionName, requestBody, requestOptions);
            if (requestOptions.PerCallPolicy != null)
            {
                message.SetProperty("RequestOptionsPerCallPolicyCallback", requestOptions.PerCallPolicy);
            }
            using var scope = _clientDiagnostics.CreateScope("SparkJobDefinitionClient.RenameSparkJobDefinition");
            scope.Start();
            try
            {
                await Pipeline.SendAsync(message, requestOptions.CancellationToken).ConfigureAwait(false);
                if (requestOptions.StatusOption == ResponseStatusOption.Default)
                {
                    switch (message.Response.Status)
                    {
                        case 200:
                        case 202:
                            return message.Response;
                        default:
                            throw await _clientDiagnostics.CreateRequestFailedExceptionAsync(message.Response).ConfigureAwait(false);
                    }
                }
                else
                {
                    return message.Response;
                }
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Renames a sparkJobDefinition. </summary>
        /// <remarks>
        /// Schema for <c>Request Body</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>newName</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> New name of the artifact. </term>
        ///   </item>
        /// </list>
        /// </remarks>
        /// <param name="sparkJobDefinitionName"> The spark job definition name. </param>
        /// <param name="requestBody"> The request body. </param>
        /// <param name="requestOptions"> The request options. </param>
#pragma warning disable AZC0002
        public virtual Response RenameSparkJobDefinition(string sparkJobDefinitionName, RequestContent requestBody, RequestOptions requestOptions = null)
#pragma warning restore AZC0002
        {
            requestOptions ??= new RequestOptions();
            HttpMessage message = CreateRenameSparkJobDefinitionRequest(sparkJobDefinitionName, requestBody, requestOptions);
            if (requestOptions.PerCallPolicy != null)
            {
                message.SetProperty("RequestOptionsPerCallPolicyCallback", requestOptions.PerCallPolicy);
            }
            using var scope = _clientDiagnostics.CreateScope("SparkJobDefinitionClient.RenameSparkJobDefinition");
            scope.Start();
            try
            {
                Pipeline.Send(message, requestOptions.CancellationToken);
                if (requestOptions.StatusOption == ResponseStatusOption.Default)
                {
                    switch (message.Response.Status)
                    {
                        case 200:
                        case 202:
                            return message.Response;
                        default:
                            throw _clientDiagnostics.CreateRequestFailedException(message.Response);
                    }
                }
                else
                {
                    return message.Response;
                }
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Create Request for <see cref="RenameSparkJobDefinition"/> and <see cref="RenameSparkJobDefinitionAsync"/> operations. </summary>
        /// <param name="sparkJobDefinitionName"> The spark job definition name. </param>
        /// <param name="requestBody"> The request body. </param>
        /// <param name="requestOptions"> The request options. </param>
        private HttpMessage CreateRenameSparkJobDefinitionRequest(string sparkJobDefinitionName, RequestContent requestBody, RequestOptions requestOptions = null)
        {
            var message = Pipeline.CreateMessage();
            var request = message.Request;
            request.Method = RequestMethod.Post;
            var uri = new RawRequestUriBuilder();
            uri.Reset(endpoint);
            uri.AppendPath("/sparkJobDefinitions/", false);
            uri.AppendPath(sparkJobDefinitionName, true);
            uri.AppendPath("/rename", false);
            uri.AppendQuery("api-version", apiVersion, true);
            request.Uri = uri;
            request.Headers.Add("Accept", "application/json");
            request.Headers.Add("Content-Type", "application/json");
            request.Content = requestBody;
            return message;
        }

        /// <summary> Debug the spark job definition. </summary>
        /// <remarks>
        /// Schema for <c>Request Body</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>etag</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> Resource Etag. </term>
        ///   </item>
        ///   <item>
        ///     <term>id</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> Fully qualified resource ID for the resource. Ex - /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceType}/{resourceName}. </term>
        ///   </item>
        ///   <item>
        ///     <term>name</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The name of the resource. </term>
        ///   </item>
        ///   <item>
        ///     <term>type</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The type of the resource. E.g. &quot;Microsoft.Compute/virtualMachines&quot; or &quot;Microsoft.Storage/storageAccounts&quot;. </term>
        ///   </item>
        ///   <item>
        ///     <term>properties</term>
        ///     <term>SparkJobDefinition</term>
        ///     <term>Yes</term>
        ///     <term> Properties of spark job definition. </term>
        ///   </item>
        /// </list>
        /// Schema for <c>SparkJobDefinition</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>description</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The description of the Spark job definition. </term>
        ///   </item>
        ///   <item>
        ///     <term>targetBigDataPool</term>
        ///     <term>BigDataPoolReference</term>
        ///     <term>Yes</term>
        ///     <term> Big data pool reference. </term>
        ///   </item>
        ///   <item>
        ///     <term>requiredSparkVersion</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The required Spark version of the application. </term>
        ///   </item>
        ///   <item>
        ///     <term>language</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The language of the Spark application. </term>
        ///   </item>
        ///   <item>
        ///     <term>jobProperties</term>
        ///     <term>SparkJobProperties</term>
        ///     <term>Yes</term>
        ///     <term> The properties of the Spark job. </term>
        ///   </item>
        /// </list>
        /// Schema for <c>BigDataPoolReference</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>type</term>
        ///     <term>&quot;BigDataPoolReference&quot;</term>
        ///     <term>Yes</term>
        ///     <term> Big data pool reference type. </term>
        ///   </item>
        ///   <item>
        ///     <term>referenceName</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> Reference big data pool name. </term>
        ///   </item>
        /// </list>
        /// Schema for <c>SparkJobProperties</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>name</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The name of the job. </term>
        ///   </item>
        ///   <item>
        ///     <term>file</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> File containing the application to execute. </term>
        ///   </item>
        ///   <item>
        ///     <term>className</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> Main class for Java/Scala application. </term>
        ///   </item>
        ///   <item>
        ///     <term>conf</term>
        ///     <term>AnyObject</term>
        ///     <term></term>
        ///     <term> Spark configuration properties. </term>
        ///   </item>
        ///   <item>
        ///     <term>args</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> Command line arguments for the application. </term>
        ///   </item>
        ///   <item>
        ///     <term>jars</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> Jars to be used in this job. </term>
        ///   </item>
        ///   <item>
        ///     <term>files</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> files to be used in this job. </term>
        ///   </item>
        ///   <item>
        ///     <term>archives</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> Archives to be used in this job. </term>
        ///   </item>
        ///   <item>
        ///     <term>driverMemory</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> Amount of memory to use for the driver process. </term>
        ///   </item>
        ///   <item>
        ///     <term>driverCores</term>
        ///     <term>number</term>
        ///     <term>Yes</term>
        ///     <term> Number of cores to use for the driver. </term>
        ///   </item>
        ///   <item>
        ///     <term>executorMemory</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> Amount of memory to use per executor process. </term>
        ///   </item>
        ///   <item>
        ///     <term>executorCores</term>
        ///     <term>number</term>
        ///     <term>Yes</term>
        ///     <term> Number of cores to use for each executor. </term>
        ///   </item>
        ///   <item>
        ///     <term>numExecutors</term>
        ///     <term>number</term>
        ///     <term>Yes</term>
        ///     <term> Number of executors to launch for this job. </term>
        ///   </item>
        /// </list>
        /// </remarks>
        /// <param name="requestBody"> The request body. </param>
        /// <param name="requestOptions"> The request options. </param>
#pragma warning disable AZC0002
        public virtual async Task<Response> DebugSparkJobDefinitionAsync(RequestContent requestBody, RequestOptions requestOptions = null)
#pragma warning restore AZC0002
        {
            requestOptions ??= new RequestOptions();
            HttpMessage message = CreateDebugSparkJobDefinitionRequest(requestBody, requestOptions);
            if (requestOptions.PerCallPolicy != null)
            {
                message.SetProperty("RequestOptionsPerCallPolicyCallback", requestOptions.PerCallPolicy);
            }
            using var scope = _clientDiagnostics.CreateScope("SparkJobDefinitionClient.DebugSparkJobDefinition");
            scope.Start();
            try
            {
                await Pipeline.SendAsync(message, requestOptions.CancellationToken).ConfigureAwait(false);
                if (requestOptions.StatusOption == ResponseStatusOption.Default)
                {
                    switch (message.Response.Status)
                    {
                        case 200:
                        case 202:
                            return message.Response;
                        default:
                            throw await _clientDiagnostics.CreateRequestFailedExceptionAsync(message.Response).ConfigureAwait(false);
                    }
                }
                else
                {
                    return message.Response;
                }
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Debug the spark job definition. </summary>
        /// <remarks>
        /// Schema for <c>Request Body</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>etag</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> Resource Etag. </term>
        ///   </item>
        ///   <item>
        ///     <term>id</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> Fully qualified resource ID for the resource. Ex - /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceType}/{resourceName}. </term>
        ///   </item>
        ///   <item>
        ///     <term>name</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The name of the resource. </term>
        ///   </item>
        ///   <item>
        ///     <term>type</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The type of the resource. E.g. &quot;Microsoft.Compute/virtualMachines&quot; or &quot;Microsoft.Storage/storageAccounts&quot;. </term>
        ///   </item>
        ///   <item>
        ///     <term>properties</term>
        ///     <term>SparkJobDefinition</term>
        ///     <term>Yes</term>
        ///     <term> Properties of spark job definition. </term>
        ///   </item>
        /// </list>
        /// Schema for <c>SparkJobDefinition</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>description</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The description of the Spark job definition. </term>
        ///   </item>
        ///   <item>
        ///     <term>targetBigDataPool</term>
        ///     <term>BigDataPoolReference</term>
        ///     <term>Yes</term>
        ///     <term> Big data pool reference. </term>
        ///   </item>
        ///   <item>
        ///     <term>requiredSparkVersion</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The required Spark version of the application. </term>
        ///   </item>
        ///   <item>
        ///     <term>language</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The language of the Spark application. </term>
        ///   </item>
        ///   <item>
        ///     <term>jobProperties</term>
        ///     <term>SparkJobProperties</term>
        ///     <term>Yes</term>
        ///     <term> The properties of the Spark job. </term>
        ///   </item>
        /// </list>
        /// Schema for <c>BigDataPoolReference</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>type</term>
        ///     <term>&quot;BigDataPoolReference&quot;</term>
        ///     <term>Yes</term>
        ///     <term> Big data pool reference type. </term>
        ///   </item>
        ///   <item>
        ///     <term>referenceName</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> Reference big data pool name. </term>
        ///   </item>
        /// </list>
        /// Schema for <c>SparkJobProperties</c>:
        /// <list type="table">
        ///   <listeader>
        ///     <term>Name</term>
        ///     <term>Type</term>
        ///     <term>Required</term>
        ///     <term>Description</term>
        ///   </listeader>
        ///   <item>
        ///     <term>name</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> The name of the job. </term>
        ///   </item>
        ///   <item>
        ///     <term>file</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> File containing the application to execute. </term>
        ///   </item>
        ///   <item>
        ///     <term>className</term>
        ///     <term>string</term>
        ///     <term></term>
        ///     <term> Main class for Java/Scala application. </term>
        ///   </item>
        ///   <item>
        ///     <term>conf</term>
        ///     <term>AnyObject</term>
        ///     <term></term>
        ///     <term> Spark configuration properties. </term>
        ///   </item>
        ///   <item>
        ///     <term>args</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> Command line arguments for the application. </term>
        ///   </item>
        ///   <item>
        ///     <term>jars</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> Jars to be used in this job. </term>
        ///   </item>
        ///   <item>
        ///     <term>files</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> files to be used in this job. </term>
        ///   </item>
        ///   <item>
        ///     <term>archives</term>
        ///     <term>string[]</term>
        ///     <term></term>
        ///     <term> Archives to be used in this job. </term>
        ///   </item>
        ///   <item>
        ///     <term>driverMemory</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> Amount of memory to use for the driver process. </term>
        ///   </item>
        ///   <item>
        ///     <term>driverCores</term>
        ///     <term>number</term>
        ///     <term>Yes</term>
        ///     <term> Number of cores to use for the driver. </term>
        ///   </item>
        ///   <item>
        ///     <term>executorMemory</term>
        ///     <term>string</term>
        ///     <term>Yes</term>
        ///     <term> Amount of memory to use per executor process. </term>
        ///   </item>
        ///   <item>
        ///     <term>executorCores</term>
        ///     <term>number</term>
        ///     <term>Yes</term>
        ///     <term> Number of cores to use for each executor. </term>
        ///   </item>
        ///   <item>
        ///     <term>numExecutors</term>
        ///     <term>number</term>
        ///     <term>Yes</term>
        ///     <term> Number of executors to launch for this job. </term>
        ///   </item>
        /// </list>
        /// </remarks>
        /// <param name="requestBody"> The request body. </param>
        /// <param name="requestOptions"> The request options. </param>
#pragma warning disable AZC0002
        public virtual Response DebugSparkJobDefinition(RequestContent requestBody, RequestOptions requestOptions = null)
#pragma warning restore AZC0002
        {
            requestOptions ??= new RequestOptions();
            HttpMessage message = CreateDebugSparkJobDefinitionRequest(requestBody, requestOptions);
            if (requestOptions.PerCallPolicy != null)
            {
                message.SetProperty("RequestOptionsPerCallPolicyCallback", requestOptions.PerCallPolicy);
            }
            using var scope = _clientDiagnostics.CreateScope("SparkJobDefinitionClient.DebugSparkJobDefinition");
            scope.Start();
            try
            {
                Pipeline.Send(message, requestOptions.CancellationToken);
                if (requestOptions.StatusOption == ResponseStatusOption.Default)
                {
                    switch (message.Response.Status)
                    {
                        case 200:
                        case 202:
                            return message.Response;
                        default:
                            throw _clientDiagnostics.CreateRequestFailedException(message.Response);
                    }
                }
                else
                {
                    return message.Response;
                }
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary> Create Request for <see cref="DebugSparkJobDefinition"/> and <see cref="DebugSparkJobDefinitionAsync"/> operations. </summary>
        /// <param name="requestBody"> The request body. </param>
        /// <param name="requestOptions"> The request options. </param>
        private HttpMessage CreateDebugSparkJobDefinitionRequest(RequestContent requestBody, RequestOptions requestOptions = null)
        {
            var message = Pipeline.CreateMessage();
            var request = message.Request;
            request.Method = RequestMethod.Post;
            var uri = new RawRequestUriBuilder();
            uri.Reset(endpoint);
            uri.AppendPath("/debugSparkJobDefinition", false);
            uri.AppendQuery("api-version", apiVersion, true);
            request.Uri = uri;
            request.Headers.Add("Accept", "application/json");
            request.Headers.Add("Content-Type", "application/json");
            request.Content = requestBody;
            return message;
        }
    }
}
