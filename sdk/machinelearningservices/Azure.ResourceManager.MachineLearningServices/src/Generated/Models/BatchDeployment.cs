// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

// <auto-generated/>

#nullable disable

using System.Collections.Generic;
using Azure.Core;

namespace Azure.ResourceManager.MachineLearningServices.Models
{
    /// <summary> Batch inference settings per deployment. </summary>
    public partial class BatchDeployment
    {
        /// <summary> Initializes a new instance of BatchDeployment. </summary>
        public BatchDeployment()
        {
            EnvironmentVariables = new ChangeTrackingDictionary<string, string>();
            Properties = new ChangeTrackingDictionary<string, string>();
        }

        /// <summary> Initializes a new instance of BatchDeployment. </summary>
        /// <param name="codeConfiguration"> Code configuration for the endpoint deployment. </param>
        /// <param name="compute"> Compute target for batch inference operation. </param>
        /// <param name="description"> Description of the endpoint deployment. </param>
        /// <param name="environmentId"> ARM resource ID of the environment specification for the endpoint deployment. </param>
        /// <param name="environmentVariables"> Environment variables configuration for the deployment. </param>
        /// <param name="errorThreshold">
        /// Error threshold, if the error count for the entire input goes above this value,
        /// the batch inference will be aborted. Range is [-1, int.MaxValue].
        /// For FileDataset, this value is the count of file failures.
        /// For TabularDataset, this value is the count of record failures.
        /// If set to -1 (the lower bound), all failures during batch inference will be ignored.
        /// </param>
        /// <param name="loggingLevel"> Logging level for batch inference operation. </param>
        /// <param name="maxConcurrencyPerInstance"> Indicates maximum number of parallelism per instance. </param>
        /// <param name="miniBatchSize">
        /// Size of the mini-batch passed to each batch invocation.
        /// For FileDataset, this is the number of files per mini-batch.
        /// For TabularDataset, this is the size of the records in bytes, per mini-batch.
        /// </param>
        /// <param name="model"> Reference to the model asset for the endpoint deployment. </param>
        /// <param name="outputAction"> Indicates how the output will be organized. </param>
        /// <param name="outputFileName"> Customized output file name for append_row output action. </param>
        /// <param name="properties"> Property dictionary. Properties can be added, but not removed or altered. </param>
        /// <param name="provisioningState"> Provisioning state for the endpoint deployment. </param>
        /// <param name="resources"> Indicates compute configuration for the job. </param>
        /// <param name="retrySettings"> Retry Settings for the batch inference operation. </param>
        internal BatchDeployment(CodeConfiguration codeConfiguration, string compute, string description, string environmentId, IDictionary<string, string> environmentVariables, int? errorThreshold, BatchLoggingLevel? loggingLevel, int? maxConcurrencyPerInstance, long? miniBatchSize, AssetReferenceBase model, BatchOutputAction? outputAction, string outputFileName, IDictionary<string, string> properties, DeploymentProvisioningState? provisioningState, ResourceConfiguration resources, BatchRetrySettings retrySettings)
        {
            CodeConfiguration = codeConfiguration;
            Compute = compute;
            Description = description;
            EnvironmentId = environmentId;
            EnvironmentVariables = environmentVariables;
            ErrorThreshold = errorThreshold;
            LoggingLevel = loggingLevel;
            MaxConcurrencyPerInstance = maxConcurrencyPerInstance;
            MiniBatchSize = miniBatchSize;
            Model = model;
            OutputAction = outputAction;
            OutputFileName = outputFileName;
            Properties = properties;
            ProvisioningState = provisioningState;
            Resources = resources;
            RetrySettings = retrySettings;
        }

        /// <summary> Code configuration for the endpoint deployment. </summary>
        public CodeConfiguration CodeConfiguration { get; set; }
        /// <summary> Compute target for batch inference operation. </summary>
        public string Compute { get; set; }
        /// <summary> Description of the endpoint deployment. </summary>
        public string Description { get; set; }
        /// <summary> ARM resource ID of the environment specification for the endpoint deployment. </summary>
        public string EnvironmentId { get; set; }
        /// <summary> Environment variables configuration for the deployment. </summary>
        public IDictionary<string, string> EnvironmentVariables { get; set; }
        /// <summary>
        /// Error threshold, if the error count for the entire input goes above this value,
        /// the batch inference will be aborted. Range is [-1, int.MaxValue].
        /// For FileDataset, this value is the count of file failures.
        /// For TabularDataset, this value is the count of record failures.
        /// If set to -1 (the lower bound), all failures during batch inference will be ignored.
        /// </summary>
        public int? ErrorThreshold { get; set; }
        /// <summary> Logging level for batch inference operation. </summary>
        public BatchLoggingLevel? LoggingLevel { get; set; }
        /// <summary> Indicates maximum number of parallelism per instance. </summary>
        public int? MaxConcurrencyPerInstance { get; set; }
        /// <summary>
        /// Size of the mini-batch passed to each batch invocation.
        /// For FileDataset, this is the number of files per mini-batch.
        /// For TabularDataset, this is the size of the records in bytes, per mini-batch.
        /// </summary>
        public long? MiniBatchSize { get; set; }
        /// <summary> Reference to the model asset for the endpoint deployment. </summary>
        public AssetReferenceBase Model { get; set; }
        /// <summary> Indicates how the output will be organized. </summary>
        public BatchOutputAction? OutputAction { get; set; }
        /// <summary> Customized output file name for append_row output action. </summary>
        public string OutputFileName { get; set; }
        /// <summary> Property dictionary. Properties can be added, but not removed or altered. </summary>
        public IDictionary<string, string> Properties { get; set; }
        /// <summary> Provisioning state for the endpoint deployment. </summary>
        public DeploymentProvisioningState? ProvisioningState { get; }
        /// <summary> Indicates compute configuration for the job. </summary>
        public ResourceConfiguration Resources { get; set; }
        /// <summary> Retry Settings for the batch inference operation. </summary>
        public BatchRetrySettings RetrySettings { get; set; }
    }
}
