// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

// <auto-generated/>

#nullable disable

namespace Azure.ResourceManager.MachineLearningServices.Models
{
    /// <summary> Batch inference specific settings per deployment. </summary>
    public partial class BatchDeploymentSettings
    {
        /// <summary> Initializes a new instance of BatchDeploymentSettings. </summary>
        public BatchDeploymentSettings()
        {
        }

        /// <summary> Initializes a new instance of BatchDeploymentSettings. </summary>
        /// <param name="computeId"> Resource ID of the compute resource. </param>
        /// <param name="errorThreshold">
        /// Error threshold, if the error count for the entire input goes above this value,
        /// 
        /// the batch inference will be aborted. Range is [-1, int.MaxValue]
        /// 
        /// -1 value indicates, ignore all failures during batch inference
        /// 
        /// For FileDataset count of file failures
        /// 
        /// For TabularDataset, this is the count of record failures.
        /// </param>
        /// <param name="retrySettings"> Retry settings for a batch inference operation. </param>
        /// <param name="partitioningScheme"> Partitioning scheme for batch inference operation. </param>
        /// <param name="loggingLevel"> Logging level for batch inference operation. </param>
        /// <param name="outputConfiguration"> Batch inference output configuration. </param>
        internal BatchDeploymentSettings(string computeId, int? errorThreshold, BatchRetrySettings retrySettings, BatchPartitioningScheme partitioningScheme, BatchLoggingLevel? loggingLevel, BatchOutputConfiguration outputConfiguration)
        {
            ComputeId = computeId;
            ErrorThreshold = errorThreshold;
            RetrySettings = retrySettings;
            PartitioningScheme = partitioningScheme;
            LoggingLevel = loggingLevel;
            OutputConfiguration = outputConfiguration;
        }

        /// <summary> Resource ID of the compute resource. </summary>
        public string ComputeId { get; set; }
        /// <summary>
        /// Error threshold, if the error count for the entire input goes above this value,
        /// 
        /// the batch inference will be aborted. Range is [-1, int.MaxValue]
        /// 
        /// -1 value indicates, ignore all failures during batch inference
        /// 
        /// For FileDataset count of file failures
        /// 
        /// For TabularDataset, this is the count of record failures.
        /// </summary>
        public int? ErrorThreshold { get; set; }
        /// <summary> Retry settings for a batch inference operation. </summary>
        public BatchRetrySettings RetrySettings { get; set; }
        /// <summary> Partitioning scheme for batch inference operation. </summary>
        public BatchPartitioningScheme PartitioningScheme { get; set; }
        /// <summary> Logging level for batch inference operation. </summary>
        public BatchLoggingLevel? LoggingLevel { get; set; }
        /// <summary> Batch inference output configuration. </summary>
        public BatchOutputConfiguration OutputConfiguration { get; set; }
    }
}
