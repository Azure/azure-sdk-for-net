// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

// <auto-generated/>

#nullable disable

using System;
using System.Collections.Generic;

namespace Azure.ResourceManager.MachineLearning.Models
{
    /// <summary> Triton inferencing server configurations. </summary>
    public partial class TritonInferencingServer : InferencingServer
    {
        /// <summary> Initializes a new instance of <see cref="TritonInferencingServer"/>. </summary>
        public TritonInferencingServer()
        {
            ServerType = InferencingServerType.Triton;
        }

        /// <summary> Initializes a new instance of <see cref="TritonInferencingServer"/>. </summary>
        /// <param name="serverType"> [Required] Inferencing server type for various targets. </param>
        /// <param name="serializedAdditionalRawData"> Keeps track of any properties unknown to the library. </param>
        /// <param name="inferenceConfiguration"> Inference configuration for Triton. </param>
        internal TritonInferencingServer(InferencingServerType serverType, IDictionary<string, BinaryData> serializedAdditionalRawData, OnlineInferenceConfiguration inferenceConfiguration) : base(serverType, serializedAdditionalRawData)
        {
            InferenceConfiguration = inferenceConfiguration;
            ServerType = serverType;
        }

        /// <summary> Inference configuration for Triton. </summary>
        public OnlineInferenceConfiguration InferenceConfiguration { get; set; }
    }
}
