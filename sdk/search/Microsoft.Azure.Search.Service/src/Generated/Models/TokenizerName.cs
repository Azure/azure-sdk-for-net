// <auto-generated>
// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License. See License.txt in the project root for
// license information.
//
// Code generated by Microsoft (R) AutoRest Code Generator.
// Changes may cause incorrect behavior and will be lost if the code is
// regenerated.
// </auto-generated>

namespace Microsoft.Azure.Search.Models
{
    using Newtonsoft.Json;

    /// <summary>
    /// Defines values for TokenizerName.
    /// </summary>
    /// <summary>
    /// Determine base value for a given allowed value if exists, else return
    /// the value itself
    /// </summary>
    [JsonConverter(typeof(TokenizerNameConverter))]
    public struct TokenizerName : System.IEquatable<TokenizerName>
    {
        private TokenizerName(string underlyingValue)
        {
            UnderlyingValue=underlyingValue;
        }

        /// <summary>
        /// Grammar-based tokenizer that is suitable for processing most
        /// European-language documents. See
        /// http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/ClassicTokenizer.html
        /// </summary>
        public static readonly TokenizerName Classic = "classic";

        /// <summary>
        /// Tokenizes the input from an edge into n-grams of the given size(s).
        /// See
        /// https://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.html
        /// </summary>
        public static readonly TokenizerName EdgeNGram = "edgeNGram";

        /// <summary>
        /// Emits the entire input as a single token. See
        /// http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/KeywordTokenizer.html
        /// </summary>
        public static readonly TokenizerName Keyword = "keyword_v2";

        /// <summary>
        /// Divides text at non-letters. See
        /// http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LetterTokenizer.html
        /// </summary>
        public static readonly TokenizerName Letter = "letter";

        /// <summary>
        /// Divides text at non-letters and converts them to lower case. See
        /// http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LowerCaseTokenizer.html
        /// </summary>
        public static readonly TokenizerName Lowercase = "lowercase";

        /// <summary>
        /// Divides text using language-specific rules.
        /// </summary>
        public static readonly TokenizerName MicrosoftLanguageTokenizer = "microsoft_language_tokenizer";

        /// <summary>
        /// Divides text using language-specific rules and reduces words to
        /// their base forms.
        /// </summary>
        public static readonly TokenizerName MicrosoftLanguageStemmingTokenizer = "microsoft_language_stemming_tokenizer";

        /// <summary>
        /// Tokenizes the input into n-grams of the given size(s). See
        /// http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenizer.html
        /// </summary>
        public static readonly TokenizerName NGram = "nGram";

        /// <summary>
        /// Tokenizer for path-like hierarchies. See
        /// http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/path/PathHierarchyTokenizer.html
        /// </summary>
        public static readonly TokenizerName PathHierarchy = "path_hierarchy_v2";

        /// <summary>
        /// Tokenizer that uses regex pattern matching to construct distinct
        /// tokens. See
        /// http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/pattern/PatternTokenizer.html
        /// </summary>
        public static readonly TokenizerName Pattern = "pattern";

        /// <summary>
        /// Standard Lucene analyzer; Composed of the standard tokenizer,
        /// lowercase filter and stop filter. See
        /// http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/StandardTokenizer.html
        /// </summary>
        public static readonly TokenizerName Standard = "standard_v2";

        /// <summary>
        /// Tokenizes urls and emails as one token. See
        /// http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.html
        /// </summary>
        public static readonly TokenizerName UaxUrlEmail = "uax_url_email";

        /// <summary>
        /// Divides text at whitespace. See
        /// http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/WhitespaceTokenizer.html
        /// </summary>
        public static readonly TokenizerName Whitespace = "whitespace";


        /// <summary>
        /// Underlying value of enum TokenizerName
        /// </summary>
        private readonly string UnderlyingValue;

        /// <summary>
        /// Returns string representation for TokenizerName
        /// </summary>
        public override string ToString()
        {
            return UnderlyingValue == null ? null : UnderlyingValue.ToString();
        }

        /// <summary>
        /// Compares enums of type TokenizerName
        /// </summary>
        public bool Equals(TokenizerName e)
        {
            return UnderlyingValue.Equals(e.UnderlyingValue);
        }

        /// <summary>
        /// Implicit operator to convert string to TokenizerName
        /// </summary>
        public static implicit operator TokenizerName(string value)
        {
            return new TokenizerName(value);
        }

        /// <summary>
        /// Implicit operator to convert TokenizerName to string
        /// </summary>
        public static implicit operator string(TokenizerName e)
        {
            return e.UnderlyingValue;
        }

        /// <summary>
        /// Overriding == operator for enum TokenizerName
        /// </summary>
        public static bool operator == (TokenizerName e1, TokenizerName e2)
        {
            return e2.Equals(e1);
        }

        /// <summary>
        /// Overriding != operator for enum TokenizerName
        /// </summary>
        public static bool operator != (TokenizerName e1, TokenizerName e2)
        {
            return !e2.Equals(e1);
        }

        /// <summary>
        /// Overrides Equals operator for TokenizerName
        /// </summary>
        public override bool Equals(object obj)
        {
            return obj is TokenizerName && Equals((TokenizerName)obj);
        }

        /// <summary>
        /// Returns for hashCode TokenizerName
        /// </summary>
        public override int GetHashCode()
        {
            return UnderlyingValue.GetHashCode();
        }

    }
}
