// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

// <auto-generated/>

#nullable disable

using System;
using System.ClientModel.Primitives;
using System.Text.Json;
using Azure.Search.Documents;

namespace Azure.Search.Documents.Models
{
    /// <summary>
    /// Base type for tokenizers.
    /// Please note this is the abstract base class. The derived classes available for instantiation are: <see cref="ClassicTokenizer"/>, <see cref="EdgeNGramTokenizer"/>, <see cref="KeywordTokenizer"/>, <see cref="KeywordTokenizerV2"/>, <see cref="MicrosoftLanguageTokenizer"/>, <see cref="MicrosoftLanguageStemmingTokenizer"/>, <see cref="NGramTokenizer"/>, <see cref="PathHierarchyTokenizerV2"/>, <see cref="PatternTokenizer"/>, <see cref="LuceneStandardTokenizer"/>, <see cref="LuceneStandardTokenizerV2"/>, and <see cref="UaxUrlEmailTokenizer"/>.
    /// </summary>
    [PersistableModelProxy(typeof(UnknownLexicalTokenizer))]
    public abstract partial class LexicalTokenizer : IJsonModel<LexicalTokenizer>
    {
        /// <summary> Initializes a new instance of <see cref="LexicalTokenizer"/> for deserialization. </summary>
        internal LexicalTokenizer()
        {
        }

        /// <param name="writer"> The JSON writer. </param>
        /// <param name="options"> The client options for reading and writing models. </param>
        void IJsonModel<LexicalTokenizer>.Write(Utf8JsonWriter writer, ModelReaderWriterOptions options)
        {
            writer.WriteStartObject();
            JsonModelWriteCore(writer, options);
            writer.WriteEndObject();
        }

        /// <param name="writer"> The JSON writer. </param>
        /// <param name="options"> The client options for reading and writing models. </param>
        protected virtual void JsonModelWriteCore(Utf8JsonWriter writer, ModelReaderWriterOptions options)
        {
            string format = options.Format == "W" ? ((IPersistableModel<LexicalTokenizer>)this).GetFormatFromOptions(options) : options.Format;
            if (format != "J")
            {
                throw new FormatException($"The model {nameof(LexicalTokenizer)} does not support writing '{format}' format.");
            }
            writer.WritePropertyName("@odata.type"u8);
            writer.WriteStringValue(OdataType);
            writer.WritePropertyName("name"u8);
            writer.WriteStringValue(Name);
            if (options.Format != "W" && _additionalBinaryDataProperties != null)
            {
                foreach (var item in _additionalBinaryDataProperties)
                {
                    writer.WritePropertyName(item.Key);
#if NET6_0_OR_GREATER
                    writer.WriteRawValue(item.Value);
#else
                    using (JsonDocument document = JsonDocument.Parse(item.Value))
                    {
                        JsonSerializer.Serialize(writer, document.RootElement);
                    }
#endif
                }
            }
        }

        /// <param name="reader"> The JSON reader. </param>
        /// <param name="options"> The client options for reading and writing models. </param>
        LexicalTokenizer IJsonModel<LexicalTokenizer>.Create(ref Utf8JsonReader reader, ModelReaderWriterOptions options) => JsonModelCreateCore(ref reader, options);

        /// <param name="reader"> The JSON reader. </param>
        /// <param name="options"> The client options for reading and writing models. </param>
        protected virtual LexicalTokenizer JsonModelCreateCore(ref Utf8JsonReader reader, ModelReaderWriterOptions options)
        {
            string format = options.Format == "W" ? ((IPersistableModel<LexicalTokenizer>)this).GetFormatFromOptions(options) : options.Format;
            if (format != "J")
            {
                throw new FormatException($"The model {nameof(LexicalTokenizer)} does not support reading '{format}' format.");
            }
            using JsonDocument document = JsonDocument.ParseValue(ref reader);
            return DeserializeLexicalTokenizer(document.RootElement, options);
        }

        /// <param name="element"> The JSON element to deserialize. </param>
        /// <param name="options"> The client options for reading and writing models. </param>
        internal static LexicalTokenizer DeserializeLexicalTokenizer(JsonElement element, ModelReaderWriterOptions options)
        {
            if (element.ValueKind == JsonValueKind.Null)
            {
                return null;
            }
            if (element.TryGetProperty("@odata.type"u8, out JsonElement discriminator))
            {
                switch (discriminator.GetString())
                {
                    case "#Microsoft.Azure.Search.ClassicTokenizer":
                        return ClassicTokenizer.DeserializeClassicTokenizer(element, options);
                    case "#Microsoft.Azure.Search.EdgeNGramTokenizer":
                        return EdgeNGramTokenizer.DeserializeEdgeNGramTokenizer(element, options);
                    case "#Microsoft.Azure.Search.KeywordTokenizer":
                        return KeywordTokenizer.DeserializeKeywordTokenizer(element, options);
                    case "#Microsoft.Azure.Search.KeywordTokenizerV2":
                        return KeywordTokenizerV2.DeserializeKeywordTokenizerV2(element, options);
                    case "#Microsoft.Azure.Search.MicrosoftLanguageTokenizer":
                        return MicrosoftLanguageTokenizer.DeserializeMicrosoftLanguageTokenizer(element, options);
                    case "#Microsoft.Azure.Search.MicrosoftLanguageStemmingTokenizer":
                        return MicrosoftLanguageStemmingTokenizer.DeserializeMicrosoftLanguageStemmingTokenizer(element, options);
                    case "#Microsoft.Azure.Search.NGramTokenizer":
                        return NGramTokenizer.DeserializeNGramTokenizer(element, options);
                    case "#Microsoft.Azure.Search.PathHierarchyTokenizerV2":
                        return PathHierarchyTokenizerV2.DeserializePathHierarchyTokenizerV2(element, options);
                    case "#Microsoft.Azure.Search.PatternTokenizer":
                        return PatternTokenizer.DeserializePatternTokenizer(element, options);
                    case "#Microsoft.Azure.Search.StandardTokenizer":
                        return LuceneStandardTokenizer.DeserializeLuceneStandardTokenizer(element, options);
                    case "#Microsoft.Azure.Search.StandardTokenizerV2":
                        return LuceneStandardTokenizerV2.DeserializeLuceneStandardTokenizerV2(element, options);
                    case "#Microsoft.Azure.Search.UaxUrlEmailTokenizer":
                        return UaxUrlEmailTokenizer.DeserializeUaxUrlEmailTokenizer(element, options);
                }
            }
            return UnknownLexicalTokenizer.DeserializeUnknownLexicalTokenizer(element, options);
        }

        /// <param name="options"> The client options for reading and writing models. </param>
        BinaryData IPersistableModel<LexicalTokenizer>.Write(ModelReaderWriterOptions options) => PersistableModelWriteCore(options);

        /// <param name="options"> The client options for reading and writing models. </param>
        protected virtual BinaryData PersistableModelWriteCore(ModelReaderWriterOptions options)
        {
            string format = options.Format == "W" ? ((IPersistableModel<LexicalTokenizer>)this).GetFormatFromOptions(options) : options.Format;
            switch (format)
            {
                case "J":
                    return ModelReaderWriter.Write(this, options, AzureSearchDocumentsContext.Default);
                default:
                    throw new FormatException($"The model {nameof(LexicalTokenizer)} does not support writing '{options.Format}' format.");
            }
        }

        /// <param name="data"> The data to parse. </param>
        /// <param name="options"> The client options for reading and writing models. </param>
        LexicalTokenizer IPersistableModel<LexicalTokenizer>.Create(BinaryData data, ModelReaderWriterOptions options) => PersistableModelCreateCore(data, options);

        /// <param name="data"> The data to parse. </param>
        /// <param name="options"> The client options for reading and writing models. </param>
        protected virtual LexicalTokenizer PersistableModelCreateCore(BinaryData data, ModelReaderWriterOptions options)
        {
            string format = options.Format == "W" ? ((IPersistableModel<LexicalTokenizer>)this).GetFormatFromOptions(options) : options.Format;
            switch (format)
            {
                case "J":
                    using (JsonDocument document = JsonDocument.Parse(data, ModelSerializationExtensions.JsonDocumentOptions))
                    {
                        return DeserializeLexicalTokenizer(document.RootElement, options);
                    }
                default:
                    throw new FormatException($"The model {nameof(LexicalTokenizer)} does not support reading '{options.Format}' format.");
            }
        }

        /// <param name="options"> The client options for reading and writing models. </param>
        string IPersistableModel<LexicalTokenizer>.GetFormatFromOptions(ModelReaderWriterOptions options) => "J";
    }
}
