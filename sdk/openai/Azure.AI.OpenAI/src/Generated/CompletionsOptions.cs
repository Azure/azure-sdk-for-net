// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

// <auto-generated/>

#nullable disable

using System.Collections.Generic;
using System.Linq;
using Azure.Core;

namespace Azure.AI.OpenAI
{
    /// <summary> Post body schema to create a prompt completion from a deployment. </summary>
    public partial class CompletionsOptions
    {
        /// <summary> Initializes a new instance of CompletionsOptions. </summary>
        public CompletionsOptions()
        {
            Prompt = new ChangeTrackingList<string>();
            LogitBias = new ChangeTrackingDictionary<string, int>();
            Stop = new ChangeTrackingList<string>();
        }

        /// <summary> Initializes a new instance of CompletionsOptions. </summary>
        /// <param name="prompt">
        /// An optional prompt to complete from, encoded as a string, a list of strings, or
        /// a list of token lists. Defaults to &lt;|endoftext|&gt;. The prompt to complete from.
        /// If you would like to provide multiple prompts, use the POST variant of this
        /// method. Note that &lt;|endoftext|&gt; is the document separator that the model sees
        /// during training, so if a prompt is not specified the model will generate as if
        /// from the beginning of a new document. Maximum allowed size of string list is
        /// 2048.
        /// </param>
        /// <param name="maxTokens"> The maximum number of tokens to generate. Has minimum of 0. </param>
        /// <param name="temperature">
        /// What sampling temperature to use. Higher values means the model will take more
        /// risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones
        /// with a well-defined answer.
        /// We generally recommend using this or `top_p` but
        /// not both.
        /// Minimum of 0 and maximum of 2 allowed.
        /// 
        /// </param>
        /// <param name="nucleusSamplingFactor">
        /// An alternative to sampling with temperature, called nucleus sampling, where the
        /// model considers the results of the tokens with top_p probability mass. So 0.1
        /// means only the tokens comprising the top 10% probability mass are
        /// considered.
        /// We generally recommend using this or `temperature` but not
        /// both.
        /// Minimum of 0 and maximum of 1 allowed.
        /// 
        /// </param>
        /// <param name="logitBias">
        /// Defaults to null. Modify the likelihood of specified tokens appearing in the
        /// completion. Accepts a json object that maps tokens (specified by their token ID
        /// in the GPT tokenizer) to an associated bias value from -100 to 100. You can use
        /// this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to
        /// token IDs. Mathematically, the bias is added to the logits generated by the
        /// model prior to sampling. The exact effect will vary per model, but values
        /// between -1 and 1 should decrease or increase likelihood of selection; values
        /// like -100 or 100 should result in a ban or exclusive selection of the relevant
        /// token. As an example, you can pass {"50256" &amp;#58; -100} to prevent the
        /// &lt;|endoftext|&gt; token from being generated.
        /// </param>
        /// <param name="user"> The ID of the end-user, for use in tracking and rate-limiting. </param>
        /// <param name="snippetCount">
        /// How many snippets to generate for each prompt. Minimum of 1 and maximum of 128
        /// allowed.
        /// </param>
        /// <param name="logProbability">
        /// Include the log probabilities on the `logprobs` most likely tokens, as well the
        /// chosen tokens. So for example, if `logprobs` is 10, the API will return a list
        /// of the 10 most likely tokens. If `logprobs` is 0, only the chosen tokens will
        /// have logprobs returned. Minimum of 0 and maximum of 100 allowed.
        /// </param>
        /// <param name="model"> The name of the model to use. </param>
        /// <param name="echo"> Echo back the prompt in addition to the completion. </param>
        /// <param name="stop"> A sequence which indicates the end of the current document. </param>
        /// <param name="completionConfig"> Completion configuration. </param>
        /// <param name="cacheLevel">
        /// can be used to disable any server-side caching, 0=no cache, 1=prompt prefix
        /// enabled, 2=full cache
        /// </param>
        /// <param name="presencePenalty">
        /// How much to penalize new tokens based on their existing frequency in the text
        /// so far. Decreases the model's likelihood to repeat the same line verbatim. Has
        /// minimum of -2 and maximum of 2.
        /// </param>
        /// <param name="frequencyPenalty">
        /// How much to penalize new tokens based on whether they appear in the text so
        /// far. Increases the model's likelihood to talk about new topics.
        /// </param>
        /// <param name="generationSampleCount">
        /// How many generations to create server side, and display only the best. Will not
        /// stream intermediate progress if best_of &gt; 1. Has maximum value of 128.
        /// </param>
        internal CompletionsOptions(IList<string> prompt, int? maxTokens, float? temperature, float? nucleusSamplingFactor, IDictionary<string, int> logitBias, string user, int? snippetCount, int? logProbability, string model, bool? echo, IList<string> stop, string completionConfig, int? cacheLevel, float? presencePenalty, float? frequencyPenalty, int? generationSampleCount)
        {
            Prompt = prompt.ToList();
            MaxTokens = maxTokens;
            Temperature = temperature;
            NucleusSamplingFactor = nucleusSamplingFactor;
            LogitBias = logitBias;
            User = user;
            SnippetCount = snippetCount;
            LogProbability = logProbability;
            Model = model;
            Echo = echo;
            Stop = stop.ToList();
            CompletionConfig = completionConfig;
            CacheLevel = cacheLevel;
            PresencePenalty = presencePenalty;
            FrequencyPenalty = frequencyPenalty;
            GenerationSampleCount = generationSampleCount;
        }

        /// <summary>
        /// An optional prompt to complete from, encoded as a string, a list of strings, or
        /// a list of token lists. Defaults to &lt;|endoftext|&gt;. The prompt to complete from.
        /// If you would like to provide multiple prompts, use the POST variant of this
        /// method. Note that &lt;|endoftext|&gt; is the document separator that the model sees
        /// during training, so if a prompt is not specified the model will generate as if
        /// from the beginning of a new document. Maximum allowed size of string list is
        /// 2048.
        /// </summary>
        public IList<string> Prompt { get; }
        /// <summary> The maximum number of tokens to generate. Has minimum of 0. </summary>
        public int? MaxTokens { get; set; }
        /// <summary>
        /// What sampling temperature to use. Higher values means the model will take more
        /// risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones
        /// with a well-defined answer.
        /// We generally recommend using this or `top_p` but
        /// not both.
        /// Minimum of 0 and maximum of 2 allowed.
        /// 
        /// </summary>
        public float? Temperature { get; set; }
        /// <summary>
        /// Defaults to null. Modify the likelihood of specified tokens appearing in the
        /// completion. Accepts a json object that maps tokens (specified by their token ID
        /// in the GPT tokenizer) to an associated bias value from -100 to 100. You can use
        /// this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to
        /// token IDs. Mathematically, the bias is added to the logits generated by the
        /// model prior to sampling. The exact effect will vary per model, but values
        /// between -1 and 1 should decrease or increase likelihood of selection; values
        /// like -100 or 100 should result in a ban or exclusive selection of the relevant
        /// token. As an example, you can pass {"50256" &amp;#58; -100} to prevent the
        /// &lt;|endoftext|&gt; token from being generated.
        /// </summary>
        public IDictionary<string, int> LogitBias { get; }
        /// <summary> The ID of the end-user, for use in tracking and rate-limiting. </summary>
        public string User { get; set; }
        /// <summary> The name of the model to use. </summary>
        public string Model { get; set; }
        /// <summary> Echo back the prompt in addition to the completion. </summary>
        public bool? Echo { get; set; }
        /// <summary> A sequence which indicates the end of the current document. </summary>
        public IList<string> Stop { get; }
        /// <summary> Completion configuration. </summary>
        public string CompletionConfig { get; set; }
        /// <summary>
        /// can be used to disable any server-side caching, 0=no cache, 1=prompt prefix
        /// enabled, 2=full cache
        /// </summary>
        public int? CacheLevel { get; set; }
        /// <summary>
        /// How much to penalize new tokens based on their existing frequency in the text
        /// so far. Decreases the model's likelihood to repeat the same line verbatim. Has
        /// minimum of -2 and maximum of 2.
        /// </summary>
        public float? PresencePenalty { get; set; }
        /// <summary>
        /// How much to penalize new tokens based on whether they appear in the text so
        /// far. Increases the model's likelihood to talk about new topics.
        /// </summary>
        public float? FrequencyPenalty { get; set; }
    }
}
