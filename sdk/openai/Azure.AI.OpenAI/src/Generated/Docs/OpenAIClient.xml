<?xml version="1.0" encoding="utf-8"?>
<doc>
  <members>
    <member name="GetEmbeddingsAsync(String,RequestContent,RequestContext)">
<example>
This sample shows how to call GetEmbeddingsAsync with required parameters and request content, and how to parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {
    input = new {},
};

Response response = await client.GetEmbeddingsAsync("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("data")[0].GetProperty("embedding")[0].ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("index").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
This sample shows how to call GetEmbeddingsAsync with all parameters and request content, and how to parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {
    user = "<user>",
    input_type = "<input_type>",
    model = "<model>",
    input = new {},
};

Response response = await client.GetEmbeddingsAsync("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("data")[0].GetProperty("embedding")[0].ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("index").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
</example>
<remarks>
Below is the JSON schema for the request and response payloads.

Request Body:

Schema for <c>EmbeddingsOptions</c>:
<code>{
  user: string, # Optional. The ID of the end-user, for use in tracking and rate-limiting.
  input_type: string, # Optional. input type of embedding search to use
  model: string, # Optional. The model name to provide as part of this completions request.
Not applicable to Azure OpenAI, where deployment information should be included in the Azure
resource URI that&apos;s connected to.
  input: Union, # Required. Input text to get embeddings for, encoded as a string.
To get embeddings for multiple inputs in a single request, pass an array of strings.
Each input must not exceed 2048 tokens in length.

Unless you are embedding code, we suggest replacing newlines (\n) in your input with a single space,
as we have observed inferior results when newlines are present.
}
</code>

Response Body:

Schema for <c>Embeddings</c>:
<code>{
  data: [
    {
      embedding: [number], # Required. List of embeddings value for the input prompt. These represents a measurement of releated of text strings
      index: number, # Required. Index of the prompt to which the EmbeddingItem corresponds
    }
  ], # Required. Embedding values for the prompts submitted in the request
  usage: {
    prompt_tokens: number, # Required. Number of tokens sent in the original request
    total_tokens: number, # Required. Total number of tokens transacted in this request/response
  }, # Required. Usage counts for tokens input using the embeddings API
}
</code>

</remarks>
    </member>
    <member name="GetEmbeddings(String,RequestContent,RequestContext)">
<example>
This sample shows how to call GetEmbeddings with required parameters and request content, and how to parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {
    input = new {},
};

Response response = client.GetEmbeddings("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("data")[0].GetProperty("embedding")[0].ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("index").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
This sample shows how to call GetEmbeddings with all parameters and request content, and how to parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {
    user = "<user>",
    input_type = "<input_type>",
    model = "<model>",
    input = new {},
};

Response response = client.GetEmbeddings("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("data")[0].GetProperty("embedding")[0].ToString());
Console.WriteLine(result.GetProperty("data")[0].GetProperty("index").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
</example>
<remarks>
Below is the JSON schema for the request and response payloads.

Request Body:

Schema for <c>EmbeddingsOptions</c>:
<code>{
  user: string, # Optional. The ID of the end-user, for use in tracking and rate-limiting.
  input_type: string, # Optional. input type of embedding search to use
  model: string, # Optional. The model name to provide as part of this completions request.
Not applicable to Azure OpenAI, where deployment information should be included in the Azure
resource URI that&apos;s connected to.
  input: Union, # Required. Input text to get embeddings for, encoded as a string.
To get embeddings for multiple inputs in a single request, pass an array of strings.
Each input must not exceed 2048 tokens in length.

Unless you are embedding code, we suggest replacing newlines (\n) in your input with a single space,
as we have observed inferior results when newlines are present.
}
</code>

Response Body:

Schema for <c>Embeddings</c>:
<code>{
  data: [
    {
      embedding: [number], # Required. List of embeddings value for the input prompt. These represents a measurement of releated of text strings
      index: number, # Required. Index of the prompt to which the EmbeddingItem corresponds
    }
  ], # Required. Embedding values for the prompts submitted in the request
  usage: {
    prompt_tokens: number, # Required. Number of tokens sent in the original request
    total_tokens: number, # Required. Total number of tokens transacted in this request/response
  }, # Required. Usage counts for tokens input using the embeddings API
}
</code>

</remarks>
    </member>
    <member name="GetCompletionsAsync(String,RequestContent,RequestContext)">
<example>
This sample shows how to call GetCompletionsAsync with required parameters and parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {};

Response response = await client.GetCompletionsAsync("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("created").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("completion_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
This sample shows how to call GetCompletionsAsync with all parameters and request content, and how to parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {
    prompt = new[] {
        "<String>"
    },
    max_tokens = 1234,
    temperature = 123.45f,
    top_p = 123.45f,
    logit_bias = new {
        key = 1234,
    },
    user = "<user>",
    n = 1234,
    logprobs = 1234,
    echo = true,
    stop = new[] {
        "<String>"
    },
    presence_penalty = 123.45f,
    frequency_penalty = 123.45f,
    best_of = 1234,
    stream = true,
    model = "<model>",
};

Response response = await client.GetCompletionsAsync("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("created").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("text").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("index").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("tokens")[0].ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("token_logprobs")[0].ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("top_logprobs")[0].GetProperty("<test>").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("text_offset")[0].ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("finish_reason").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("completion_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
</example>
<remarks>
Below is the JSON schema for the request and response payloads.

Request Body:

Schema for <c>CompletionsOptions</c>:
<code>{
  prompt: [string], # Optional. The prompts to generate completions from. Defaults to a single prompt of &amp;lt;|endoftext|&amp;gt; if not
otherwise specified.
  max_tokens: number, # Optional. The maximum number of tokens to generate.
  temperature: number, # Optional. The sampling temperature to use that controls the apparent creativity of generated completions.
Higher values will make output more random while lower values will make results more focused
and deterministic.
It is not recommended to modify temperature and top_p for the same completions request as the
interaction of these two settings is difficult to predict.
  top_p: number, # Optional. An alternative to sampling with temperature called nucleus sampling. This value causes the
model to consider the results of tokens with the provided probability mass. As an example, a
value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be
considered.
It is not recommended to modify temperature and top_p for the same completions request as the
interaction of these two settings is difficult to predict.
  logit_bias: Dictionary&lt;string, number&gt;, # Optional. A map between GPT token IDs and bias scores that influences the probability of specific tokens
appearing in a completions response. Token IDs are computed via external tokenizer tools, while
bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to
a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias
score varies by model.
  user: string, # Optional. The ID of the end-user, for use in tracking and rate-limiting.
  n: number, # Optional. The number of completions choices that should be generated per provided prompt as part of an
overall completions response.
Because this setting can generate many completions, it may quickly consume your token quota.
Use carefully and ensure reasonable settings for max_tokens and stop.
  logprobs: number, # Optional. A value that controls the emission of log probabilities for the provided number of most likely
tokens within a completions response.
  echo: boolean, # Optional. A value specifying whether completions responses should include input prompts as prefixes to
their generated output.
  stop: [string], # Optional. A collection of textual sequences that will end completions generation.
  presence_penalty: number, # Optional. A value that influences the probability of generated tokens appearing based on their existing
presence in generated text.
Positive values will make tokens less likely to appear when they already exist and increase the
model&apos;s likelihood to output new topics.
  frequency_penalty: number, # Optional. A value that influences the probability of generated tokens appearing based on their cumulative
frequency in generated text.
Positive values will make tokens less likely to appear as their frequency increases and
decrease the likelihood of the model repeating the same statements verbatim.
  best_of: number, # Optional. A value that controls how many completions will be internally generated prior to response
formulation.
When used together with n, best_of controls the number of candidate completions and must be
greater than n.
Because this setting can generate many completions, it may quickly consume your token quota.
Use carefully and ensure reasonable settings for max_tokens and stop.
  stream: boolean, # Optional. A value indicating whether chat completions should be streamed for this request.
  model: string, # Optional. The model name to provide as part of this completions request.
Not applicable to Azure OpenAI, where deployment information should be included in the Azure
resource URI that&apos;s connected to.
}
</code>

Response Body:

Schema for <c>Completions</c>:
<code>{
  id: string, # Required. A unique identifier associated with this completions response.
  created: number, # Required. The first timestamp associated with generation activity for this completions response,
represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970.
  choices: [
    {
      text: string, # Required. The generated text for a given completions prompt.
      index: number, # Required. The ordered index associated with this completions choice.
      logprobs: {
        tokens: [string], # Optional. The textual forms of tokens evaluated in this probability model.
        token_logprobs: [number], # Optional. A collection of log probability values for the tokens in this completions data.
        top_logprobs: [Dictionary&lt;string, number&gt;], # Optional. A mapping of tokens to maximum log probability values in this completions data.
        text_offset: [number], # Optional. The text offsets associated with tokens in this completions data.
      }, # Optional. The log probabilities model for tokens associated with this completions choice.
      finish_reason: &quot;None&quot; | &quot;Stopped&quot; | &quot;TokenLimitReached&quot; | &quot;ContentFiltered&quot;, # Required. Reason for finishing
    }
  ], # Optional. The collection of completions choices associated with this completions response.
Generally, `n` choices are generated per provided prompt with a default value of 1.
Token limits and other settings may limit the number of choices generated.
  usage: {
    completion_tokens: number, # Required. The number of tokens generated across all completions emissions.
    prompt_tokens: number, # Required. The number of tokens in the provided prompts for the completions request.
    total_tokens: number, # Required. The total number of tokens processed for the completions request and response.
  }, # Required. Usage information for tokens processed and generated as part of this completions operation.
}
</code>

</remarks>
    </member>
    <member name="GetCompletions(String,RequestContent,RequestContext)">
<example>
This sample shows how to call GetCompletions with required parameters and parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {};

Response response = client.GetCompletions("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("created").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("completion_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
This sample shows how to call GetCompletions with all parameters and request content, and how to parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {
    prompt = new[] {
        "<String>"
    },
    max_tokens = 1234,
    temperature = 123.45f,
    top_p = 123.45f,
    logit_bias = new {
        key = 1234,
    },
    user = "<user>",
    n = 1234,
    logprobs = 1234,
    echo = true,
    stop = new[] {
        "<String>"
    },
    presence_penalty = 123.45f,
    frequency_penalty = 123.45f,
    best_of = 1234,
    stream = true,
    model = "<model>",
};

Response response = client.GetCompletions("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("created").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("text").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("index").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("tokens")[0].ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("token_logprobs")[0].ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("top_logprobs")[0].GetProperty("<test>").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("logprobs").GetProperty("text_offset")[0].ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("finish_reason").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("completion_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
</example>
<remarks>
Below is the JSON schema for the request and response payloads.

Request Body:

Schema for <c>CompletionsOptions</c>:
<code>{
  prompt: [string], # Optional. The prompts to generate completions from. Defaults to a single prompt of &amp;lt;|endoftext|&amp;gt; if not
otherwise specified.
  max_tokens: number, # Optional. The maximum number of tokens to generate.
  temperature: number, # Optional. The sampling temperature to use that controls the apparent creativity of generated completions.
Higher values will make output more random while lower values will make results more focused
and deterministic.
It is not recommended to modify temperature and top_p for the same completions request as the
interaction of these two settings is difficult to predict.
  top_p: number, # Optional. An alternative to sampling with temperature called nucleus sampling. This value causes the
model to consider the results of tokens with the provided probability mass. As an example, a
value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be
considered.
It is not recommended to modify temperature and top_p for the same completions request as the
interaction of these two settings is difficult to predict.
  logit_bias: Dictionary&lt;string, number&gt;, # Optional. A map between GPT token IDs and bias scores that influences the probability of specific tokens
appearing in a completions response. Token IDs are computed via external tokenizer tools, while
bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to
a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias
score varies by model.
  user: string, # Optional. The ID of the end-user, for use in tracking and rate-limiting.
  n: number, # Optional. The number of completions choices that should be generated per provided prompt as part of an
overall completions response.
Because this setting can generate many completions, it may quickly consume your token quota.
Use carefully and ensure reasonable settings for max_tokens and stop.
  logprobs: number, # Optional. A value that controls the emission of log probabilities for the provided number of most likely
tokens within a completions response.
  echo: boolean, # Optional. A value specifying whether completions responses should include input prompts as prefixes to
their generated output.
  stop: [string], # Optional. A collection of textual sequences that will end completions generation.
  presence_penalty: number, # Optional. A value that influences the probability of generated tokens appearing based on their existing
presence in generated text.
Positive values will make tokens less likely to appear when they already exist and increase the
model&apos;s likelihood to output new topics.
  frequency_penalty: number, # Optional. A value that influences the probability of generated tokens appearing based on their cumulative
frequency in generated text.
Positive values will make tokens less likely to appear as their frequency increases and
decrease the likelihood of the model repeating the same statements verbatim.
  best_of: number, # Optional. A value that controls how many completions will be internally generated prior to response
formulation.
When used together with n, best_of controls the number of candidate completions and must be
greater than n.
Because this setting can generate many completions, it may quickly consume your token quota.
Use carefully and ensure reasonable settings for max_tokens and stop.
  stream: boolean, # Optional. A value indicating whether chat completions should be streamed for this request.
  model: string, # Optional. The model name to provide as part of this completions request.
Not applicable to Azure OpenAI, where deployment information should be included in the Azure
resource URI that&apos;s connected to.
}
</code>

Response Body:

Schema for <c>Completions</c>:
<code>{
  id: string, # Required. A unique identifier associated with this completions response.
  created: number, # Required. The first timestamp associated with generation activity for this completions response,
represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970.
  choices: [
    {
      text: string, # Required. The generated text for a given completions prompt.
      index: number, # Required. The ordered index associated with this completions choice.
      logprobs: {
        tokens: [string], # Optional. The textual forms of tokens evaluated in this probability model.
        token_logprobs: [number], # Optional. A collection of log probability values for the tokens in this completions data.
        top_logprobs: [Dictionary&lt;string, number&gt;], # Optional. A mapping of tokens to maximum log probability values in this completions data.
        text_offset: [number], # Optional. The text offsets associated with tokens in this completions data.
      }, # Optional. The log probabilities model for tokens associated with this completions choice.
      finish_reason: &quot;None&quot; | &quot;Stopped&quot; | &quot;TokenLimitReached&quot; | &quot;ContentFiltered&quot;, # Required. Reason for finishing
    }
  ], # Optional. The collection of completions choices associated with this completions response.
Generally, `n` choices are generated per provided prompt with a default value of 1.
Token limits and other settings may limit the number of choices generated.
  usage: {
    completion_tokens: number, # Required. The number of tokens generated across all completions emissions.
    prompt_tokens: number, # Required. The number of tokens in the provided prompts for the completions request.
    total_tokens: number, # Required. The total number of tokens processed for the completions request and response.
  }, # Required. Usage information for tokens processed and generated as part of this completions operation.
}
</code>

</remarks>
    </member>
    <member name="GetChatCompletionsAsync(String,RequestContent,RequestContext)">
<example>
This sample shows how to call GetChatCompletionsAsync with required parameters and parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {};

Response response = await client.GetChatCompletionsAsync("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("created").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("completion_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
This sample shows how to call GetChatCompletionsAsync with all parameters and request content, and how to parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {
    messages = new[] {
        new {
            role = "system",
            content = "<content>",
        }
    },
    max_tokens = 1234,
    temperature = 123.45f,
    top_p = 123.45f,
    logit_bias = new {
        key = 1234,
    },
    user = "<user>",
    n = 1234,
    stop = new[] {
        "<String>"
    },
    presence_penalty = 123.45f,
    frequency_penalty = 123.45f,
    stream = true,
    model = "<model>",
};

Response response = await client.GetChatCompletionsAsync("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("created").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("message").GetProperty("role").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("message").GetProperty("content").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("index").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("finish_reason").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("delta").GetProperty("role").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("delta").GetProperty("content").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("completion_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
</example>
<remarks>
Below is the JSON schema for the request and response payloads.

Request Body:

Schema for <c>ChatCompletionsOptions</c>:
<code>{
  messages: [
    {
      role: &quot;system&quot; | &quot;assistant&quot; | &quot;user&quot;, # Required. The role associated with this message payload.
      content: string, # Optional. The text associated with this message payload.
    }
  ], # Optional. The collection of context messages associated with this chat completions request.
Typical usage begins with a chat message for the System role that provides instructions for
the behavior of the assistant, followed by alternating messages between the User and
Assistant roles.
  max_tokens: number, # Optional. The maximum number of tokens to generate.
  temperature: number, # Optional. The sampling temperature to use that controls the apparent creativity of generated completions.
Higher values will make output more random while lower values will make results more focused
and deterministic.
It is not recommended to modify temperature and top_p for the same completions request as the
interaction of these two settings is difficult to predict.
  top_p: number, # Optional. An alternative to sampling with temperature called nucleus sampling. This value causes the
model to consider the results of tokens with the provided probability mass. As an example, a
value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be
considered.
It is not recommended to modify temperature and top_p for the same completions request as the
interaction of these two settings is difficult to predict.
  logit_bias: Dictionary&lt;string, number&gt;, # Optional. A map between GPT token IDs and bias scores that influences the probability of specific tokens
appearing in a completions response. Token IDs are computed via external tokenizer tools, while
bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to
a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias
score varies by model.
  user: string, # Optional. The ID of the end-user, for use in tracking and rate-limiting.
  n: number, # Optional. The number of completions choices that should be generated per provided prompt as part of an
overall completions response.
Because this setting can generate many completions, it may quickly consume your token quota.
Use carefully and ensure reasonable settings for max_tokens and stop.
  stop: [string], # Optional. A collection of textual sequences that will end completions generation.
  presence_penalty: number, # Optional. A value that influences the probability of generated tokens appearing based on their existing
presence in generated text.
Positive values will make tokens less likely to appear when they already exist and increase the
model&apos;s likelihood to output new topics.
  frequency_penalty: number, # Optional. A value that influences the probability of generated tokens appearing based on their cumulative
frequency in generated text.
Positive values will make tokens less likely to appear as their frequency increases and
decrease the likelihood of the model repeating the same statements verbatim.
  stream: boolean, # Optional. A value indicating whether chat completions should be streamed for this request.
  model: string, # Optional. The model name to provide as part of this completions request.
Not applicable to Azure OpenAI, where deployment information should be included in the Azure
resource URI that&apos;s connected to.
}
</code>

Response Body:

Schema for <c>ChatCompletions</c>:
<code>{
  id: string, # Required. A unique identifier associated with this chat completions response.
  created: number, # Required. The first timestamp associated with generation activity for this completions response,
represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970.
  choices: [
    {
      message: {
        role: &quot;system&quot; | &quot;assistant&quot; | &quot;user&quot;, # Required. The role associated with this message payload.
        content: string, # Optional. The text associated with this message payload.
      }, # Optional. The chat message for a given chat completions prompt.
      index: number, # Required. The ordered index associated with this chat completions choice.
      finish_reason: &quot;None&quot; | &quot;Stopped&quot; | &quot;TokenLimitReached&quot; | &quot;ContentFiltered&quot;, # Required. The reason that this chat completions choice completed its generated.
      delta: ChatMessage, # Optional. The delta message content for a streaming response.
    }
  ], # Optional. The collection of completions choices associated with this completions response.
Generally, `n` choices are generated per provided prompt with a default value of 1.
Token limits and other settings may limit the number of choices generated.
  usage: {
    completion_tokens: number, # Required. The number of tokens generated across all completions emissions.
    prompt_tokens: number, # Required. The number of tokens in the provided prompts for the completions request.
    total_tokens: number, # Required. The total number of tokens processed for the completions request and response.
  }, # Required. Usage information for tokens processed and generated as part of this completions operation.
}
</code>

</remarks>
    </member>
    <member name="GetChatCompletions(String,RequestContent,RequestContext)">
<example>
This sample shows how to call GetChatCompletions with required parameters and parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {};

Response response = client.GetChatCompletions("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("created").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("completion_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
This sample shows how to call GetChatCompletions with all parameters and request content, and how to parse the result.
<code><![CDATA[
var credential = new AzureKeyCredential("<key>");
var endpoint = new Uri("<https://my-service.azure.com>");
var client = new OpenAIClient(endpoint, credential);

var data = new {
    messages = new[] {
        new {
            role = "system",
            content = "<content>",
        }
    },
    max_tokens = 1234,
    temperature = 123.45f,
    top_p = 123.45f,
    logit_bias = new {
        key = 1234,
    },
    user = "<user>",
    n = 1234,
    stop = new[] {
        "<String>"
    },
    presence_penalty = 123.45f,
    frequency_penalty = 123.45f,
    stream = true,
    model = "<model>",
};

Response response = client.GetChatCompletions("<deploymentId>", RequestContent.Create(data));

JsonElement result = JsonDocument.Parse(response.ContentStream).RootElement;
Console.WriteLine(result.GetProperty("id").ToString());
Console.WriteLine(result.GetProperty("created").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("message").GetProperty("role").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("message").GetProperty("content").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("index").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("finish_reason").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("delta").GetProperty("role").ToString());
Console.WriteLine(result.GetProperty("choices")[0].GetProperty("delta").GetProperty("content").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("completion_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("prompt_tokens").ToString());
Console.WriteLine(result.GetProperty("usage").GetProperty("total_tokens").ToString());
]]></code>
</example>
<remarks>
Below is the JSON schema for the request and response payloads.

Request Body:

Schema for <c>ChatCompletionsOptions</c>:
<code>{
  messages: [
    {
      role: &quot;system&quot; | &quot;assistant&quot; | &quot;user&quot;, # Required. The role associated with this message payload.
      content: string, # Optional. The text associated with this message payload.
    }
  ], # Optional. The collection of context messages associated with this chat completions request.
Typical usage begins with a chat message for the System role that provides instructions for
the behavior of the assistant, followed by alternating messages between the User and
Assistant roles.
  max_tokens: number, # Optional. The maximum number of tokens to generate.
  temperature: number, # Optional. The sampling temperature to use that controls the apparent creativity of generated completions.
Higher values will make output more random while lower values will make results more focused
and deterministic.
It is not recommended to modify temperature and top_p for the same completions request as the
interaction of these two settings is difficult to predict.
  top_p: number, # Optional. An alternative to sampling with temperature called nucleus sampling. This value causes the
model to consider the results of tokens with the provided probability mass. As an example, a
value of 0.15 will cause only the tokens comprising the top 15% of probability mass to be
considered.
It is not recommended to modify temperature and top_p for the same completions request as the
interaction of these two settings is difficult to predict.
  logit_bias: Dictionary&lt;string, number&gt;, # Optional. A map between GPT token IDs and bias scores that influences the probability of specific tokens
appearing in a completions response. Token IDs are computed via external tokenizer tools, while
bias scores reside in the range of -100 to 100 with minimum and maximum values corresponding to
a full ban or exclusive selection of a token, respectively. The exact behavior of a given bias
score varies by model.
  user: string, # Optional. The ID of the end-user, for use in tracking and rate-limiting.
  n: number, # Optional. The number of completions choices that should be generated per provided prompt as part of an
overall completions response.
Because this setting can generate many completions, it may quickly consume your token quota.
Use carefully and ensure reasonable settings for max_tokens and stop.
  stop: [string], # Optional. A collection of textual sequences that will end completions generation.
  presence_penalty: number, # Optional. A value that influences the probability of generated tokens appearing based on their existing
presence in generated text.
Positive values will make tokens less likely to appear when they already exist and increase the
model&apos;s likelihood to output new topics.
  frequency_penalty: number, # Optional. A value that influences the probability of generated tokens appearing based on their cumulative
frequency in generated text.
Positive values will make tokens less likely to appear as their frequency increases and
decrease the likelihood of the model repeating the same statements verbatim.
  stream: boolean, # Optional. A value indicating whether chat completions should be streamed for this request.
  model: string, # Optional. The model name to provide as part of this completions request.
Not applicable to Azure OpenAI, where deployment information should be included in the Azure
resource URI that&apos;s connected to.
}
</code>

Response Body:

Schema for <c>ChatCompletions</c>:
<code>{
  id: string, # Required. A unique identifier associated with this chat completions response.
  created: number, # Required. The first timestamp associated with generation activity for this completions response,
represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970.
  choices: [
    {
      message: {
        role: &quot;system&quot; | &quot;assistant&quot; | &quot;user&quot;, # Required. The role associated with this message payload.
        content: string, # Optional. The text associated with this message payload.
      }, # Optional. The chat message for a given chat completions prompt.
      index: number, # Required. The ordered index associated with this chat completions choice.
      finish_reason: &quot;None&quot; | &quot;Stopped&quot; | &quot;TokenLimitReached&quot; | &quot;ContentFiltered&quot;, # Required. The reason that this chat completions choice completed its generated.
      delta: ChatMessage, # Optional. The delta message content for a streaming response.
    }
  ], # Optional. The collection of completions choices associated with this completions response.
Generally, `n` choices are generated per provided prompt with a default value of 1.
Token limits and other settings may limit the number of choices generated.
  usage: {
    completion_tokens: number, # Required. The number of tokens generated across all completions emissions.
    prompt_tokens: number, # Required. The number of tokens in the provided prompts for the completions request.
    total_tokens: number, # Required. The total number of tokens processed for the completions request and response.
  }, # Required. Usage information for tokens processed and generated as part of this completions operation.
}
</code>

</remarks>
    </member>
  </members>
</doc>