// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

// <auto-generated/>

#nullable disable

using Azure;
using Azure.Core;
using Azure.Core.Pipeline;
using System.ServiceModel.Rest;
using System.ServiceModel.Rest.Core;
using System.ServiceModel.Rest.Core.Pipeline;
using System.ServiceModel.Rest.Shared;
using System.ServiceModel.Rest.Shared.Core;
using System.ServiceModel.Rest.Shared.Core.Pipeline;

namespace Platform.OpenAI
{
    // Data plane generated client.
    /// <summary> Azure OpenAI APIs for completions and search. </summary>
    public partial class OpenAIClient
    {
        private const string AuthorizationHeader = "Authorization";
        private readonly KeyCredential _keyCredential;
        private readonly MessagePipeline _pipeline;
        private readonly Uri _endpoint;
        private readonly string _apiVersion;

        /// <summary> The ClientDiagnostics is used to provide tracing support for the client library. </summary>
        internal TelemetrySource ClientDiagnostics { get; }

        /// <summary> The HTTP pipeline for sending and receiving REST requests and responses. </summary>
        public virtual MessagePipeline Pipeline => _pipeline;

        /// <summary> Initializes a new instance of OpenAIClient for mocking. </summary>
        protected OpenAIClient()
        {
        }

        /// <summary> Initializes a new instance of OpenAIClient. </summary>
        /// <param name="endpoint">
        /// Supported Cognitive Services endpoints (protocol and hostname, for example:
        /// https://westus.api.cognitive.microsoft.com).
        /// </param>
        /// <param name="credential"> A credential used to authenticate to an Azure Service. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="endpoint"/> or <paramref name="credential"/> is null. </exception>
        public OpenAIClient(Uri endpoint, KeyCredential credential) : this(endpoint, credential, new OpenAIClientOptions())
        {
        }

        /// <summary> Initializes a new instance of OpenAIClient. </summary>
        /// <param name="endpoint">
        /// Supported Cognitive Services endpoints (protocol and hostname, for example:
        /// https://westus.api.cognitive.microsoft.com).
        /// </param>
        /// <param name="credential"> A credential used to authenticate to an Azure Service. </param>
        /// <param name="options"> The options for configuring the client. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="endpoint"/> or <paramref name="credential"/> is null. </exception>
        public OpenAIClient(Uri endpoint, KeyCredential credential, OpenAIClientOptions options)
        {
            ClientUtilities.AssertNotNull(endpoint, nameof(endpoint));
            ClientUtilities.AssertNotNull(credential, nameof(credential));
            options ??= new OpenAIClientOptions();

            ClientDiagnostics = new TelemetrySource(options, true);
            _keyCredential = credential;
            _pipeline = HttpPipelineBuilder.Build(options, Array.Empty<HttpPipelinePolicy>(), new HttpPipelinePolicy[] { new KeyCredentialPolicy(_keyCredential, AuthorizationHeader, "Bearer") }, new ResponseClassifier());
            _endpoint = endpoint;
            _apiVersion = options.Version;
        }

        /// <summary> Return the embeddings for a given prompt. </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="embeddingsOptions">
        /// The configuration information for an embeddings request.
        /// Embeddings measure the relatedness of text strings and are commonly used for search, clustering,
        /// recommendations, and other similar scenarios.
        /// </param>
        /// <param name="cancellationToken"> The cancellation token to use. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="embeddingsOptions"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetEmbeddingsAsync(string,EmbeddingsOptions,CancellationToken)']/*" />
        public virtual async Task<Result<Embeddings>> GetEmbeddingsAsync(string deploymentId, EmbeddingsOptions embeddingsOptions, CancellationToken cancellationToken = default)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(embeddingsOptions, nameof(embeddingsOptions));

            RequestContext context = FromCancellationToken(cancellationToken);
            Result result = await GetEmbeddingsAsync(deploymentId, embeddingsOptions.ToRequestContent(), context).ConfigureAwait(false);
            return Result.FromValue(Embeddings.FromResponse(result), result);
        }

        /// <summary> Return the embeddings for a given prompt. </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="embeddingsOptions">
        /// The configuration information for an embeddings request.
        /// Embeddings measure the relatedness of text strings and are commonly used for search, clustering,
        /// recommendations, and other similar scenarios.
        /// </param>
        /// <param name="cancellationToken"> The cancellation token to use. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="embeddingsOptions"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetEmbeddings(string,EmbeddingsOptions,CancellationToken)']/*" />
        public virtual Result<Embeddings> GetEmbeddings(string deploymentId, EmbeddingsOptions embeddingsOptions, CancellationToken cancellationToken = default)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(embeddingsOptions, nameof(embeddingsOptions));

            RequestContext context = FromCancellationToken(cancellationToken);
            Result result = GetEmbeddings(deploymentId, embeddingsOptions.ToRequestContent(), context);
            return Result.FromValue(Embeddings.FromResponse(result), result);
        }

        /// <summary>
        /// [Protocol Method] Return the embeddings for a given prompt.
        /// <list type="bullet">
        /// <item>
        /// <description>
        /// This <see href="https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/core/Azure.Core/samples/ProtocolMethods.md">protocol method</see> allows explicit creation of the request and processing of the response for advanced scenarios.
        /// </description>
        /// </item>
        /// <item>
        /// <description>
        /// Please try the simpler <see cref="GetEmbeddingsAsync(string,EmbeddingsOptions,CancellationToken)"/> convenience overload with strongly typed models first.
        /// </description>
        /// </item>
        /// </list>
        /// </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="content"> The content to send as the body of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetEmbeddingsAsync(string,RequestContent,RequestContext)']/*" />
        public virtual async Task<Result> GetEmbeddingsAsync(string deploymentId, RequestBody content, PipelineOptions context = null)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateSpan("OpenAIClient.GetEmbeddings");
            scope.Start();
            try
            {
                using RestMessage message = CreateGetEmbeddingsRequest(deploymentId, content, context);
                return await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary>
        /// [Protocol Method] Return the embeddings for a given prompt.
        /// <list type="bullet">
        /// <item>
        /// <description>
        /// This <see href="https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/core/Azure.Core/samples/ProtocolMethods.md">protocol method</see> allows explicit creation of the request and processing of the response for advanced scenarios.
        /// </description>
        /// </item>
        /// <item>
        /// <description>
        /// Please try the simpler <see cref="GetEmbeddings(string,EmbeddingsOptions,CancellationToken)"/> convenience overload with strongly typed models first.
        /// </description>
        /// </item>
        /// </list>
        /// </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="content"> The content to send as the body of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetEmbeddings(string,RequestContent,RequestContext)']/*" />
        public virtual Result GetEmbeddings(string deploymentId, RequestBody content, PipelineOptions context = null)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateSpan("OpenAIClient.GetEmbeddings");
            scope.Start();
            try
            {
                using RestMessage message = CreateGetEmbeddingsRequest(deploymentId, content, context);
                return _pipeline.ProcessMessage(message, context);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary>
        /// Gets completions for the provided input prompts.
        /// Completions support a wide variety of tasks and generate text that continues from or "completes"
        /// provided prompt data.
        /// </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="completionsOptions">
        /// The configuration information for a completions request.
        /// Completions support a wide variety of tasks and generate text that continues from or "completes"
        /// provided prompt data.
        /// </param>
        /// <param name="cancellationToken"> The cancellation token to use. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="completionsOptions"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetCompletionsAsync(string,CompletionsOptions,CancellationToken)']/*" />
        public virtual async Task<Result<Completions>> GetCompletionsAsync(string deploymentId, CompletionsOptions completionsOptions, CancellationToken cancellationToken = default)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(completionsOptions, nameof(completionsOptions));

            RequestContext context = FromCancellationToken(cancellationToken);
            Result result = await GetCompletionsAsync(deploymentId, completionsOptions.ToRequestContent(), context).ConfigureAwait(false);
            return Result.FromValue(Completions.FromResponse(result), result);
        }

        /// <summary>
        /// Gets completions for the provided input prompts.
        /// Completions support a wide variety of tasks and generate text that continues from or "completes"
        /// provided prompt data.
        /// </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="completionsOptions">
        /// The configuration information for a completions request.
        /// Completions support a wide variety of tasks and generate text that continues from or "completes"
        /// provided prompt data.
        /// </param>
        /// <param name="cancellationToken"> The cancellation token to use. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="completionsOptions"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetCompletions(string,CompletionsOptions,CancellationToken)']/*" />
        public virtual Result<Completions> GetCompletions(string deploymentId, CompletionsOptions completionsOptions, CancellationToken cancellationToken = default)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(completionsOptions, nameof(completionsOptions));

            RequestContext context = FromCancellationToken(cancellationToken);
            Result result = GetCompletions(deploymentId, completionsOptions.ToRequestContent(), context);
            return Result.FromValue(Completions.FromResponse(result), result);
        }

        /// <summary>
        /// [Protocol Method] Gets completions for the provided input prompts.
        /// Completions support a wide variety of tasks and generate text that continues from or "completes"
        /// provided prompt data.
        /// <list type="bullet">
        /// <item>
        /// <description>
        /// This <see href="https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/core/Azure.Core/samples/ProtocolMethods.md">protocol method</see> allows explicit creation of the request and processing of the response for advanced scenarios.
        /// </description>
        /// </item>
        /// <item>
        /// <description>
        /// Please try the simpler <see cref="GetCompletionsAsync(string,CompletionsOptions,CancellationToken)"/> convenience overload with strongly typed models first.
        /// </description>
        /// </item>
        /// </list>
        /// </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="content"> The content to send as the body of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetCompletionsAsync(string,RequestContent,RequestContext)']/*" />
        public virtual async Task<Result> GetCompletionsAsync(string deploymentId, RequestBody content, PipelineOptions context = null)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateSpan("OpenAIClient.GetCompletions");
            scope.Start();
            try
            {
                using RestMessage message = CreateGetCompletionsRequest(deploymentId, content, context);
                return await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary>
        /// [Protocol Method] Gets completions for the provided input prompts.
        /// Completions support a wide variety of tasks and generate text that continues from or "completes"
        /// provided prompt data.
        /// <list type="bullet">
        /// <item>
        /// <description>
        /// This <see href="https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/core/Azure.Core/samples/ProtocolMethods.md">protocol method</see> allows explicit creation of the request and processing of the response for advanced scenarios.
        /// </description>
        /// </item>
        /// <item>
        /// <description>
        /// Please try the simpler <see cref="GetCompletions(string,CompletionsOptions,CancellationToken)"/> convenience overload with strongly typed models first.
        /// </description>
        /// </item>
        /// </list>
        /// </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="content"> The content to send as the body of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetCompletions(string,RequestContent,RequestContext)']/*" />
        public virtual Result GetCompletions(string deploymentId, RequestBody content, PipelineOptions context = null)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateSpan("OpenAIClient.GetCompletions");
            scope.Start();
            try
            {
                using RestMessage message = CreateGetCompletionsRequest(deploymentId, content, context);
                return _pipeline.ProcessMessage(message, context);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary>
        /// Gets chat completions for the provided chat messages.
        /// Completions support a wide variety of tasks and generate text that continues from or "completes"
        /// provided prompt data.
        /// </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="chatCompletionsOptions">
        /// The configuration information for a chat completions request.
        /// Completions support a wide variety of tasks and generate text that continues from or "completes"
        /// provided prompt data.
        /// </param>
        /// <param name="cancellationToken"> The cancellation token to use. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="chatCompletionsOptions"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetChatCompletionsAsync(string,ChatCompletionsOptions,CancellationToken)']/*" />
        internal virtual async Task<Result<ChatCompletions>> GetChatCompletionsAsync(string deploymentId, ChatCompletionsOptions chatCompletionsOptions, CancellationToken cancellationToken = default)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(chatCompletionsOptions, nameof(chatCompletionsOptions));

            RequestContext context = FromCancellationToken(cancellationToken);
            Result result = await GetChatCompletionsAsync(deploymentId, chatCompletionsOptions.ToRequestContent(), context).ConfigureAwait(false);
            return Result.FromValue(ChatCompletions.FromResponse(result), result);
        }

        /// <summary>
        /// Gets chat completions for the provided chat messages.
        /// Completions support a wide variety of tasks and generate text that continues from or "completes"
        /// provided prompt data.
        /// </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="chatCompletionsOptions">
        /// The configuration information for a chat completions request.
        /// Completions support a wide variety of tasks and generate text that continues from or "completes"
        /// provided prompt data.
        /// </param>
        /// <param name="cancellationToken"> The cancellation token to use. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="chatCompletionsOptions"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetChatCompletions(string,ChatCompletionsOptions,CancellationToken)']/*" />
        internal virtual Result<ChatCompletions> GetChatCompletions(string deploymentId, ChatCompletionsOptions chatCompletionsOptions, CancellationToken cancellationToken = default)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(chatCompletionsOptions, nameof(chatCompletionsOptions));

            RequestContext context = FromCancellationToken(cancellationToken);
            Result result = GetChatCompletions(deploymentId, chatCompletionsOptions.ToRequestContent(), context);
            return Result.FromValue(ChatCompletions.FromResponse(result), result);
        }

        // The convenience method of this operation is made internal because this operation directly or indirectly uses a low confident type, for instance, unions, literal types with number values, etc.
        /// <summary>
        /// [Protocol Method] Gets chat completions for the provided chat messages.
        /// Completions support a wide variety of tasks and generate text that continues from or "completes"
        /// provided prompt data.
        /// <list type="bullet">
        /// <item>
        /// <description>
        /// This <see href="https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/core/Azure.Core/samples/ProtocolMethods.md">protocol method</see> allows explicit creation of the request and processing of the response for advanced scenarios.
        /// </description>
        /// </item>
        /// </list>
        /// </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="content"> The content to send as the body of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetChatCompletionsAsync(string,RequestContent,RequestContext)']/*" />
        public virtual async Task<Result> GetChatCompletionsAsync(string deploymentId, RequestBody content, PipelineOptions context = null)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateSpan("OpenAIClient.GetChatCompletions");
            scope.Start();
            try
            {
                using RestMessage message = CreateGetChatCompletionsRequest(deploymentId, content, context);
                return await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        // The convenience method of this operation is made internal because this operation directly or indirectly uses a low confident type, for instance, unions, literal types with number values, etc.
        /// <summary>
        /// [Protocol Method] Gets chat completions for the provided chat messages.
        /// Completions support a wide variety of tasks and generate text that continues from or "completes"
        /// provided prompt data.
        /// <list type="bullet">
        /// <item>
        /// <description>
        /// This <see href="https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/core/Azure.Core/samples/ProtocolMethods.md">protocol method</see> allows explicit creation of the request and processing of the response for advanced scenarios.
        /// </description>
        /// </item>
        /// </list>
        /// </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="content"> The content to send as the body of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetChatCompletions(string,RequestContent,RequestContext)']/*" />
        public virtual Result GetChatCompletions(string deploymentId, RequestBody content, PipelineOptions context = null)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateSpan("OpenAIClient.GetChatCompletions");
            scope.Start();
            try
            {
                using RestMessage message = CreateGetChatCompletionsRequest(deploymentId, content, context);
                return _pipeline.ProcessMessage(message, context);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        /// <summary>
        /// Gets chat completions for the provided chat messages.
        /// This is an Azure-specific version of chat completions that supports integration with configured data sources and
        /// other augmentations to the base chat completions capabilities.
        /// </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="chatCompletionsOptions">
        /// The configuration information for a chat completions request.
        /// Completions support a wide variety of tasks and generate text that continues from or "completes"
        /// provided prompt data.
        /// </param>
        /// <param name="cancellationToken"> The cancellation token to use. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="chatCompletionsOptions"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetChatCompletionsWithAzureExtensionsAsync(string,ChatCompletionsOptions,CancellationToken)']/*" />
        internal virtual async Task<Result<ChatCompletions>> GetChatCompletionsWithAzureExtensionsAsync(string deploymentId, ChatCompletionsOptions chatCompletionsOptions, CancellationToken cancellationToken = default)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(chatCompletionsOptions, nameof(chatCompletionsOptions));

            RequestContext context = FromCancellationToken(cancellationToken);
            Result result = await GetChatCompletionsWithAzureExtensionsAsync(deploymentId, chatCompletionsOptions.ToRequestContent(), context).ConfigureAwait(false);
            return Result.FromValue(ChatCompletions.FromResponse(result), result);
        }

        /// <summary>
        /// Gets chat completions for the provided chat messages.
        /// This is an Azure-specific version of chat completions that supports integration with configured data sources and
        /// other augmentations to the base chat completions capabilities.
        /// </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="chatCompletionsOptions">
        /// The configuration information for a chat completions request.
        /// Completions support a wide variety of tasks and generate text that continues from or "completes"
        /// provided prompt data.
        /// </param>
        /// <param name="cancellationToken"> The cancellation token to use. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="chatCompletionsOptions"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetChatCompletionsWithAzureExtensions(string,ChatCompletionsOptions,CancellationToken)']/*" />
        internal virtual Result<ChatCompletions> GetChatCompletionsWithAzureExtensions(string deploymentId, ChatCompletionsOptions chatCompletionsOptions, CancellationToken cancellationToken = default)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(chatCompletionsOptions, nameof(chatCompletionsOptions));

            RequestContext context = FromCancellationToken(cancellationToken);
            Result result = GetChatCompletionsWithAzureExtensions(deploymentId, chatCompletionsOptions.ToRequestContent(), context);
            return Response.FromValue(ChatCompletions.FromResponse(result), result);
        }

        // The convenience method of this operation is made internal because this operation directly or indirectly uses a low confident type, for instance, unions, literal types with number values, etc.
        /// <summary>
        /// [Protocol Method] Gets chat completions for the provided chat messages.
        /// This is an Azure-specific version of chat completions that supports integration with configured data sources and
        /// other augmentations to the base chat completions capabilities.
        /// <list type="bullet">
        /// <item>
        /// <description>
        /// This <see href="https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/core/Azure.Core/samples/ProtocolMethods.md">protocol method</see> allows explicit creation of the request and processing of the response for advanced scenarios.
        /// </description>
        /// </item>
        /// <item>
        /// <description>
        /// Please try the simpler <see cref="GetChatCompletionsWithAzureExtensionsAsync(string,ChatCompletionsOptions,CancellationToken)"/> convenience overload with strongly typed models first.
        /// </description>
        /// </item>
        /// </list>
        /// </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="content"> The content to send as the body of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetChatCompletionsWithAzureExtensionsAsync(string,RequestContent,RequestContext)']/*" />
        internal virtual async Task<Result> GetChatCompletionsWithAzureExtensionsAsync(string deploymentId, RequestBody content, PipelineOptions context = null)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateSpan("OpenAIClient.GetChatCompletionsWithAzureExtensions");
            scope.Start();
            try
            {
                using RestMessage message = CreateGetChatCompletionsWithAzureExtensionsRequest(deploymentId, content, context);
                return await _pipeline.ProcessMessageAsync(message, context).ConfigureAwait(false);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        // The convenience method of this operation is made internal because this operation directly or indirectly uses a low confident type, for instance, unions, literal types with number values, etc.
        /// <summary>
        /// [Protocol Method] Gets chat completions for the provided chat messages.
        /// This is an Azure-specific version of chat completions that supports integration with configured data sources and
        /// other augmentations to the base chat completions capabilities.
        /// <list type="bullet">
        /// <item>
        /// <description>
        /// This <see href="https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/core/Azure.Core/samples/ProtocolMethods.md">protocol method</see> allows explicit creation of the request and processing of the response for advanced scenarios.
        /// </description>
        /// </item>
        /// <item>
        /// <description>
        /// Please try the simpler <see cref="GetChatCompletionsWithAzureExtensions(string,ChatCompletionsOptions,CancellationToken)"/> convenience overload with strongly typed models first.
        /// </description>
        /// </item>
        /// </list>
        /// </summary>
        /// <param name="deploymentId"> Specifies either the model deployment name (when using Azure OpenAI) or model name (when using non-Azure OpenAI) to use for this request. </param>
        /// <param name="content"> The content to send as the body of the request. </param>
        /// <param name="context"> The request context, which can override default behaviors of the client pipeline on a per-call basis. </param>
        /// <exception cref="ArgumentNullException"> <paramref name="deploymentId"/> or <paramref name="content"/> is null. </exception>
        /// <exception cref="ArgumentException"> <paramref name="deploymentId"/> is an empty string, and was expected to be non-empty. </exception>
        /// <exception cref="RequestFailedException"> Service returned a non-success status code. </exception>
        /// <returns> The response returned from the service. </returns>
        /// <include file="Docs/OpenAIClient.xml" path="doc/members/member[@name='GetChatCompletionsWithAzureExtensions(string,RequestContent,RequestContext)']/*" />
        internal virtual Result GetChatCompletionsWithAzureExtensions(string deploymentId, RequestBody content, PipelineOptions context = null)
        {
            ClientUtilities.AssertNotNullOrEmpty(deploymentId, nameof(deploymentId));
            ClientUtilities.AssertNotNull(content, nameof(content));

            using var scope = ClientDiagnostics.CreateSpan("OpenAIClient.GetChatCompletionsWithAzureExtensions");
            scope.Start();
            try
            {
                using RestMessage message = CreateGetChatCompletionsWithAzureExtensionsRequest(deploymentId, content, context);
                return _pipeline.ProcessMessage(message, context);
            }
            catch (Exception e)
            {
                scope.Failed(e);
                throw;
            }
        }

        internal RestMessage CreateGetEmbeddingsRequest(string deploymentId, RequestBody content, PipelineOptions context)
        {
            var message = new HttpMessage(_pipeline.CreateRestMessage(context, ResponseClassifier200));
            var request = message.Request;
            request.Method = RequestMethod.Post;
            var uri = new RequestUri();
            uri.Reset(_endpoint);
            uri.AppendRawPathOrQueryOrHostOrScheme("/openai", false);
            uri.AppendPath("/deployments/", false);
            uri.AppendPath(deploymentId, true);
            uri.AppendPath("/embeddings", false);
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = new RawRequestUri(uri);
            request.Headers.Add("Accept", "application/json");
            request.Headers.Add("Content-Type", "application/json");
            request.Content = new RequestBodyContent(content);
            return message;
        }

        internal RestMessage CreateGetCompletionsRequest(string deploymentId, RequestBody content, PipelineOptions context)
        {
            var message = new HttpMessage(_pipeline.CreateRestMessage(context, ResponseClassifier200));
            var request = message.Request;
            request.Method = RequestMethod.Post;
            var uri = new RequestUri();
            uri.Reset(_endpoint);

            // TODO: This is a manual hack to make the endpoint point to the
            // public non-Azure service.
            // Note: Possible need to change generation to add the version differently,
            // because they don't use the Azure `?api-version=<version>` convention?

            //uri.AppendRaw("/openai", false);
            //uri.AppendPath("/deployments/", false);
            //uri.AppendPath(deploymentId, true);
            //uri.AppendPath("/completions", false);
            //uri.AppendQuery("api-version", _apiVersion, true);

            // <hack>
            uri.AppendPath("v1"); // <-- Note different service version convention.
            uri.AppendPath("/completions", false);
            // </hack>

            request.Uri = new RawRequestUri(uri);
            request.Headers.Add("Accept", "application/json");
            request.Headers.Add("Content-Type", "application/json");
            request.Content = new RequestBodyContent(content);
            return message;
        }

        internal RestMessage CreateGetChatCompletionsRequest(string deploymentId, RequestBody content, PipelineOptions context)
        {
            var message = new HttpMessage(_pipeline.CreateRestMessage(context, ResponseClassifier200));
            var request = message.Request;
            request.Method = RequestMethod.Post;
            var uri = new RequestUri();
            uri.Reset(_endpoint);
            uri.AppendRawPathOrQueryOrHostOrScheme("/openai", false);
            uri.AppendPath("/deployments/", false);
            uri.AppendPath(deploymentId, true);
            uri.AppendPath("/chat/completions", false);
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = new RawRequestUri(uri);
            request.Headers.Add("Accept", "application/json");
            request.Headers.Add("Content-Type", "application/json");
            request.Content = new RequestBodyContent(content);
            return message;
        }

        internal RestMessage CreateGetChatCompletionsWithAzureExtensionsRequest(string deploymentId, RequestBody content, PipelineOptions context)
        {
            var message = new HttpMessage(_pipeline.CreateRestMessage(context, ResponseClassifier200));
            var request = message.Request;
            request.Method = RequestMethod.Post;
            var uri = new RequestUri();
            uri.Reset(_endpoint);
            uri.AppendRawPathOrQueryOrHostOrScheme("/openai", false);
            uri.AppendPath("/deployments/", false);
            uri.AppendPath(deploymentId, true);
            uri.AppendPath("/extensions/chat/completions", false);
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = new RawRequestUri(uri);
            request.Headers.Add("Accept", "application/json");
            request.Headers.Add("Content-Type", "application/json");
            request.Content = new RequestBodyContent(content);
            return message;
        }

        internal RestMessage CreateBeginAzureBatchImageGenerationRequest(RequestBody content, PipelineOptions context)
        {
            var message = new HttpMessage(_pipeline.CreateRestMessage(context, ResponseClassifier202));
            var request = message.Request;
            request.Method = RequestMethod.Post;
            var uri = new RequestUri();
            uri.Reset(_endpoint);
            uri.AppendRawPathOrQueryOrHostOrScheme("/openai", false);
            uri.AppendPath("/images/generations:submit", false);
            uri.AppendQuery("api-version", _apiVersion, true);
            request.Uri = new RawRequestUri(uri);
            request.Headers.Add("Accept", "application/json");
            request.Headers.Add("Content-Type", "application/json");
            request.Content = new RequestBodyContent(content);
            return message;
        }

        private static RequestContext DefaultRequestContext = new RequestContext();
        internal static RequestContext FromCancellationToken(CancellationToken cancellationToken = default)
        {
            if (!cancellationToken.CanBeCanceled)
            {
                return DefaultRequestContext;
            }

            return new RequestContext() { CancellationToken = cancellationToken };
        }

        private static ResponseClassifier _responseClassifier200;
        private static ResponseClassifier ResponseClassifier200 => _responseClassifier200 ??= new StatusCodeClassifier(stackalloc ushort[] { 200 });
        private static ResponseClassifier _responseClassifier202;
        private static ResponseClassifier ResponseClassifier202 => _responseClassifier202 ??= new StatusCodeClassifier(stackalloc ushort[] { 202 });
    }
}
